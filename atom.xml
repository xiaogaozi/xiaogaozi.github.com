<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Freedom]]></title>
  <link href="https://blog.xiaogaozi.org/atom.xml" rel="self"/>
  <link href="https://blog.xiaogaozi.org/"/>
  <updated>2021-02-04T11:15:59+08:00</updated>
  <id>https://blog.xiaogaozi.org/</id>
  <author>
    <name><![CDATA[xiaogaozi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #8]]></title>
    <link href="https://blog.xiaogaozi.org/2021/02/04/maybe-news-issue-8/"/>
    <updated>2021-02-04T10:07:06+08:00</updated>
    <id>https://blog.xiaogaozi.org/2021/02/04/maybe-news-issue-8</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>The Google File System</h2>

<p><a href="https://research.google/pubs/pub51">[链接]</a></p>

<p>2003 年的 SOSP 会议上作为一家刚成立 5 年的创业公司，Google 发表了这篇影响深远的论文。论文的第一作者 Sanjay Ghemawat 相比他的同事 Jeff Dean 可能不太为外界所知，但看过他的履历以后就会发现早在 DEC 工作期间他就已经与 Jeff Dean 共事，当 Jeff Dean 在 1999 年加入 Google 后不久 Sanjay Ghemawat 也随即加入，并一起研发了 Google File System（以下简称 GFS）、MapReduce、Bigtable、Spanner、TensorFlow 这些每一个都鼎鼎大名的系统，是当之无愧的 Google 元老。</p>

<p>18 年后的今天再来回顾这篇论文依然能发现很多值得借鉴的地方，作为 GFS 最著名的开源实现，HDFS 近年来虽然已经有了很多自己的改进，但核心架构依然沿用的是这篇论文的思想。让我们回到十几年前，去探求为什么 Google 当时要研发这样一个分布式文件系统。</p>

<blockquote><p>GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability. However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions.</p></blockquote>

<p>论文开篇的第一段话已经很好地概括了 GFS 设计的初衷，这是一个完全基于 Google 业务特点设计的系统。回想一下 Google 的业务是什么？搜索引擎。搜索引擎依靠的是爬虫抓取大量数据，通过用户输入的关键词在这个庞大的数据库中检索，最后通过 Google 独有的<a href="https://en.wikipedia.org/wiki/PageRank">排序算法</a>把搜索结果展示给用户。GFS 面对的业务场景有下面几个特点：</p>

<ul>
<li><strong>组件故障随处可见</strong>：存储集群由成百上千台普通商用机器组成（与之对应的是昂贵的超级计算机），再加上应用程序和操作系统的 bug、人为错误、各种硬件故障，系统随时都面临着很多不稳定的因素。因此持续监控、错误检测、容错以及自动恢复就显得尤为重要。</li>
<li><strong>大文件为主</strong>：GB 级文件非常常见，每个文件通常包含很多应用对象（application objects），比如 web 文档。对于数十亿对象的 TB 级数据集来说，把文件切分成 KB 级大小会使得管理变得非常复杂，即使系统能够支撑这样的量级。因此系统设计的假设、文件块的大小都需要重新衡量。</li>
<li><strong>大多数文件都只是追加写而不是覆盖</strong>：随机写的场景完全不存在，文件一旦写入，只会涉及读操作，且通常是顺序读。多种类型的数据都具有这样的特征，例如某些数据是被数据分析程序批量扫描、某些数据是由数据流持续生成、某些数据是归档数据、某些数据属于中间结果（由某一台机器生成然后被另一台机器处理）。</li>
</ul>


<p>Google 当时已经部署了多个 GFS 集群，最大的一个集群有超过 1000 个存储节点以及超过 300TB 的磁盘，同时被数百个客户端访问。</p>

<p>论文的第二章节详细介绍了 GFS 的设计假设，除了前面提到的 3 个以外还包括：</p>

<ul>
<li><strong>业务场景主要包含两种读取模式：大批量的流式读取和小量的随机读取。</strong>对于前一种模式，每次请求一般读取数百 KB 或者 MB 级的数据，同一个客户端的连续请求一般也是读取某个文件的连续区域。而后一种模式通常从文件任意偏移位置读取几 KB 数据，对于那些性能敏感的应用会把多个随机读请求排序后批量发送，避免在单个文件中来来回回。</li>
<li><strong>系统需要针对并发追加写同一个文件的场景设计好的语义</strong>：典型的应用场景是把 GFS 作为消息队列，数百个生产者并发追加数据到同一个文件；或者多路合并文件，想象一下 MapReduce 的 reduce 阶段。这个文件有可能是边写边读，也有可能是写完以后再读。因此用最小的同步开销保证原子性是非常有必要的。</li>
<li><strong>高吞吐比低延时更重要</strong>：GFS 面对的大多数应用追求的是高速率批量处理数据，只有少部分应用对于点查有严格的延时要求。</li>
</ul>


<p>像传统的文件系统一样，GFS 提供包括创建、删除、打开、关闭、读取、写入这样的接口，但是 GFS 并不提供 POSIX 这样的标准 API。文件通过目录结构组织，可以通过路径名来标识某一个文件。除此之外，GFS 还提供快照（snapshot）和原子追加写（record append）功能。</p>

<p>在介绍完 GFS 的设计背景以及假设以后，接下来是详细的 GFS 架构讲解。<strong>GFS 服务端由一个 master 和多个 chunkserver 组成，通过特定的 client 库（实现了 GFS 的文件系统 API）与应用集成。</strong>GFS 是非常经典的分布式系统架构，影响了后来很多系统的设计。</p>

<p>Master 负责维护整个文件系统的元数据（metadata），包括命名空间（namespace）、访问控制（access control）信息、文件到 chunk 的映射以及每一个 chunk 的具体位置（location）。命名空间可以理解为目录结构、文件名等信息。除此之外，master 还承担一些系统级的活动，例如 chunk 的租约（lease）管理、垃圾回收无效 chunk、在不同 chunkserver 之间迁移 chunk。Master 会周期性地与每一个 chunkserver 进行心跳通信，心跳信息中同时还会包含 master 下发的指令以及 chunkserver 上报的状态。元数据都是保存在 master 的内存中，因此 master 的操作都非常快。每个 chunk 的元数据大约会占用 64 字节内存空间，每个文件的命名空间信息也是占用 64 字节左右（因为 master 针对文件名进行了前缀压缩），相对来说内存的开销是很小的，随着文件数的增加对 master 节点进行纵向扩展即可。比较重要的元数据信息（比如命名空间、文件到 chunk 的映射）还会同步持久化操作日志（operation log）到 master 的本地磁盘以及复制到远端机器，保证系统的可靠性，避免元数据丢失。当操作日志增长到一定大小，master 会生成一个检查点（checkpoint）用于加快状态恢复，检查点文件是一个类似 B 树的结构，可以不经过解析映射到内存直接查询。每个 chunk 的具体位置不会被持久化，master 每次启动时会通过请求所有 chunkserver 来获取这些信息。最初设计时其实考虑过持久化 chunk 的位置信息，但是后来发现在 chunkserver 拓扑经常变化（比如宕机、扩缩容）的情况下如何保持 master 和 chunkserver 之间的数据同步是一个难题。此外 GFS 还提供仅用于只读场景的影子（shadow）master，影子 master 的数据不是实时同步，因此不保证是最新的数据。</p>

<p>采用单 master 的架构极大地简化了 GFS 的设计（也成为了之后被人诟病的因素），master 作为掌握全局信息的唯一入口，必须确保最小程度影响读写操作，否则就会变成整个系统的瓶颈。<strong>因此 GFS 的设计是 client 读写数据永远不会经过 master。</strong>实现方式很简单，client 请求 master 获取到具体需要通信（不管是读还是写）的 chunkserver 列表，把这个列表缓存在本地，之后就直接请求 chunkserver。</p>

<p>Chunkserver 这个名字的来历其实是因为 GFS 把文件分割成了多个固定大小的 chunk。每个 chunk 的大小是 64MiB，相比传统文件系统的块（block）大小大了很多（比如 ext4 默认的块大小是 4KiB），同时 master 会为每一个 chunk 分配一个全局唯一的 64 位 ID。Chunkserver 除了将 chunk 存储到本地磁盘上，还会复制到其它 chunkserver，GFS 默认会存储 3 个副本，当然用户也可以为不同的目录指定不同的复制等级。为什么 GFS 会选择 64MiB 这么大的 chunk 大小呢？论文中列举了几个原因：</p>

<ul>
<li><strong>减少 client 与 master 的交互</strong>：前面提到 client 不管是读还是写数据都需要首先与 master 通信，GFS 的业务场景通常都是顺序读写大文件，chunk 大小越大 client 就能在 1 次请求中获取到更多的信息。即使是随机读的场景，client 也能更多地缓存 chunk 位置信息。</li>
<li><strong>降低 chunkserver 的网络开销</strong>：更大的 chunk，client 越可能执行更多的操作，因此可以降低 client 与 chunkserver 之间的 TCP 长连接的网络开销。</li>
<li><strong>减少 master 维护的元数据大小</strong>：chunk 越大，master 就可以在内存中保存更多元数据。</li>
</ul>


<p>每个 chunk 以及它的副本有两种角色：主副本（primary replica）和从副本（secondary replica），主副本只有 1 个，其它的都是从副本，至于具体哪个是主副本是由 master 决定的。master 会授权一个租约（lease）给主副本，租约的初始超时时间是 60 秒，但是只要 chunk 还在被修改，主副本可以无限续租，master 也可以随时废除租约。基于主从副本和租约的概念，数据写入 GFS 的流程是：</p>

<ol>
<li>Client 请求 master 获取当前 chunk 所有副本所在的 chunkserver 列表，如果目前还没有租约，master 会授权给其中一个副本（也就是说这个副本升级为主副本）。</li>
<li>Master 将主副本的 ID 以及从副本的位置回复给 client，client 会将这些信息缓存在本地，只有当主副本无法通信或者租约失效时才会再次请求 master。</li>
<li>Client 发送<strong>数据</strong>给主从副本所在的全部 chunkserver。发送顺序无所谓，一般是发送给离 client 最近的一个 chunkserver，然后这个 chunkserver 会传递给离它最近的下一个 chunkserver，依此类推。Chunkserver 之间的距离是通过 IP 地址估算出来的，之所以采用这种线性传递数据的方式，目的是最大化网络吞吐。Chunkserver 不会等到一个 chunk 全部接收完毕才发送出去，而是采用管道（pipeline）的方式，只要接收到一定的数据就立即发送。<strong>值得一提的是，当时 Google 的网络带宽是 100Mbps，而现在（2021 年）AWS 上的机器网络带宽能达到 25Gbps，是当年的 250 倍。</strong></li>
<li>一旦所有副本都回复收到了数据，client 就发送<strong>写请求</strong>给主副本。这个请求包含了上一步发送的所有数据的标识符，主副本会分配连续的序列号给写请求，并按照序列号的顺序修改它的状态。</li>
<li>主副本转发写请求给其它从副本，从副本也会按照相同的序列号顺序修改状态。</li>
<li>当所有从副本都回复给主副本，即表示这次写请求已经完成。</li>
<li>主副本回复请求给 client。如果任何副本发生了错误也会一并回复，GFS 的客户端会尝试重试。步骤 3~步骤 7 执行时也会有一定的重试机制，避免每次都从头开始。</li>
</ol>


<p>原子追加写（record append）的流程大体上和上面介绍的一样，区别在于第 4 步时主副本会检查写入以后是否会超过最后一个 chunk 的大小（64MiB），如果没超过就追加到后面，如果超过了会把最后一个 chunk 填充（pad）满，并回复 client 重试。</p>

<p>快照（snapshot）功能基于 copy-on-write 实现，master 通过仅仅复制元数据的方式能够在短时间内完成快照的创建。当 client 需要修改快照数据时，master 会通知所有 chunkserver 本地复制对应的 chunk，新的修改会在复制后的 chunk 上进行。</p>

<p>限于本期的篇幅，还有很多 GFS 的特性没有介绍，例如命名空间管理与锁、副本放置策略（placement policy）、chunk 重新复制（re-replication）、数据均衡（rebalancing）、垃圾回收、高可用等。最后是一个彩蛋，如果你仔细看论文最后的感谢名单，会发现一个熟悉的名字（当然不是 Jeff Dean）。</p>

<h2>Colossus: Successor to the Google File System</h2>

<p><a href="https://www.systutorials.com/colossus-successor-to-google-file-system-gfs">[链接]</a></p>

<p>自从 GFS 的论文发布以来，Google 的数据已经增长了好几个数量级，很显然 GFS 的架构已经无法支撑如此大规模的数据存储。那 Google 下一代的文件存储是什么呢？答案就是 Colossus。这个神秘的项目直到目前为止都没有在公开场合被全面正式地介绍过，我们只能通过很多碎片的信息来拼凑出它的模样，上面链接中的内容即是通过这些信息整理出来的。一些有趣的信息是：元数据服务（Curators）基于 Bigtable；相比 GFS 至少可以横向扩展 100 倍；GFS 依然存在，只不过是用来存储文件系统元数据的元数据（metametadata）；Colossus 可以基于另一个 Colossus 构建，就像俄罗斯套娃一样无限嵌套（让我想到了分形）；存储数据的服务叫做 D server；默认使用 Reed-Solomon 编码存储数据，也就是通常所说的纠删码（erasure code）。建议配合这个 2017 年的 <a href="http://www.pdsw.org/pdsw-discs17/slides/PDSW-DISCS-Google-Keynote.pdf">slide</a> 以及这篇<a href="https://levy.at/blog/22">中文博客</a>一起阅读。</p>

<h2>Storage Reimagined for a Streaming World</h2>

<p><a href="https://blog.pravega.io/2017/04/09/storage-reimagined-for-a-streaming-world">[链接]</a></p>

<p>流式计算这几年应该算是红到发紫？看看 Flink 社区的发展便可知晓。不过本期要介绍的不是流式计算，而是流式存储。说到与流式计算有关的存储，首先想到的可能是 Kafka，作为实时数据流的消息总线，Kafka 承担着非常重要的角色。但是 Kafka 也不是完美的，它的诞生其实比流式计算更早。Kafka 2011 年<a href="https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka">开源</a>，Spark <a href="https://spark.apache.org/news/spark-0-7-0-released.html">v0.7.0</a> 2013 年发布开始支持 streaming，Flink <a href="https://flink.apache.org/news/2014/11/04/release-0.7.0.html">v0.7.0</a> 2014 年发布开始支持 streaming（跟 Spark 是同一个版本号不知是否是巧合）。因此 Kafka 的很多设计并不是针对流式计算场景优化。比如 topic partition 这个概念，本质上是为了提高读或者写的并发，但是 partition 本身是一个静态配置，并不能做到动态伸缩。再比如 Kafka 的数据存储，目前只支持内存和本地磁盘两种，消费新数据都是从内存，如果是旧数据就可能读磁盘，但是 Kafka 集群的存储容量上限毕竟还是受限于磁盘空间，在流式计算越来越重以及云计算大行其道的今天集群运维是一个难题（某些公司已经自研了 Kafka on HDFS 的方案，比如<a href="https://cloud.tencent.com/developer/news/599446">快手</a>）。Pravega 便这样应运而生，这是一个来自戴尔的<a href="https://github.com/pravega/pravega">开源项目</a>，一些设计亮点是动态 partition 以及自动数据分层（Apache BookKeeper + HDFS）。</p>

<h2>Why We Built lakeFS: Atomic and versioned Data Lake Operations</h2>

<p><a href="https://lakefs.io/why-we-built-lakefs-atomic-and-versioned-data-lake-operations">[链接]</a></p>

<p>在数据库领域 ACID 和 MVCC 已经不是什么新鲜的概念，但是文件系统领域似乎还是一个属于比较「早期」的阶段，虽然过去已经有类似 <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a>、<a href="https://en.wikipedia.org/wiki/Btrfs">Btrfs</a> 这样创新的设计，但它们并不是广泛被大众了解以及使用的技术。特别是当云计算以及 S3 这样的「傻瓜」方案出现后，人们似乎已经习惯了开箱即用的产品。数据湖（data lake）这个词汇不知道从什么时候开始流行，对象存储的角色变得越来越重（至少云厂商是这样希望的？）。人们对这个「万能」的存储有着越来越多的期望，但是对象存储并不是万能的。为了解决对象存储的各种问题（这里不赘述具体问题）或者说填补它的一些缺失，越来越多基于对象存储的项目诞生。<a href="https://lakefs.io">lakeFS</a> 即是其中一个，lakeFS 希望通过提供类似 Git 的体验来管理对象存储中的数据，并且保证 ACID。比如创建一个数据的「分支」即可实现多版本管理。lakeFS 的开发团队来自以色列（<a href="https://www.treeverse.io">公司官网</a>挺有意思），项目使用 Go 语言实现。一些类似的项目还有 <a href="https://dvc.org">DVC</a>、<a href="https://github.com/quiltdata/quilt">Quilt</a> 以及 <a href="https://github.com/tensorwerk/hangar-py">Hanger</a>。</p>

<h2>Magnet: A scalable and performant shuffle architecture for Apache Spark</h2>

<p><a href="https://engineering.linkedin.com/blog/2020/introducing-magnet">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6/">第 6 期</a> Maybe News 曾经介绍过 Facebook 的 Cosco，一个给 Hive/Spark 使用的 remote shuffle service 实现。本期介绍的 Magnet 来自 LinkedIn，也是一个 shuffle service。跟 Cosco 的区别在于 Magnet 不是存算分离架构，不依赖外部存储，核心思想是 mapper 把 shuffle 数据先写到本地的 shuffle 服务，然后这些 shuffle 数据会根据某种负载均衡算法推到远端的 shuffle 服务上，远端 shuffle 服务会定期合并（merge）数据，最后 reducer 从远端 shuffle 服务读取数据。这里的「远端」其实是一个相对的概念，有可能 reducer 跟 shuffle 服务在同一个节点上，那就不需要发送 RPC 请求而是直接读取本地磁盘的数据。更多技术细节可以参考 VLDB 2020 的<a href="http://www.vldb.org/pvldb/vol13/p3382-shen.pdf">论文</a>，另外 LinkedIn 的工程师也在积极将 Magnet 贡献给 Spark 社区，目前已经合入了几个 PR，具体请参考 <a href="https://issues.apache.org/jira/browse/SPARK-30602">SPARK-30602</a>。</p>

<h2>支付宝研究员王益：Go+ 可有效补全 Python 的不足</h2>

<p><a href="https://tech.antfin.com/community/articles/993">[链接]</a></p>

<p><a href="https://github.com/wangkuiyi">王益</a>目前是蚂蚁集团研究员，同时也是开源项目 <a href="https://github.com/sql-machine-learning/sqlflow">SQLFlow</a> 和 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a> 的负责人（这两个项目也很有意思，有兴趣的同学可以去了解了解）。这里介绍的 <a href="https://github.com/goplus/gop">Go+</a> 是七牛创始人许式伟发起的开源项目，从 Go+ 的 slogan「the language for data science」就能看出项目的设计初衷。如果说目前什么编程语言在数据科学和机器学习领域最受欢迎，那可能就是 Python 了。但是 Python 的语言特性决定了它可能并不是最适合的，Go+ 依托 Go 语言作为基础，很好地弥补了 Python 的缺失。推荐对机器学习感兴趣的同学看看这篇文章，其中提到的一些八卦历史也很有趣。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #7]]></title>
    <link href="https://blog.xiaogaozi.org/2020/11/15/maybe-news-issue-7/"/>
    <updated>2020-11-15T16:37:20+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/11/15/maybe-news-issue-7</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores</h2>

<p><a href="https://databricks.com/research/delta-lake-high-performance-acid-table-storage-overcloud-object-stores">[链接]</a></p>

<p>14 年前，Amazon 发布了 EC2（Elastic Compute Cloud）和 S3（Simple Storage Service）这两个划时代的产品，从此「云计算」这个词开始进入大众的视野，经过十几年的发展已经逐渐被大众所认知与接受。「云」意味着近乎无限的资源，EC2 为用户提供了计算资源，S3 为用户提供了存储资源。传统基于 Hadoop 的大数据平台是将这两种资源绑定在一起的，而迁移到云端以后非常自然地会想到将存储资源转到类似 S3 的对象存储中，从而真正实现存储计算分离的架构，能够更加弹性地管理计算和存储这两种天生异构的资源，既大幅节约了成本还省去了运维 HDFS 集群的各种烦恼。</p>

<p>作为 Spark 的发明者，Databricks 这家商业公司的很多客户同时也是 AWS 的客户，因此有着非常丰富的在大数据场景使用 S3 的经验。这些经验暴露了 S3（或者类似的对象存储）作为 HDFS 替代者的种种缺陷。</p>

<p>对象存储（object store）的用户可以创建很多 bucket，每个 bucket 中存储了很多对象（object），每个对象都会有一个唯一的 key 作为标识。因此对象存储本质上是一个 K/V 存储，这一点非常重要，因为通常的认知都会将对象存储等同于文件系统（file system）。对象存储中的「目录」其实是通过 key 的前缀模拟出来的，虽然对象存储提供类似 LIST 目录这样的 API，底层实现却是遍历相同前缀的对象，这个操作在文件系统中是 O(1) 的时间复杂度，但在对象存储中是 O(n)。更加严重的情况是，S3 的 LIST API 每次请求最多返回 1000 个 key，单次请求延时通常为几十到几百毫秒，因此当处理超大规模的数据集时单单花在遍历上的时间就可能是分钟级。重命名对象或者目录也是一样，文件系统是一个原子操作，对象存储是先拷贝到新路径，再删除原路径的对象，代价非常高。</p>

<p>另一个对象存储严重的问题是一致性模型，S3 的一致性模型是<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">最终一致性</a>。当某个客户端上传了一个新的对象以后，其它客户端并不一定保证能立即 LIST 或者读取这个对象。当一个对象被更新或者删除以后也会发生同样的现象，即使是负责写入的这个客户端自己也有可能遇到。S3 能确保的一致性是 read-after-write，也就是说 PUT 请求产生以后的 GET 请求是保证一定能返回正确数据的。</p>

<p>论文概述了目前大数据存储的 3 种方案：分区目录、自定义存储引擎、元数据在对象存储中，下面分别介绍。</p>

<p><strong>分区目录</strong>顾名思义就是将数据按照某些属性进行分区，比如日期。这是大数据领域非常普遍的做法，好处是可以根据分区过滤不需要的数据，也就能减少 LIST 请求的数量。这个方案并没有解决前面提到的对象存储的问题，因此缺点也很明显：不支持跨多个对象的原子操作、最终一致性、低性能、不支持多版本和审计日志。</p>

<p><strong>自定义存储引擎</strong>的意思是在云上实现一个独立的元数据服务，类似 Snowflake、<a href="https://juicefs.com">JuiceFS</a> 的做法。对象存储只是被当作一个无限容量的块存储，一切元数据操作都依赖这个单独的元数据服务。这个方案的挑战是：</p>

<ul>
<li>所有 I/O 操作都需要经过元数据服务，这会带来额外的请求开销，降低性能和可用性。</li>
<li>实现一个与现有计算引擎互通的连接器（connector）需要更高的工程成本</li>
<li>用户会因为元数据服务而绑定在某一个特定的服务供应商上，没法直接访问对象存储中的数据。</li>
</ul>


<p><strong>元数据在对象存储中</strong>是 Databricks 提倡的方案，即今天介绍的 Delta Lake。这个方案和前一个的本质区别是不存在一个中心化的元数据服务，元数据是通过「日志」的形式直接存放在对象存储中。从目录结构上来看，Delta Lake 定义了一种特殊的存储格式，例如对于更新或者删除的数据会产生很多小的 delta 文件。这一点上其实跟 <a href="https://issues.apache.org/jira/browse/HIVE-5317">Hive 实现 ACID</a> 的设计很像，后者在 2013 年就已经开始开发，而 Delta Lake 项目是 2016 年启动，很难说有没有借鉴的成分。更加类似 Delta Lake 的是另外两个项目：<a href="https://hudi.apache.org">Apache Hudi</a> 和 <a href="https://iceberg.apache.org">Apache Iceberg</a>，关于这几个项目的异同后面会有一个更详细的介绍，Databricks 目前宣传的一些 Delta Lake 独有的特性（比如 Z-order clustering）其实并没有开源。</p>

<p>Delta Lake 的思想其实很好理解，本质上是把所有操作都通过日志的形式记录下来，当读取时需要重放这些日志来得到最新的数据状态，最终实现 ACID 的语义。优化的点在于怎么加速整个流程，比如定期合并日志为一个 checkpoint、索引最新的 checkpoint 等。这种把日志作为元数据的设计解决了前面提到的对象存储最终一致性的问题，即只依赖日志来确定具体读取的文件，而不是简单通过 LIST 一个目录。但是从论文描述的场景来看还是有可能因为最终一致性踩坑（因为依然会用到 LIST API），至于这个概率有多大就不知道了，因此我对于是否能根本性解决一致性问题存疑。</p>

<p>写数据的时候有一个地方需要特别注意，日志文件的文件名是递增且全局唯一的 ID，因为写入存在并发，所以需要在这一步保证操作的原子性。根据不同的对象存储有不同的解决方案：</p>

<ul>
<li>Google Cloud Storage 和 Azure Blob Store 因为支持原子 put-if-absent 操作，因此可以通过这个 API 实现。</li>
<li>对于支持原子 rename 的文件系统（比如 HDFS、Azure Data Lake Storage），可以通过这种方式实现。</li>
<li>如果以上功能都不支持（比如 S3），在 Databricks 的企业版本里是通过一个独立的轻量级协调服务（coordination service）来确保 ID 递增的原子性。在开源版本的 Spark 连接器里是通过 Spark driver 来统一分配 ID，这样也能保证在 1 个 Spark 任务里可以并发写。你也可以通过 <code>LogStore</code> 这个接口实现一个类似 Databricks 提供的协调服务。因为依赖了一个中心化的服务（虽然只是在写数据时），也一定程度上破坏了 Delta Lake 宣扬的去中心化思想。</li>
</ul>


<p>由于日志中记录了所有的历史操作，并且数据和日志都是不可变的（immutable），因此 Delta Lake 可以很轻松实现时间旅行（Time Travel）功能，也就是重现某个历史时刻的数据状态。Delta Lake 通过类似 <code>TIMESTAMP AS OF</code> 这种语法的 SQL 可以让用户指定读取某个时间的数据，不过这个 SQL 语法目前在开源版本中还<a href="https://github.com/delta-io/delta/issues/128">不支持</a>。</p>

<p>Delta Lake 也可以很好地跟流式计算进行结合，不管是生产者还是消费者都可以利用 Delta Lake 的 API 来实现流式写和读数据。当然毕竟因为是 Databricks 开发的产品，目前结合得最好的肯定是 Spark Structured Streaming。你问支持 Flink 吗？至少 Databricks 员工的<a href="https://github.com/delta-io/delta/issues/156#issuecomment-552503730">回答</a>是还在计划中，短期内估计没戏。</p>

<p>最后是性能评测部分。首先评测的是 LIST 大量文件的场景，通过对同一张表进行不同程度地分区来模拟不同量级的文件，评测的引擎是 Hive、Presto、Databricks Runtime（企业版 Spark，以下简称 DR），其中 Hive 和 Presto 读取的数据格式是 Parquet，DR 读取的格式是 Parquet 和 Delta Lake。Hive 在有 1 万个分区时总时间已经超过 1 小时；Presto 稍好一些在 10 万个分区时才超过 1 小时；DR + Parquet 在 10 万个分区时的耗时是 450 秒（得益于并发执行 LIST 请求）；DR + Delta Lake 在 1 百万分区时的耗时才 108 秒，如果启用了本地缓存可以进一步缩短到 17 秒，可以看出来优化效果非常明显。这个结果也基本符合预期，毕竟 Delta Lake 主要目标之一就是优化 list 的性能（以及一致性），对象存储在元数据性能上肯定没有优势。</p>

<p>接下来是更接近真实场景的 TPC-DS 测试，数据集大小是 1 TB，测试结果取的是 3 次运行时间的平均值。最后的数据是 Presto + Parquet 耗时 3.76 小时，社区版 Spark + Parquet 耗时 1.44 小时，DR + Parquet 耗时 0.99 小时，DR + Delta Lake 耗时 0.93 小时。DR + Parquet 相比社区版 Spark 快的主要原因是 DR 做了很多运行时和执行计划的优化，相比之下 DR + Delta Lake 并没有比直接读取 Parquet 提升太多，论文中的解释是 TPC-DS 的表分区都不大，不能完全体现 Delta Lake 的优势。</p>

<p>总结一下，Delta Lake 的思想其实并不复杂，也是工业界为了解决对象存储诸多问题的一种尝试，虽然并不能完全解决（比如原子重命名和删除）。在大数据存储上实现 ACID 这一点对于构建实时数仓至关重要，Delta Lake 通过一种简单统一的方式实现了这个需求，而不用像传统的 <a href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda 架构</a>一样再单独部署一套存储系统（比如 HBase、Kudu）。但现在流式计算领域的风头已经从 Spark 逐渐转向了 Flink，像 Delta Lake 这种只对 Spark 支持的技术在某种程度上也会限制它的普及，相比之下 Iceberg 和 Hudi 似乎更有竞争力。</p>

<h2>Delta Engine: High Performance Query Engine for Delta Lake</h2>

<p><a href="https://www.youtube.com/watch?v=o54YMz8zvCY">[链接]</a></p>

<p>前面介绍了 Delta Lake，算是 Databricks 今年一个重量级的开源产品，但其实真正的杀手锏并没有开放出来，也就是这里要介绍的 Delta Engine。简单介绍这是一个在 Delta Lake 之上，基于 Spark 3.0 的计算引擎。Delta Engine 主要包含 3 部分：原生执行引擎（Native Execution Engine），查询优化器（Query Optimizer）以及缓存（Caching）。这个视频重点介绍了原生执行引擎，这个引擎的代号是 Photon，它使用 C++ 编写，并且实现了目前在 OLAP 领域很火的向量化（vectorization）功能，感兴趣的同学强烈建议阅读 <a href="http://cidrdb.org/cidr2005/papers/P19.pdf">MonetDB/X100: Hyper-Pipelining Query Execution</a> 这篇论文，Databricks 厉害的地方在于是跟论文作者 Peter Boncz 一起合作设计。在 30 TB 的 TPC-DS 测试中，Photon 带来了 3.3 倍的性能提升。关于查询优化器以及缓存功能的介绍可以参考 Delta Engine 的<a href="https://docs.databricks.com/delta/optimizations/index.html">文档</a>。</p>

<h2>A Thorough Comparison of Delta Lake, Iceberg and Hudi</h2>

<p><a href="https://databricks.com/session_na20/a-thorough-comparison-of-delta-lake-iceberg-and-hudi">[链接]</a></p>

<p>Iceberg 和 Hudi 是另外两个会经常拿来跟 Delta Lake 做比较的对象，Iceberg 是 Netflix 开源，而 Hudi 是 Uber 开源。它们之间有着诸多相似之处，又有着很多截然不同的设计思想。这个视频来自腾讯云数据湖团队的陈俊杰，比较系统地对比了这 3 种技术。相对来说 Iceberg 的设计是这 3 个里面最中立的，不跟某种特定的格式和引擎绑定，这也是腾讯选择 Iceberg 的原因之一，具体可以看<a href="https://www.infoq.cn/article/59lbbuvcrzlusmdowjbb">「为什么腾讯看好 Apache Iceberg？」</a>这篇文章。</p>

<h2>Bringing HPC Techniques to Deep Learning</h2>

<p><a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce">[链接]</a></p>

<p>深度学习的核心之一是 <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>（Stochastic Gradient Descent），通过把数据集拆分成若干小的集合（mini-batch），再基于这些小集合反复进行前向传播（forward propagation）和反向传播（backpropagation）计算，不断获取新的梯度（gradient）和权重（weight）。分布式训练本质上要解决的问题就是怎么让多机计算的效率线性提升，即所谓的「线性加速比」，理论值当然是 100%，但是实际情况往往差了很多。传统的同步 SGD 在每一轮计算完以后需要把所有梯度汇总，再重新计算新的权重，类似一个 MapReduce 的过程，此时 reducer 需要等待所有 mapper 计算完成，计算性能会随着 mapper 数量的增加而线性下降。怎么解决这个问题呢？这篇 2017 年的旧文介绍的便是影响至今的 Ring Allreduce 算法，作者 Andrew Gibiansky 之前在百度硅谷 AI 实验室工作，后来联合创办了语音合成公司 <a href="https://www.voicery.com">Voicery</a>（不过悲剧地发现这家公司今年 10 月份已经关了）。基于 Andrew Gibiansky 的成果，Uber 开源了目前公认的 Ring Allreduce 标准框架 <a href="https://github.com/uber/horovod">Horovod</a>。</p>

<h2>Introducing TensorFlow Recommenders</h2>

<p><a href="https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html">[链接]</a></p>

<p>推荐系统是一直都是机器学习一个重要的应用领域，如果你不了解什么是推荐系统可以看我之前写的<a href="https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1/">一篇简介</a>。使用 TensorFlow 可以很方便地训练一个推荐系统模型，不管是召回模型还是排序模型。现在 TensorFlow 官方将这个流程进一步简化，推出了 TensorFlow Recommenders（TFRS）库，旨在让训练、评估、serving 推荐系统模型更加容易，并且融合一些 Google 自己的经验，对于初学者来说会是一个好的入门指南。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[环球易购数据平台如何做到既提速又省钱？]]></title>
    <link href="https://blog.xiaogaozi.org/2020/11/03/globalegrow-big-data-platform-user-case/"/>
    <updated>2020-11-03T11:58:51+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/11/03/globalegrow-big-data-platform-user-case</id>
    <content type="html"><![CDATA[<blockquote><p>这篇文章最初发表在 JuiceFS 官方博客，点击<a href="https://juicefs.com/blog/cn/posts/globalegrow-big-data-platform-user-case">这里</a>查看原文。</p></blockquote>

<!-- more -->


<h2>客户简介</h2>

<p><a href="https://www.globalegrow.com">环球易购</a>创建于 2007 年，致力于打造惠通全球的 B2C 跨境电商新零售生态，2014 年通过与百圆裤业并购完成上市，上市公司「跨境通（SZ002640）」是 A 股上市跨境电商第一股。经过多年的努力，在海外市场建立了广阔的销售网络，得到了美国、欧洲等多国客户的广泛认可，公司业务多年来一直保持着 100% 的增长速度。</p>

<h2>数据平台现状及需求</h2>

<p>环球易购提供面向全球的跨境电商服务，选择 AWS 作为云服务商。基于 EC2 和 EBS 自建 CDH 集群，计算引擎使用了 Hive 和 Spark。当时的环球易购大数据平台面临这么几个问题：</p>

<ul>
<li>基于 EBS 搭建的 HDFS 集群成本很高</li>
<li>Hadoop 集群缺乏弹性伸缩能力</li>
</ul>


<p>因此希望能够在降低 HDFS 存储成本的同时，不会在性能上造成太大损失。说到降低成本那么很自然地会联想到 S3，S3 在提供高达 11 个 9 的数据持久性的同时也能够做到足够低廉的存储成本。但是大数据集群存储由 HDFS 迁移到 S3 是唯一选择么？迁移和使用中会遇到哪些问题呢？这些我们在后面都会详细介绍，不过首先来看看为什么 EBS 自建的 HDFS 集群成本很高。</p>

<h2>云上自建 HDFS 的痛点</h2>

<p>EBS 是一种易于使用的高性能数据块存储服务，通过挂载到 EC2 上来提供近乎无限容量的存储空间。为了保证 EBS 上数据的可用性，所有数据都会自动在同一可用区内进行复制，防止数据丢失。</p>

<p>HDFS 是目前大数据领域最常使用的分布式文件系统，每个文件由一系列的数据块组成。同样的，为了保证数据的可用性，HDFS 默认会将这些数据块自动复制到集群中的多个节点上，例如当设置副本数为 3 时同一数据块在集群中将会有 3 份拷贝。</p>

<p>通过以上介绍可以看到 EBS 和 HDFS 都会通过复制数据来保证可用性，区别在于 EBS 是只针对每块存储卷（即磁盘）的数据进行复制，而 HDFS 是针对整个集群的数据。这种双重冗余的机制其实有些多余，也变相增加了存储成本。同时 HDFS 的多副本特性使得集群的实际可用容量会小很多，例如当副本数为 3 时实际可用容量其实只有总磁盘空间大小的 1/3，再加上通常会在集群空间到达一定水位时就进行扩容，这会进一步压缩可用容量。<strong>基于以上原因，在云上通过 EBS 自建 HDFS 集群的存储成本通常会高达￥1000/TB/月。</strong></p>

<h2>从 HDFS 迁移到 S3 我们需要考虑什么？</h2>

<p>Hadoop 社区版默认已经支持从 S3 读写数据，即通常所说的「S3A」。但是如果你去看 S3A 的<a href="http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#Warnings">官方文档</a>，会在最开始看到几个大大的警告，里面列举了一些类 S3 的对象存储都会存在的问题。</p>

<h3>一致性模型（Consistency Model）</h3>

<p>S3 的一致性模型是<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">最终一致性</a>，也就是说当创建了一个新文件以后，并不一定能立即看到它；当对一个文件执行删除或者更新操作后，有可能还是会读到旧的数据。这些一致性问题会导致程序崩溃，比如常见的 <code>java.io.FileNotFoundException</code>，也可能导致错误的计算结果，更麻烦的是这种错误很难发现。我们在测试过程中就因为 S3 的一致性问题使得执行 DistCp 任务频繁报错，导致数据迁移受到严重影响。</p>

<h3>没有真实的目录</h3>

<p>S3 中的「目录」其实是通过对象名称的前缀模拟出来的，因此它并不等价于通常我们在 HDFS 中见到的目录。例如当遍历一个目录时，S3 的实现是搜索具有相同前缀的对象。这会导致几个比较严重的问题：</p>

<ul>
<li><strong>遍历目录可能会很慢。</strong>遍历的时间复杂度取决于目录中的总文件数。</li>
<li><strong>重命名目录也可能会很慢。</strong>跟遍历目录一样，总文件数是影响性能的重要因素。同时 S3 重命名一个文件其实是先拷贝到新路径，再删除原始文件，这个过程也是比较耗时的。</li>
<li><strong>重命名或者删除目录不是原子操作。</strong>HDFS 上只需要 O(1) 的操作，在 S3 上变成了 O(n)。如果操作过程中任务失败，将会导致数据变成一个不可知的中间状态。</li>
</ul>


<h3>认证模型（Authorization Model）</h3>

<p>S3 的认证模型是在 S3 服务内部基于 IAM 实现的，这区别于传统的文件系统。因此当通过 Hadoop 访问 S3 时会看到文件的 owner 和 group 会随着当前用户的身份而动态变化，文件的权限都是 666，而目录的权限都是 777。这种与 HDFS 大相径庭的认证模型会使得权限管理复杂化，并且也显得不够通用，只能限定在 AWS 内使用。</p>

<h2>JuiceFS 带来了什么？</h2>

<p>JuiceFS 基于对象存储实现了一个<strong>强一致性的分布式文件系统</strong>，一方面保持了 S3 弹性伸缩无限容量，99.999999999% 的数据持久性安全特性，另一方面前面提到的 S3 的种种「问题」都能完美解决。同时 JuiceFS 完整兼容 Hadoop 生态的各种组件，对于用户来说可以做到无缝接入。认证模型上 JuiceFS 遵循与 HDFS 类似的 user/group 权限控制方式，保证数据的安全性，也能对接 Hadoop 生态中常用的如 Kerberos、Ranger、Sentry 这些组件。更加重要的是，相比环球易购现有的基于 EBS 的存储方案，使用 JuiceFS 以后<strong>每 TB 每月的存储成本将会至少节省 70%</strong>。</p>

<p>存储成本大幅下降的同时，性能表现又如何呢？下面分享一下相关的测试结果。</p>

<h2>测试结果</h2>

<p>测试环境是 AWS 上自建的 CDH 集群，CDH 版本为 5.8.5。测试的计算引擎包括 Hive 和 Spark，数据格式包括纯文本和 ORC，使用 TPC-DS 20G 和 100G 这两个规模的数据集。对比的存储系统有 S3A、HDFS 及 JuiceFS。</p>

<h3>创建表</h3>

<p><img src="https://blog.xiaogaozi.org/images/posts/globalegrow-create-table.png" alt="" />
这里以创建 <code>store_sales</code> 这个分区表为例</p>

<h3>修复表分区</h3>

<p><img src="https://blog.xiaogaozi.org/images/posts/globalegrow-repair-table.png" alt="" />
这里以修复 <code>store_sales</code> 这个表的分区为例</p>

<h3>写入数据</h3>

<p><img src="https://blog.xiaogaozi.org/images/posts/globalegrow-insert-table.png" alt="" />
这里以读取 <code>store_sales</code> 这个分区表并插入临时表为例</p>

<h3>读取纯文本格式数据</h3>

<p><img src="https://blog.xiaogaozi.org/images/posts/globalegrow-text-20g.png" alt="" />
<img src="https://blog.xiaogaozi.org/images/posts/globalegrow-text-100g.png" alt="" />
分别使用 Spark 测试了 20G 和 100G 这两个数据集，取 TPC-DS 前 10 个查询，数据格式为纯文本。</p>

<h3>读取 ORC 格式数据</h3>

<p><img src="https://blog.xiaogaozi.org/images/posts/globalegrow-orc-20g.png" alt="" />
<img src="https://blog.xiaogaozi.org/images/posts/globalegrow-orc-100g.png" alt="" />
分别使用 Spark 测试了 20G 和 100G 这两个数据集，取 TPC-DS 前 10 个查询，数据格式为 ORC。</p>

<h3>测试结果总结</h3>

<p>对于建表和修复表分区这样的操作，因为依赖对底层元数据的频繁访问（例如遍历目录），JuiceFS 的性能大幅领先于 S3A，<strong>最多有 60 倍的性能提升</strong>。</p>

<p>在写入数据的场景，JuiceFS 的性能相对于 S3A 有 5 倍的提升。这对于 ETL 类型的任务来说非常重要，通常 ETL 任务都会涉及多个临时表的生成和销毁，这个过程会产生大量的元数据操作（例如重命名、删除）。</p>

<p>当读取类似 ORC 这种列式存储格式的数据时，区别于纯文本文件的顺序读取模式，列式存储格式会产生很多随机访问，JuiceFS 的性能再次大幅领先 S3A，<strong>最高可达 63 倍</strong>。同时相比于 HDFS，JuiceFS 也能有最多 2 倍的性能提升。</p>

<h2>数据迁移</h2>

<p>环球易购的大数据平台经过长期的发展已经积攒大量的数据和业务，怎么从现有方案迁移到新的方案也是评估新方案是否合适的重要因素。在这方面，JuiceFS 提供了多种数据迁移方式：</p>

<ul>
<li><strong>将数据拷贝到 JuiceFS。</strong>这种方式的读取性能最好，可以高效地利用本地磁盘缓存和分布式缓存，也能保证数据的强一致性。但是涉及数据拷贝，因此迁移成本比较高。</li>
<li><strong>通过 import 命令将 S3 的数据导入。</strong>这种方式只涉及元数据的导入，将 S3 上面的对象导入到 JuiceFS 的目录树。这种方式无需拷贝数据，迁移速度快。但是没有办法保证强一致性，并且不能利用缓存加速功能。</li>
<li><strong>通过符号链接将已有数据和新数据融合到一起。</strong>JuiceFS 不仅可以在文件系统内部建立符号链接，也可以跨文件系统建立符号链接。例如通过 <code>ln -s hdfs://dir /jfs/hdfs_dir</code> 这行命令可以创建一个指向 HDFS 的符号链接。基于这种方式，可以将历史数据直接链接到 JuiceFS 中，然后通过统一的 JuiceFS 命名空间访问其它所有 Hadoop 文件系统。</li>
</ul>


<h2>选择</h2>

<p>结合测试结果以及综合成本分析，全面对比了 HDFS、S3 和 JuiceFS 的方案，环球易购认为 JuiceFS 相比另外两个方案有显著的性能和成本优势，决定用 JuiceFS 替换自建的 HDFS。这些优势具体体现为以下 3 个方面：</p>

<p>首先，JuiceFS 可以实现从 HDFS 的平滑迁移，对上游的计算引擎可以做到全面兼容，对现有的权限管理体系可以保持一致，同时性能上没有任何下降。这几点对数据平台的迁移可以说是至关重要的，没有这样的基础，数据平台的迁移将是一场耗时耗力的战役。<strong>而有了这样的基础，客户只用不到一个月的时间就完成了业务和数据的迁移。</strong></p>

<p>第二，在成本方面，「云上自建 HDFS 的痛点」一节中已经有过说明，基于 EBS 自建 HDFS 单独计算磁盘成本就大约有￥1000/TB/月，而 JuiceFS 仅为 27%。这还不是 TCO 成本，TCO 还应该包括 HDFS 所消耗的 CPU、内存、运维管理投入的人力成本，按经验值来说至少翻倍。而 JuiceFS 客户使用全托管服务，没有任何运维管理的投入。<strong>这样从 TCO 角度看，可以节省近 90% 的成本。</strong></p>

<p>最后，也是最重要的一点。大数据平台的存储引擎从 HDFS 换成 JuiceFS 后，整个平台就实现了存储计算分离，在 <a href="https://juicefs.com/blog/cn/posts/why-disaggregated-compute-and-storage-is-future">「为什么说存储和计算分离的架构才是未来？」</a> 一文中详细分析了存储计算耦合的痛点，以及业界的一些实践。现在 JuiceFS 作为完全兼容 HDFS 的云原生文件系统，已经是 基于 Hadoop 生态构建的大数据平台的完美存储方案。存储计算分离是大数据平台弹性伸缩的基础，这一步的改造对环球易购数据平台的架构设计来说也有着重要的意义，接下来环球易购的数据团队将深入到集群弹性伸缩、工作负载混合部署等研究和实践中。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #6]]></title>
    <link href="https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6/"/>
    <updated>2020-09-15T18:54:00+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Presto: SQL on Everything</h2>

<p><a href="https://prestosql.io/Presto_SQL_on_Everything.pdf">[链接]</a></p>

<p>Presto 是 Facebook 2012 年开始开发并于 2013 年开源的分布式查询引擎。和<a href="https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3/">第 3 期</a>介绍的 Kudu 一样，主要应用在 OLAP 场景，但跟 Kudu 不一样的地方是，Presto 仅仅是一个查询引擎，并不负责数据存储。这也是论文标题「SQL on Everything」的含义，这里的「Everything」指代的即是任意类型的存储，比如 HDFS、MySQL 等。</p>

<p>论文开篇先总结了 Presto 几个值得关注的特点：</p>

<ul>
<li>Presto 是一个自适应的多租户系统（adaptive multi-tenant system），可以很容易扩展到上千节点的同时，还能有效利用集群资源。这里的「自适应」很重要，是将 Presto 和其它系统进行比较的要点之一。</li>
<li>Presto 可以很方便地和多种数据源进行集成，甚至在 1 条查询语句里就可以同时查询多个数据源。Presto 通过连接器（connector）的概念统一底层存储的访问。</li>
<li>通过不同的配置可以让 Presto 同时适配不同的场景。关于这一点在后续介绍 Facebook 的查询场景时也有体现。</li>
<li>Presto 通过很多关键特性实现了一个高性能的查询引擎。多个并行的查询在同一个 JVM 中运行，虽然可以降低响应时间，但同时也需要在调度、资源管理、隔离这些方面特别注意。</li>
</ul>


<p>接下来介绍 Facebook 目前使用 Presto 的几个主要场景：</p>

<ul>
<li><strong>交互式分析（Interactive Analytics）</strong>：这是将 Facebook 数据仓库作为数据源的查询场景。这个场景通常查询的数据量较小，压缩后大概 50GB-3TB 左右。单集群需要支持的并发查询量在 50-100 左右，秒级或者分钟级返回结果。用户对于查询时间非常敏感，但同时对于查询所需的资源量没有特别精确的判断。在进行探索式分析时，用户通常不需要返回所有结果集，只要有初步的结果或者满足 <code>LIMIT</code> 的限制整个查询就可以提前终止。</li>
<li><strong>批量 ETL（Batch ETL）</strong>：这个场景的用户一般是数据工程师，目前已经是 Facebook 内部一个很大的 Presto 应用场景。相比交互式查询，ETL 需要的资源也更多，不管是 CPU 还是内存，特别是当涉及到聚合或者 join 很多大表的时候。在这个场景查询时间反而没有那么重要，更加重要的是资源利用率和整体集群的吞吐。</li>
<li><strong>A/B 测试（A/B Testing）</strong>：为了满足用户对产品验证越来越快的需求，A/B 测试的结果需要在小时级（而不是天级）内得到，并保证数据完整且精确。当用户需要进行更深层次的分析时，查询结果需要在 5-30 秒左右返回。很难通过预聚合（pre-aggregating）的方式满足这些查询需求，因此必须通过在线计算来解决。查询会涉及到 join 多个大的数据集，同时查询语句的特征是相对固定的。</li>
<li><strong>开发者／广告主分析（Developer/Advertiser Analytics）</strong>：这是面向外部开发者或者广告主的分析场景，比如 <a href="https://analytics.facebook.com">Facebook Analytics</a>。同 A/B 测试场景一样，这个场景的查询语句特征也是相对固定的。虽然总的数据规模很大，但是用户查询时因为会限制在他自己的数据里相对来说查询量会小很多。数据接入（data ingestion）的时延大概是分钟级，查询时延需要严格限定在 50 毫秒到 5 秒左右。因为应用在外部商业产品，Presto 集群的可用性需要保证在 99.999%，并且支持上百并发查询。</li>
</ul>


<p>以上这些场景可能除了 ETL 以外，也是目前很多公司使用 Presto 的主要场景，总的来说主要还是应用在交互式查询上。</p>

<p>然后是 Presto 整体的架构介绍，集群分为两种类型的节点：coordinator 和 worker。Coordinator 节点负责解析、规划以及优化查询，通常只会有 1 个。Worker 节点负责处理查询请求，根据集群规模可以横向扩展。</p>

<p>当客户端通过 HTTP 请求将 SQL 发送给 coordinator 时，经过解析和分析，coordinator 会生成一个分布式执行计划（distributed execution plan）。这个执行计划由多个 stage 连接而成，类似一个 DAG 的形式。因为这是一个分布式执行计划，stage 会被分发到不同的 worker，因此 stage 之间需要通过 shuffle 来交换数据。每个 stage 内部由多个 task 组成，一个 task 可以被看作一个处理单元（processing unit）。Task 内又由多个 pipeline 构成，一个 pipeline 内包含一系列的 operator。到这里，operator 已经是最小的处理单位，通常只负责某一类单一计算任务。</p>

<p>Coordinator 很大一部分工作是负责调度，调度分为三个维度：stage、task 和 split。Stage 调度决定 stage 的执行顺序；task 调度决定多少任务需要被调度以及应该分配给哪些 worker；split 调度决定 split 会被分配给哪些任务（关于 split 这个概念后面会详细介绍）。</p>

<p>调度 stage 分为两种策略：all-at-once 和 phased。All-at-once 很好理解就是所有 stage 并行执行，这个策略可以最大化执行效率，适合时延敏感的场景（如交互式分析）。而 phased 策略就是只并行执行那些强关联的组件，整体任务分阶段执行，这个策略可以有效降低内存占用，适合 ETL 场景。</p>

<p>当 stage 调度成功，coordinator 即会开始分配 task。任务调度器将 stage 分为两类：leaf 和 intermediate。Leaf stage 负责从连接器中读取数据，intermediate stage 负责处理来自其它 stage 的中间结果。对于 leaf stage，任务调度器会根据如网络拓扑、数据本地性这些因素来决定应该把 task 分配给哪些 worker 节点，这个过程依赖连接器实现的 Data Layout API。如果没有任何限制，Presto 倾向于把 leaf stage 的任务分散到整个集群，以加快数据读取效率。Intermediate stage 的任务可以被分配到任意节点上，但是调度器仍然需要决定当前每个 stage 有多少任务需要被调度，且这个任务数是可以在运行时动态调整的。</p>

<p>当 leaf stage 的任务分配好以后，这个 worker 节点便可以开始接收来自 coordinator 分配的 split。Split 是对底层数据的逻辑封装，例如底层存储是 HDFS，那一个 split 通常包含的信息有文件路径、文件偏移等。Leaf stage 的任务必须至少分配一个 split 才能开始运行，而 intermediate stage 的任务是一直可运行的。Split 的创建由连接器负责，并且懒分配给 leaf stage 的任务，也就是说并不会等到所有 split 都创建完毕。这样做有几个好处：</p>

<ul>
<li>将连接器创建 split 的时间从查询中解耦。某些连接器（如 Hive）可能需要花费很长时间去遍历分区和 list 文件。</li>
<li>查询可以尽快开始执行而不用等到所有数据处理完毕。在交互式分析场景很有可能查询会被提前中断。</li>
<li>每个 worker 维护了一个 split 的队列，coordinator 分配 split 时会优先选择队列长度最短的节点。</li>
<li>不用一次保存所有 split 的元信息。对于 Hive 连接器来说很有可能会产生上百万个 split，这会直接导致 coordinator 内存不足。</li>
</ul>


<p>介绍完了 coordinator 的工作接下来就是 worker。前面已经提到最小的执行单位是 operator，operator 负责处理输入数据，同时输出处理完的数据。Operator 输入输出的数据单元叫做 page，一个 page 是连接器将 split 中的多行数据转为列式存储以后产生的数据结构。Shuffle 也是 worker 的主要工作之一，区别于传统的 Hadoop 组件，Presto 是基于全内存的 shuffle 实现，这也是 Presto 性能更优的原因之一。Shuffle 的数据会暂存在内存缓冲区（buffer）中，简单理解 map 端的缓冲区为输出缓冲区，reduce 端的为输入缓冲区。这两个缓冲区都是有容量限制的，会根据数据消费的速率动态调整生产速率，确保整体任务的稳定性以及多租户之间的公平性。当输出缓冲区容量持续偏高时，Presto 会减少可消费的 split 数量。输入缓冲区这端会有一个类似 TCP 滑动窗口的策略动态控制上游的生产速率。</p>

<p>回顾开篇总结的 Presto 特点，其中很重要的一个是<strong>自适应的多租户场景</strong>，上一段落介绍 shuffle 缓冲区的时候其实已经涉及到部分针对性的优化。本质上资源管理需要考虑的就是 CPU 和内存这两种资源，Presto 分别都有不同的解决方案。</p>

<p>CPU 调度场景每个 split 都会有一个允许在一个线程上一秒内执行的最大 quanta，当 quanta 超出时这个 split 将会被放回队列释放线程给其它 split。当输出缓冲区满（下游消费慢）、输入缓冲区空（上游生产慢）或者集群内存紧张时，即使 quanta 没使用完调度器也会强行切换任务。这个基于 quanta 的调度策略使得 Presto 能够最大化 CPU 资源的利用率。当线程被释放应该如何挑选下一个运行的任务呢？Presto 建立了一个 5 级的反馈队列（feedback queue），每个等级都分配了一个可配置的 CPU 时间比例。随着一个任务使用的 CPU 时间不断累积，这个任务会移动到更高等级的队列。也就是说 Presto 倾向于优先执行那些「快」的任务，因为用户期望轻的查询尽快完成，而对于那些重的查询所需的时间不太敏感。</p>

<p>内存管理是一个比 CPU 更复杂的场景。Presto 将内存分为两种类别：用户（user）和系统（system），并分别维护不同的内存池。引擎对用户内存和总内存（用户 + 系统）都有不同的限制，超过全局（所有 worker 聚合以后）或者单节点内存限制的任务将会被强行杀掉。虽然有全局的内存限制，但是为了满足并行执行多个任务的需求通常还是会超卖（overcommit）内存，即使真的出现部分节点内存耗尽的情况，Presto 也提供了两种机制去确保整体集群的稳定性。这两种机制分别是：spilling 和预留内存池（reserved pool）。Spilling 其实就是在节点内存耗尽时按照任务的执行时间升序排列，依次把内存中的状态写到本地磁盘。不过 Facebook 内部并没有开启这个特性，因为集群资源（TB 级的内存）足够支撑用户的使用场景，全内存计算也更加能保证查询的执行时间。如果没有开启 spilling 特性，那 Presto 将会采用预留内存池的策略。这个策略的大意是把内存池分为通用（general）和预留（reserved）两种，当一个 worker 节点的通用内存池耗尽时将会把这个节点上占用最多内存的查询「晋升」到预留内存池，整个集群同一时间只允许一个查询晋升。后续的内存申请会优先满足这个晋升的查询，直到它执行完毕。这个策略当然会影响整体集群的效率，因此用户也可以选择直接杀掉查询。</p>

<p>最后是容错（fault tolerance）。作为一个多租户的分布式系统，优良的容错性是一个必不可少的需求。但是遗憾的是在这一点上 Presto 做得并不好，coordinator 依然是单点（一个题外话，Starburst Data 这家提供商业 Presto 版本的公司<a href="https://docs.starburstdata.com/latest/aws/high-availability.html">支持 coordinator HA</a>），worker 宕机将会导致所有运行在这个节点上的查询失败（社区有<a href="https://github.com/prestodb/presto/issues/9855">一</a><a href="https://github.com/prestosql/presto/issues/455">些</a> issue 但是目前没有进展），Presto 非常依赖客户端自己去重试。Facebook 内部是通过外部的编排系统来确保集群的可用性，对于交互式分析和 ETL 场景有 standby 的 coordinator，A/B 测试和开发者／广告主分析场景部署了多活（multiple active）集群。监控系统将会识别不可用的节点自动从集群中移除，并在之后再重新加入集群。</p>

<p>在开发 Presto 的过程中，作者也总结了一些工程上的经验：</p>

<ul>
<li><strong>自适应胜过手动调优（Adaptiveness over configurability）</strong>：前面已经介绍了很多 Presto 自适应的特性，作者认为当面对一个多租户场景，且查询的特征千变万化的时候，自适应显得尤为重要。否则就需要人工去针对性地手动调优，这种方式在面对大规模的查询场景时是没法扩展的。</li>
<li><strong>非常轻松地监控（Effortless instrumentation）</strong>：Presto 作者相信可观察（observable）的系统设计是非常重要的，要允许工程师去了解和优化自己代码的性能。Presto 每个 worker 平均导出了约 10000 个监控指标，粒度细到了 operator 级别，并会聚合到 task 和 stage 级别。</li>
<li><strong>静态配置（Static configuration）</strong>：错误的配置可能会对系统性能造成非常大的影响，为了保证时刻对系统整体状态有一个清晰的了解，Presto 作者选择使用静态配置的方案而不是动态配置。</li>
<li><strong>垂直集成（Vertical integration）</strong>：这其实是一个要不要重复造轮子的问题，对于一个大型项目来说肯定会依赖很多基础库，那什么时候选择用开源实现，什么时候选择自研是一个需要认真思考的问题（当然类似 Google 这种只考虑自研的公司就没有这个困扰了）。Presto 作者倾向于在那些对性能和效率要求比较高的场景选择自研。</li>
</ul>


<p>最后是一个八卦。Presto 最早是由一批 Facebook 的员工开发，2018 年这批员工中的部分核心离职，全职建设 Presto 开源社区。2019 年 1 月 31 日<a href="http://www.prweb.com/releases/presto_software_foundation_launches_to_advance_presto_open_source_community/prweb16070792.htm">成立</a>「Presto Software Foundation」，并在 GitHub 上创建了新的组织 <a href="https://github.com/prestosql">PrestoSQL</a>。有趣的是在 2019 年 9 月 23 日 Facebook 联合多家公司<a href="https://www.linuxfoundation.org/press-release/2019/09/facebook-uber-twitter-and-alibaba-form-presto-foundation-to-tackle-distributed-data-processing-at-scale">成立</a>了一个新的基金会叫「Presto Foundation」，在 GitHub 上的组织叫 <a href="https://github.com/prestodb">PrestoDB</a>。按照 Presto 作者的<a href="https://github.com/prestosql/presto/issues/380">说法</a>，他们在成立 Presto Software Foundation 之后其实是有邀请过 Facebook 加入的，但是显然对方拒绝了这个邀请。于是你会发现目前在开源社区有两个版本的 Presto，并且项目名是一样的，不过为了便于区分一般还是分别叫做 PrestoSQL 和 PrestoDB。前者背后的商业公司主要是 Starburst Data，这家公司的 3 个 CTO 同时也是 Presto 的原始作者（是的，这家公司有 3 个 CTO）；后者背后的商业公司有 Facebook、Uber、Twitter、阿里巴巴、Alluxio 和 Ahana。为了不至于让用户混淆，Starburst Data 还在官网<a href="https://www.starburstdata.com/prestosql-and-prestodb">比较</a>了这两个版本的 Presto。目前公有云厂商提供的产品中，<a href="https://docs.aws.amazon.com/athena/latest/ug/presto-functions.html">AWS Athena</a>、<a href="https://help.aliyun.com/document_detail/169871.html">阿里云 DLA</a> 都是基于 PrestoDB 开发的 serverless 产品，<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html">AWS EMR</a> 两种 Presto 都支持，<a href="https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5">Google Dataproc 1.5</a>、<a href="https://help.aliyun.com/document_detail/132036.html?#title-fm0-jq8-sog">阿里云 EMR 3.25.0</a> 以后默认集成的是 PrestoSQL，<a href="https://cloud.tencent.com/document/product/589/20279">腾讯云 EMR</a> 默认集成的是 PrestoDB。</p>

<h2>Spark Architecture: Shuffle</h2>

<p><a href="https://0x0fff.com/spark-architecture-shuffle">[链接]</a></p>

<p>要理解什么是 shuffle 就得先了解什么是 MapReduce，自从 2004 年 Google 那篇惊世骇俗的介绍 MapReduce 的<a href="https://research.google/pubs/pub62">论文</a>发表以来，大数据的生态就被彻底改变了（并沿用至今）。基于这样一个简单的编程模型实现了各种复杂的计算逻辑，但也存在一些「问题」，shuffle 就是其中一个。当 map 任务完成以后，数据需要根据 partition 策略重新分配到不同的 reduce 任务中，这个过程即称为 shuffle。这篇文章详细介绍了 Spark 历史上各种 shuffle 方案是怎么实现的。</p>

<h2>Cosco: An Efficient Facebook-Scale Shuffle Service</h2>

<p><a href="https://databricks.com/session/cosco-an-efficient-facebook-scale-shuffle-service">[链接]</a></p>

<p>接上一篇文章，这是 Facebook 在 2018 年的 Spark+AI Summit 上的一个分享，介绍了他们实现的一个外部 shuffle 服务 Cosco，可以同时用于 Hive 和 Spark 任务。当时已经在 90%+ 的 Hive 任务上使用，并在生产环境运行 1 年以上，Spark 任务也在逐渐推广中。为什么要开发一个外部 shuffle 服务呢？Facebook 列举了一些他们当时面临的问题，比如单次 shuffle 需要交换的数据量级是 PiB 级，总共有 10 万个 mapper、1 万个 reducer，3 倍的写放大（shuffle 1 PiB 的数据实际要写 3 PiB 到磁盘），平均 IO 大小只有 200 KiB。这些都是促使他们开发 Cosco 的原因（当然不是所有公司都会遇到），另一个好处是 executor 变成了无状态，对于动态伸缩更加友好。如果对 Cosco 有兴趣还可以继续看一看他们在 2019 年的 Spark+AI Summit 上做的<a href="https://databricks.com/session_na20/flash-for-apache-spark-shuffle-with-cosco">后续分享</a>。</p>

<h2>Federated Learning: Collaborative Machine Learning without Centralized Training Data</h2>

<p><a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">[链接]</a></p>

<p>传统机器学习中的优化算法（例如 <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>）是将大规模数据集分布式运行在多个节点上，这需要低延时、高吞吐地读取训练数据，因此数据一般都是提前收集到一个中心化存储里。但是在某些场景并不适合这样做，不管是因为数据量太大不易收集，还是出于数据隐私的考虑。因此 Google 提出了联邦学习（Federated Learning）的概念，这个词源于发表在 2017 年 AISTATS 会议上的一篇论文 <a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>。联邦学习的大体思想就是在数据的生产端（例如你的手机）直接进行模型训练，经过汇总以后把对模型的更新数据发送到服务端，服务端再把其它客户端上传的更新数据一起汇总生成一个新的模型，最后下发这个新模型到所有客户端。可以看到整个过程中训练数据依然保留在客户端，并不需要上传。如果你在 Android 系统中使用 Gboard 这个 app，那其实你已经参与到联邦学习的过程中了，当然只会在当你的手机空闲并且连接电源和 Wi-Fi 的时候才会进行。</p>

<h2>分布式文件系统架构对比</h2>

<p><a href="https://juicefs.com/blog/cn/posts/distributed-filesystem-comparison">[链接]</a></p>

<p>2003 年 Google 发表了 <a href="https://research.google/pubs/pub51">The Google File System</a> 论文，就像前面提到的 MapReduce 一样，从此对业界产生了非常深远的影响。这篇博客梳理了 GlusterFS、CephFS、GFS、HDFS、MooseFS 和 <a href="https://juicefs.com">JuiceFS</a> 这几个分布式文件系统的架构设计。随着网络带宽的发展，在云计算和云原生的大趋势下，总的来说正逐步朝着存储计算分离的方向演进，这对于基础设施的架构也有着一定的要求。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to DL Platform]]></title>
    <link href="https://blog.xiaogaozi.org/2020/08/17/introduction-to-dl-platform/"/>
    <updated>2020-08-17T16:25:32+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/08/17/introduction-to-dl-platform</id>
    <content type="html"><![CDATA[<p>最近在团队内部做了一次关于深度学习平台的分享，内容上一方面来自过去的工作经验，另一方面也有很多过去想做但是由于各种原因没来得及实现的想法。</p>

<!-- more -->




<iframe src="//www.slideshare.net/slideshow/embed_code/key/mux6PLZygjwwrE" width="700" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #5]]></title>
    <link href="https://blog.xiaogaozi.org/2020/07/21/maybe-news-issue-5/"/>
    <updated>2020-07-21T14:08:28+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/07/21/maybe-news-issue-5</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>DynamicEmbedding: Extending TensorFlow for Colossal-Scale Applications</h2>

<p><a href="https://arxiv.org/abs/2004.08366">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1/">第一期</a> Maybe News 中介绍了腾讯提出的解决 TensorFlow 中大规模稀疏特征模型训练的方案，本期的这篇论文来自 Google（准确说是 Google Smart Campaigns 团队）。作为发明 TensorFlow 的公司，Google 内部团队的设计思想值得借鉴。</p>

<p>这个系统被称之为 DynamicEmbedding（DE），名字简单直观，要解决的场景也是很多公司都遇到的如何动态维护 embedding。系统内部分为两个组件：DynamicEmbedding Master（DEM）和 DynamicEmbedding Worker（DEW），合起来叫做 DynamicEmbedding Service（DES）。DEM 负责处理所有客户端请求，包括 embedding 查找（lookup）、更新（update）等。DEW 负责 embedding 存储、梯度更新等，所有请求都来自 DEM。同时新增了几个 TensorFlow API，如 <code>dynamic_embedding_lookup()</code>、<code>compute_sampled_logits()</code>，这些 API 是整个系统的关键入口，任何模型在接入 DES 的时候都需要在特定的地方调用这些 API。以上设计看起来跟大部分公司的方案没有太大差别。</p>

<p>通过实现一个叫做 EmbeddingStore 的通用接口，DEW 后端支持对接多种类型的存储，例如 Protocol Buffers、GFS、Bigtable，比较巧妙地将大规模 embedding 存储时面临的扩展性和稳定性问题转移到了外部存储系统。当然因为多了一次网络请求是否会影响整体的训练效率这点有待商榷，论文中介绍 BigtableEmbedding 时提到会将数据同时存储到本地缓存和远端，猜测这里本地缓存的目的便是为了加速存储操作。</p>

<p>Embedding 更新这一步涉及到一些常用的梯度下降（gradient descent）算法，为了保持一致，DEW 内部实现了跟 TensorFlow 原生提供的优化器（optimizer）同样的逻辑，并且大部分代码是可以复用的。当训练数据时间跨度很大时（如数月或者数年），可能存在很多无效的特征或者一些需要特殊处理的特征。因此 DEW 在每次 embedding 更新的时候会同时统计这个 embedding 的更新频率，通过设定一个恰当的阈值来保证只有部分 embedding 会持久化到存储系统里，那些低频的数据便不会继续保存。除了统计频率这种方法，通过 bloom filter 也可以实现类似的效果。</p>

<p>Serving 的时候因为 embedding 都已经存储到了外部系统，所以 DEW 就没有必要存在了，只需要在本地部署 DEM 负责处理读请求。为了提升推理的性能，本地缓存肯定是少不了的，同时批量处理查询请求也是非常重要的。</p>

<p>实验评估阶段首先比较了和原生 TensorFlow 训练同样的模型、同样的超参是否会有指标上的差异，模型选择的是 Word2Vec，梯度下降算法选择的是 SGD、Adagrad 和 Momentum。从最终训练的 loss 上看几乎没有差别，说明 DE 系统不会对模型质量有影响。</p>

<p>接着测试了字典（dictionary）大小对模型精度（accuracy）的影响，理论上 DE 系统其实是不限定字典大小的，从实验的两个模型 Word2Vec 和 Sparse2Seq 上来看也的确是字典大小越大模型精度越高。</p>

<p>然后是评测模型训练时两个重要的系统指标：集群总的内存占用和每秒训练的 global steps（GSS）。分别测试了三个模型：Word2Vec，Image2Lable 和 Seq2Seq。在使用原生的 TensorFlow 时集群内存占用会随着 worker 数量的增大而显著增长（在 Word2Vec 模型中尤为明显），相比之下 DE 系统的内存占用只跟 embedding 的总大小有关，与 DEW 的数量无关。之所以有这样的差异也是因为原生的 TensorFlow 会在不同 worker 间重复存储 embedding 数据。GSS 的对比上两者的加速比都差不多，但是总体上 DE 还是会更优。</p>

<p>最后论文中详细介绍了 DE 在 Google Smart Campaigns 产品中的一个重要应用：给广告主自动推荐投放的关键词。这是一个叫做 Sparse2Label 的模型，输出即是推荐的关键词（label）。这个模型带来的最大变化是以前需要针对每一种语言训练一个单独的模型，而现在只需要一个模型即可。通过对比一些核心指标（如 CTR），DE 推荐的关键词都明显更好。整个模型也是随着时间不断增长的，截止 2020 年 2 月这个模型的参数量已经达到了 1249 亿个，如果每个参数按 4 字节算的话模型大小差不多为 465 GiB（其实比想象中小）。</p>

<p>另一个更难评估的指标是用户搜索的关键词（query）与广告投放的关键词之间的关联度，很多时候两者之间并不是完全匹配的。作者是通过人工评估 38 万个样本的方式来解决的，每个样本都会有 5 个人类进行打分，分数区间从 -100 到 100，越高越匹配，然后计算这 5 个分数的平均值作为这个样本的最终分数。大于等于 50 分的样本认为是好（good）的样本，小于等于 0 分的则认为是不好（bad）的样本，前者除以后者被称作 GB ratio，这个比率越大越好。每个推荐的关键词都会同时有一个置信值（也就是网页和关键词 embedding 之间的 cosine 距离），从评测结果上来看当这个置信值大于 0.7 时，不好的样本量将会显著减少。实际生产环境收集的数据也印证了 DE 系统推荐的关键词是 GB ratio 最高的。</p>

<p>总结一下 DE 系统解决了原生 TensorFlow 在大规模 embedding 模型训练时效率低下（甚至不可用）的问题，短期内这个系统估计也不会开源或者合并到上游。目前可以期待的还是腾讯的方案，他们已经提交了<a href="https://github.com/tensorflow/tensorflow/pull/41371">代码</a>到 TensorFlow 社区。</p>

<h2>The Next Step for Generics</h2>

<p><a href="https://blog.golang.org/generics-next-step">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2/">第二期</a> Maybe News 中曾经介绍过 Go 语言开发者关于泛型设计的一些思考，近期 Ian Lance Taylor 又和社区同步了一下最新进展。最大的变化就是去掉了 contract 这个新增的概念，改为复用 interface。同时创建了一个新的 <a href="https://go2goplay.golang.org">playground</a>，可以方便大家试验泛型代码。如果最新的这一版设计社区没有太大异议的话，乐观估计将会在 Go 1.17 加入泛型特性，也就是在 2021 年 8 月左右。当然最终实现这个目标还是有很多的不确定性，特别是当前疫情对于全球影响的情况下。</p>

<h2>Fiber: Distributed Computing for AI Made Simple</h2>

<p><a href="https://eng.uber.com/fiberdistributed">[链接]</a></p>

<p>分布式计算在 AI 领域的需求一直都很强烈，但分布式计算不仅仅是将单机迁移到多机这样就足够了，还需要考虑如：易用性（降低用户从单机迁移的成本）、稳定性（自动容错）、弹性伸缩（和底层资源调度层配合）、线性加速（横向扩展多少机器就能带来多少性能提升）。Uber 和 OpenAI 共同开发的 Fiber 框架便是尝试解决以上问题的一个例子，从 Fiber 的<a href="https://arxiv.org/abs/2003.11164">论文</a>能看到这个框架最初设计面向的是强化学习（Reinforcement Learning）场景，在这个领域有很多类似的框架，比如 Google 的 <a href="https://github.com/google/dopamine">Dopamine</a>、Facebook 的 <a href="https://github.com/facebookresearch/ReAgent">ReAgent</a>、UC Berkeley 的 <a href="https://github.com/ray-project/ray">Ray</a>。自动容错和弹性伸缩这两个特性又让我联想到蚂蚁金服的 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a> 和才云的 <a href="https://github.com/caicloud/ftlib">FTLib</a>。</p>

<h2>The impact of slow NFS on data systems</h2>

<p><a href="https://engineering.linkedin.com/blog/2020/the-impact-of-slow-nfs-on-data-systems">[链接]</a></p>

<p>LinkedIn 分享了他们使用 NFS 进行数据库备份时遇到的性能问题，因为备份进程和数据库进程是一起部署的，因此这个问题还间接影响到了在线业务的稳定性。整个问题分析过程清晰易懂，还能顺便复习一下大学里学习的计算机网络和操作系统的一些知识。但问题的根源 NFS 服务的性能为什么这么差还是没有特别好的解决方案，可能在这个场景里 NFS 就不是特别好的选择吧。</p>

<h2>Kubeflow &amp; Kale simplify building better ML Pipelines with automatic hyperparameter tuning</h2>

<p><a href="https://medium.com/kubeflow/kubeflow-kale-simplify-building-better-ml-pipelines-with-automatic-hyperparameter-tuning-5821747f4fcb">[链接]</a></p>

<p>Jupyter Notebooks 是当下数据科学家或者算法工程师日常工作非常重要的一个组件，交互式的界面加上即时的代码运行反馈极大地提升了开发效率。但是如果要将机器学习任务提交到集群中运行往往还得依靠类似 Kubeflow Pipelines 这种 DAG 管理及调度组件，Kubeflow Pipelines 有一套基于 Python API 的语法，因此用户需要再重新定义一个独立的 pipeline。有没有办法直接将 notebook 中已经验证过的代码自动转换成 pipeline 并提交到集群呢？<a href="https://kubeflow-kale.github.io">Kale</a>（<strong>K</strong>ubeflow <strong>A</strong>utomated Pipe<strong>L</strong>ines <strong>E</strong>ngine）即是为了解决这个问题而诞生，它是一个能够将 Jupyter Notebooks 自动转换为 Kubeflow Pipelines 的工具。在最新的 0.5 版本中 Kale 新增了对 <a href="https://github.com/kubeflow/katib">Katib</a> 的集成，后者是进行自动超参调优（Hyperparameter Tuning）和神经网络架构搜索（Neural Architecture Search）的组件。</p>

<h2>GoogleCloudPlatform/spark-on-k8s-operator #976: Add support for dynamic allocation via shuffle tracking</h2>

<p><a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/pull/976">[链接]</a></p>

<p>Spark 3.0 为<a href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">动态资源分配</a>（dynamic resource allocation）新增了 shuffle tracking 特性（默认关闭），具体实现可以查看 <a href="https://issues.apache.org/jira/browse/SPARK-27963">SPARK-27963</a>。当使用动态资源分配时用户需要预先设定诸如初始、最小和最大 executor 数量这样的参数，之后 Spark 运行时会根据当前任务排队时间和 executor 空闲时间这些指标去创建或者销毁 executor。对于有状态的 executor（如 shuffle 时存储到磁盘的数据、cache 到内存和磁盘的数据）会有一些特殊的策略防止错误回收资源，过去的做法是使用外部 shuffle 服务。开启 shuffle tracking 以后就不再依赖外部 shuffle 服务，而是设置一个 executor 持有 shuffle 数据的超时时间。过去 Spark 的 K8s 模式不支持外部 shuffle 服务，有了这个新的特性以后使得动态资源分配在 K8s 模式上成为可能。spark-on-k8s-operator 项目近期也支持了这个特性，可以直接通过 <a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#dynamic-allocation">YAML 配置</a>来开启。</p>

<h2>Boiled Hippo</h2>

<p><a href="https://spacefruityrecords.bandcamp.com/album/boiled-hippo-2">[Bandcamp]</a> <a href="https://music.163.com/#/album?id=91278378">[网易云音乐]</a> <a href="https://www.xiami.com/album/1ttwrEdcce1">[虾米]</a></p>

<p>本期最后推荐一张来自我的一个好朋友的唱片，Boiled Hippo 是一支北京的迷幻摇滚乐队，经过多年的演出积累终于在今年发行了乐队的第一张同名专辑。虽说是迷幻摇滚，但如果从旋律上讲绝对是非常「好听」的。如果你有兴趣购买实体唱片（黑胶、磁带、CD 都有），目前可以在北京的 fRUITYSPACE、fRUITYSHOP、独音唱片，上海的 Daily Vinyl，金华的 Wave 这几个地方购买。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #4]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4/"/>
    <updated>2020-06-17T14:07:52+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>AliGraph: A Comprehensive Graph Neural Network Platform</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3292500.3340404">[链接]</a></p>

<p>AliGraph 是阿里巴巴团队研发的 GNN（Graph Neural Network）分布式训练框架（虽然标题里是「平台」但感觉还算不上），论文发表在 KDD 2019 和 PVLDB 2019。</p>

<p>论文开篇便提出了当下 GNN 模型训练的 4 个挑战：</p>

<ol>
<li>如何提高大规模图模型的训练效率及优化空间占用？</li>
<li>怎样优雅地将异构（heterogeneous）信息组合到一个统一的 embedding 结果中？</li>
<li>如何将结构化的拓扑（topological）信息与非结构化的属性（attribute）信息统一来共同定义那些需要保留的信息？</li>
<li>如何设计一个高效的增量更新动态图的 GNN 方法？</li>
</ol>


<p>后面的篇章便是详细介绍 AliGraph 如何解决以上这 4 个问题。框架从上至下整体分为 3 层：算子（operator）、采样（sampling）、存储（storage）。算子层包含常见的 GNN 运算操作，采样层包含几种预设的采样算法，存储层主要关注如何高效对大规模图进行分布式存储。在这 3 层基础之上可以实现任意的 GNN 算法以及应用。</p>

<p>存储层因为是要解决一个图的分布式存储问题，因此首先要将图进行分割（partition）。AliGraph 内置了 4 种图分割算法：<a href="https://dm.kaist.ac.kr/kse625/resources/metis.pdf">METIS</a>、<a href="https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez">顶点切割和边切割</a>、<a href="https://dl.acm.org/doi/10.1145/2503210.2503293">2D 分割</a>、<a href="https://dl.acm.org/doi/10.1145/2339530.2339722">流式分割</a>。这 4 种算法分别适用于不同的场景，METIS 适合处理稀疏（sparse）的图，顶点切割和边切割适合密集（dense）的图，2D 分割适合 worker 数量固定的场景，流式分割通常应用在边（edge）频繁更新的图。用户需要根据自己的需求选择恰当的分割算法，当然也可以通过插件的形式自己实现。</p>

<p>另一个存储层关心的问题是如何将图结构和属性（attribute）共同存储。这里讲的图结构即顶点和边的信息，这是最主要的图数据。同时每个顶点也会附加一些独特的属性，例如某个顶点表示一个用户，那附加在这个用户上面的属性就是类似性别、年龄、地理位置这样的信息。如果直接将属性信息和图结构一起存储会造成非常大的空间浪费，因为从全局角度看同一种类型的顶点的属性是高度重合的。并且属性与图结构的大小差异也非常明显，一个顶点 ID 通常占用 8 字节，但是属性信息的大小从 0.1KB 到 1KB 都有可能 。因此 AliGraph 选择将属性信息单独存储，通过两个单独的索引分别存储顶点和边的属性，而图结构中只存储属性索引的 ID。这样设计的好处自然是显著降低了存储所需的空间，但代价就是降低了查询性能，因为需要频繁访问索引来获取属性信息。AliGraph 选择增加一层 LRU 缓存的方式对查询性能进行优化。</p>

<p>存储层关心的最后一个问题也是跟查询性能有关。在图算法中一个顶点的邻居（neighbor）是非常重要的信息，邻居可以是直接（1 跳）的也可以是间接（多跳）的，由于图被分割以后本地只会存储直接的邻居，当需要访问间接邻居的时候就必须通过网络通信与其它存储节点进行交互，这里的网络通信代价在大规模图计算中是不容忽视的。解决思路也很直接，即在每个节点本地缓存顶点的间接邻居，但要缓存哪些顶点的邻居，要缓存几个邻居是需要仔细考量的问题。AliGraph 没有使用目前常见的一些缓存算法（如 LRU），而是提出了一种新的基于顶点重要性（importance）的算法来对间接邻居进行缓存。在有向图中计算一个顶点重要性的公式是 <code>入邻居的个数 / 出邻居的个数</code>，注意这里的邻居个数同样可以是直接的或者间接的。当这个公式的计算结果大于某个用户自定义的阈值时即认为这是一个「重要」的顶点。从实际测试中得出的经验值是通常只需要计算两跳（hop）的邻居个数就够了，而阈值本身不是一个特别敏感的数值，设置在 0.2 左右是对于缓存成本和效果一个比较好的平衡。选出所有重要的顶点以后，最终会在所有包含这些顶点的节点上缓存 <em>k</em> 跳的出邻居（out-neighbor）。</p>

<p>GNN 算法通常可以总结为 3 个步骤：采样（sample）某个顶点的邻居，聚合（aggregate）这些采样后的顶点的 embedding，将聚合后的 embedding 与顶点自己的进行合并（combine）得到新的 embedding。这里可以看到采样是整个流程中的第一步，采样的效果也会直接影响后续计算的 embedding 结果。AliGraph 抽象了 3 类采样方法：遍历采样（traverse）、近邻采样（neighborhood）和负采样（negative）。遍历采样是从本地子图中获取数据；近邻采样对于 1 跳的邻居可以从本地存储中获取，多跳的邻居如果在缓存中就从缓存中获取否则就请求其它节点；负采样通常也是从本地挑选顶点，在某些特殊情况下有可能需要从其它节点挑选。</p>

<p>在采样完邻居顶点以后就是聚合这些顶点的 embedding，常用的聚合方法有：element-wise mean、max-pooling 和 LSTM。最后是将聚合后的 embedding 与顶点自己的进行合并，通常就是将这两个 embedding 进行求和。为了加速聚合和合并这两个算子的计算，AliGraph 应用了一个物化（materialization）中间向量的策略，即每个 mini-batch 中的所有顶点共享采样的顶点，同样的聚合和合并操作的中间结果也共享，这个策略会大幅降低计算成本。</p>

<p>在最后的评估环节用了两个来自淘宝的数据集，两个数据集之间只有大小的区别，大数据集是小数据集的 6 倍左右。大数据集的基础数据是：4.8 亿个用户顶点，968 万个商品顶点，65.8 亿条用户到商品的边，2.3 亿条商品到商品的边，用户平均有 27 个属性，商品平均有 32 个属性。当使用 200 个 worker（节点配置论文中没有说明）时大数据集只需要 5 分钟即可将整个图构建完毕，相比之下以往的一些方案可能需要耗费数小时。基于顶点重要性的缓存算法相比 LRU 这些传统算法也是明显更优。3 类采样方法的性能评估结果从几毫秒到几十毫秒不等，但最长也不超过 60 毫秒，并且采样性能与数据集大小不太相关。聚合和合并算子相比传统的实现也有一个数量级的性能提升，这主要得益于前面提到的物化策略。</p>

<p>AliGraph 目前已经开源（一部分？）但是换了一个名字叫做 <a href="https://github.com/alibaba/graph-learn">graph-learn</a>，跟大多数深度学习框架一样，底层使用 C++ 语言实现并提供 Python 语言的 API，目前支持 TensorFlow，未来会支持 PyTorch。有意思的是刚刚开源不久就有人提了一个 <a href="https://github.com/alibaba/graph-learn/issues/16">issue</a> 希望能够跟另外几个流行的 GNN 框架进行比较，但是项目成员的回答比较含糊。</p>

<h2>Building Uber’s Go Monorepo with Bazel</h2>

<p><a href="https://eng.uber.com/go-monorepo-bazel">[链接]</a></p>

<p>Uber 应该是除了 Google 以外很早选择在后端服务中大规模使用 Go 语言的公司之一，并贡献了很多著名的 Go 语言项目（如 <a href="https://github.com/uber-go/zap">zap</a>、<a href="https://github.com/jaegertracing/jaeger">Jaeger</a>）。早在 2017 年，Uber 的 Android 和 iOS 团队就已经只使用一个代码仓库进行开发，俗称 monorepo。实践 monorepo 最著名的公司应该还是 Google，有兴趣可以看看 <a href="https://research.google/pubs/pub45424">Why Google Stores Billions of Lines of Code in a Single Repository</a> 这篇文章。现在后端团队也开始采用 monorepo 来管理 Go 语言项目，但是和客户端团队的不同之处在于没有用 <a href="https://buck.build">Buck</a> 而是用 <a href="https://bazel.build">Bazel</a>（前者是 Facebook 开源，后者是 Google 开源）。这篇文章介绍了在 monorepo 中将 Go 语言和 Bazel 结合遇到的一些问题。</p>

<h2>Optimising Docker Layers for Better Caching with Nix</h2>

<p><a href="https://grahamc.com/blog/nix-and-layered-docker-images">[链接]</a></p>

<p>恐怕大多数时候接触容器是从构建一个 Docker 镜像开始的，这一步往往也是最容易被忽视的。为什么我的镜像这么大？为什么每次拉取镜像都要从头开始？这些问题可能会随着使用时间越来越长逐渐浮现出来，要回答它们需要了解 Docker 镜像的一个核心概念「layer」，本质上你在 <code>Dockerfile</code> 里写的每一行命令都会生成一个 layer，一个镜像便是由很多 layer 构成。Layer 之间是有层级关系的，当拉取镜像时如果本地已经存在某个 layer 就不会重复拉取。在传统的 Linux 发行版中安装依赖时 Docker 是不知道具体有哪些文件被修改的，而 <a href="https://github.com/NixOS/nix">Nix</a> 这个特殊的包管理器采用了不一样的设计思路使得安装依赖这件事情对于 Docker layer 缓存非常友好。衍生阅读推荐 Jérôme Petazzoni 写的关于如何减少镜像大小的<a href="https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html">系列文章</a>。</p>

<h2>Proposal: Permit embedding of interfaces with overlapping method sets</h2>

<p><a href="https://github.com/golang/proposal/blob/master/design/6977-overlapping-interfaces.md">[链接]</a></p>

<p>Interface 是 Go 语言一个重要的特性，类似很多其它语言中的概念，接口定义好以后是需要通过 struct 来实现的。但不同之处又在于 struct 不需要显式声明实现了什么 interface，只要满足 interface 中定义的接口就行，这个关键设计使得 Go 语言的 interface 使用场景可以非常灵活。跟 struct 一样 interface 也允许嵌套，也就是可以在一个 interface 定义中嵌套另一个 interface。如果同时嵌套了多个 interface，并且这些 interface 之间有重复的接口在编译时是会报错的。实际开发过程中为了规避这个限制可能需要修改 interface 的定义，这对于开发者来说不太友好。上面这个提案允许开发者在不修改代码的情况下避开这个限制，目前这个功能已经在 <a href="https://golang.org/doc/go1.14#language">Go 1.14</a> 中发布。</p>

<h2>VexTab</h2>

<p><a href="https://github.com/0xfe/vextab">[链接]</a></p>

<p>不管是音乐创作还是音乐演奏，乐谱都是一个必不可少的东西。还记得刚学吉他那会儿非常热衷的一件事情就是去网上搜集各种歌曲的六线谱，这些乐谱的格式从最朴素的纯文本到高级的 <a href="https://www.guitar-pro.com">Guitar Pro</a> 格式都有。再后来开始学习扒歌，也面临把扒下来的谱子纪录下来的需求。虽然 Guitar Pro 很好但毕竟是一个收费软件，文件格式也是私有的。就像我更喜欢 Markdown 而不是直接用 Word 一样，一直希望能有一个类似的标记语言用于编写乐谱。VexTab 即是这样一个专门用于编写五线谱和六线谱的语言，也提供一个 JavaScript 库方便嵌入到网页中。有意思的是 VexTab 的作者同时也是 Google 的一名员工。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #3]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3/"/>
    <updated>2020-06-10T17:37:27+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Kudu: Storage for Fast Analytics on Fast Data</h2>

<p><a href="https://kudu.apache.org/kudu.pdf">[链接]</a></p>

<p><a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a>（Online Analytical Processing）一直是大数据领域非常重要的应用场景，光有数据也不行，你得「分析」啊。自从有了 Hadoop，OLAP 的工具就一直在演变，从最早的裸写 MapReduce 任务，到 <a href="https://pig.apache.org">Pig</a>、<a href="https://hive.apache.org">Hive</a>、<a href="https://prestosql.io">Presto</a>、<a href="https://impala.apache.org">Impala</a>、<a href="https://druid.apache.org">Druid</a>、<a href="https://clickhouse.tech">ClickHouse</a>，以及今天要介绍的 <a href="https://kudu.apache.org">Kudu</a>。一个明显的趋势是 OLAP 引擎在逐步朝着「去 Hadoop 化」和「实时化」发展，当然这些项目里最新的也已经是 2016 年发布的了，接下来会怎么变化还是个未知数。</p>

<p>先讲讲为什么会有类似 Kudu 这样的项目诞生。传统的 OLAP 引擎因为是构建在 HDFS 上的，要想分析数据首先得将数据存储到 HDFS 上，而这个过程（通常叫做 ETL）往往是比较耗时以及复杂的。同时由于 HDFS 天生不支持随机读写，为了弥补这个「缺陷」，有了 HBase 这样的项目。但 HBase 对于 OLAP 场景是不够友好的，因此往往需要把数据从 HBase 再导入到 HDFS 中，这个过程也可能比较耗时，维护成本也比较高。因此 Kudu 的目标是实现一个即支持随机读写（主要是写），又针对大批量查询进行优化的存储引擎。这种时候 HDFS 就显得很累赘了，这也是为什么越来越多引擎选择不依赖 HDFS 的缘故（Kudu 官网也在 FAQ 中专门<a href="https://kudu.apache.org/faq.html#why-doesnt-kudu-store-its-data-in-hdfs">解释</a>了为什么不用 HDFS）。当然并不是说 HDFS 就没用了，有很多数据还是非常静态的，对于实时性要求也不高，此时用 HDFS 是一种简单经济的选择。</p>

<p>这篇论文虽然介绍的是 Kudu 早期的一些设计思想，但基本上属于最核心的功能。跟很多分布式数据库一样，Kudu 也是受 <a href="https://research.google/pubs/pub39966">Spanner</a> 启发。系统架构上分为一个 Master 服务和若干 Tablet 服务。Master 负责维护元信息，包括 Tablet 节点和数据的。Tablet 服务则负责数据存储，每台节点上会有几十至数百个 tablet，每个 tablet 中包含了若干数据，最大可以达到几十 GB 的规模（这里你可以把 tablet 类比为很多别的系统中的 region 概念）。</p>

<p>跟很多关系数据库一样，Kudu 是有 table 的概念的。但跟很多 NoSQL 数据库不一样的地方是，强制用户必须显式定义 schema。Kudu 一个有意思的设计在于同时支持了 hash 和 range 这两种数据 partition 方法，而不像别的系统只支持其中一种（有关这两种 partition 的介绍可以看我之前的<a href="https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5/">一篇文章</a>）。这样设计的好处是即保留了 hash 的数据均匀分配特点，可以在一定程度上防止读写热点，又保留了 range 对于范围扫描的友好性。</p>

<p>Tablet 服务之间是通过 Raft 来进行数据复制，因此可以认为 Kudu 是一个保证强一致性的存储系统。值得注意的是 Kudu 的默认设置是 500 毫秒的心跳间隔以及 1.5 秒的选举超时，这个跟 Raft 论文推荐的时间相比长了不少（推荐的选举超时是 150~300 毫秒）。当集群扩容时，新节点将会首先进入 <code>PRE_VOTER</code> 状态，等到 log 追上以后再变成 <code>VOTER</code> 状态，这个设计也是 Raft 论文中建议的，不过论文中叫做 learner 或者 non-voting member。Master 服务虽然是单点设计（即状态不是分布式存储），但为了保障高可用也可以通过 Raft 实现多节点状态复制，只不过任意时间只能有一个节点工作。</p>

<p>Kudu 的数据存储引擎是完全自己设计的，没有直接用任何现有的引擎，虽然也能多少看出一些别的引擎的影子。关于这一点可以理解，OLAP 系统区别于 <a href="https://en.wikipedia.org/wiki/Online_transaction_processing">OLTP</a> （Online Transactional Processing）系统的最大不同即在于数据存储的形式，简单理解后者是行式（row-oriented）存储，而前者是列式（column-oriented）存储。著名的 <a href="https://parquet.apache.org">Parquet</a> 就是广泛被用于 OLAP 场景的列式存储格式，Kudu 在实现上也复用了很多 Parquet 的代码。</p>

<p>每个 table 在存储级别会被分割为多个 RowSets，顾名思义每个 RowSets 是由很多行（row）组成，RowSets 之间不会有重复的数据，但主键的范围可能会交叉。</p>

<p>当有新的数据时会首先存储到内存中的 MemRowSets，底层实现是一个使用乐观锁（optimistic locking）的并发（concurrent）B 树。比较特别的一点是数据并不是一开始就按照列式进行存储，MemRowSets 中还是用的行式存储。当数据累积到一定程度 MemRowSets 就会持久化到磁盘上，称之为 DiskRowSets，每个 DiskRowSet 大小上限是 32MB。DiskRowSet 由两部分组成：基础数据（base data）和增量数据（delta stores）。</p>

<p>基础数据是列式格式，即每一列都单独连续存储，每一列内部又划分成了多个小的页（page），有一个 B 树根据行号索引了这些页。每一列可以由用户指定不同的编码（encoding）方法（如 dictionary encoding、bitshuffle、front coding），同时也可以使用通用的压缩算法对数据进行压缩（如 LZ4、gzip、bzip2），基于列的编码及数据压缩是列式存储非常大的一个特点。</p>

<p>增量数据也分为内存和磁盘两种形式。内存中的叫做 DeltaMemStores，这个跟 MemRowSets 的实现一样。磁盘中的叫做 DeltaFiles，是一个二进制类型的列块（column block）。不管是内存还是磁盘上的数据都会有一个额外的从 <code>(row_offset, timestamp)</code> 到 RowChangeList 的映射，<code>row_offset</code> 是某一行在一个 RowSet 中的偏移，RowChangList 是二进制编码以后的增量操作（如更新某一列、删除某一行）列表。同样的，DeltaMemStores 也会持久化到磁盘上变成 DeltaFiles。</p>

<p>这些增量数据会定期跟基础数据进行合并（compation），以防止过多的增量文件。同时 DiskRowSets 之间也会进行合并，目的是清理已经被删除的行以及减少 DiskRowSets 之间的主键交叉范围。</p>

<p>前面提到的将内存中的数据持久化到磁盘及对数据进行合并操作，都是由一组单独的后台任务来完成，但是什么时候执行什么操作是由一个调度器来控制的。有趣的是 Kudu 将调度器的逻辑抽象成了一个<a href="https://en.wikipedia.org/wiki/Knapsack_problem">背包问题</a>，只不过需要权衡的不是背包容量，而是 I/O 带宽。</p>

<p>Kudu 本身只提供编程语言级别的 API（如 Java、C++），而 OLAP 系统中常用的 SQL 需要配合其它项目来实现。Kudu 原生已经跟 Spark 和 Impala 集成，也就是说你可以在这两个系统中通过 SQL 来查询。</p>

<p>最后是性能评测。在 <a href="http://www.tpc.org/tpch">TPC-H</a> 数据集上与 Parquet 进行对比测试，Kudu 平均有 31% 的性能提升，尽管如此 Kudu 团队认为随着 Parquet 的迭代这个差距可能会逐渐缩小。在与 <a href="http://phoenix.apache.org">Phoenix</a> 的对比测试中也有 16~187 倍的性能提升。最后与 HBase 进行的 <a href="https://github.com/brianfrankcooper/YCSB">YCSB</a> 测试是为了看看 Kudu 在 OLTP 场景的性能，虽然它本身并不是为 OLTP 场景而设计，结果上的确也是 HBase 表现更好，但 Kudu P99 6 毫秒的响应时间在某些时候也许可以代替 OLTP 系统。</p>

<p>Kudu 由 Cloudera 公司开发并于 2016 年正式发布，现已捐献给 Apache 基金会，整个系统使用 C++ 语言编写。</p>

<p>顺带说个题外话这两年炒得比较火的 <a href="https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing">HTAP</a>（Hybrid Transactional/Analytical Processing），本质上是希望在一个引擎中同时适配 OLTP 和 OLAP 这两个场景。但在我看来就目前的技术现状这个愿景实现起来还是比较困难，软件工程界的名言<a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">「没有银弹」</a>告诉我们不存在一个可以通吃的、完美的方案，因此 HTAP 目前更多还只是一个营销概念吧。</p>

<h2>字节跳动自研强一致在线 KV &amp; 表格存储实践</h2>

<p><a href="https://mp.weixin.qq.com/s/jdPE9WClBuimIHVxJnwwUw">[上篇]</a> <a href="https://mp.weixin.qq.com/s/DvUBnWBqb0XGnicKUb-iqg">[下篇]</a></p>

<p><a href="https://github.com/cockroachdb/cockroach">又</a><a href="https://github.com/pingcap/tidb">又</a><a href="https://github.com/dgraph-io/dgraph">又</a><a href="https://kudu.apache.org">又</a><a href="https://github.com/vesoft-inc/nebula">又</a>一个受 Spanner 启发的分布式存储（Google 功德无量！Jeff Dean 万寿无疆！），这次的项目来自字节跳动。关键词：range 分割、Raft、RocksDB、MVCC、分布式事务、SQL 层，看看这些也基本能对整体设计猜个八九不离十了，比较有价值的信息是学习学习字节跳动在他们的实践中的一些经验。项目使用 C++ 语言编写，目前没有开源。</p>

<h2>Challenges Supporting MIG in Kubernetes</h2>

<p><a href="https://docs.google.com/document/d/1Dxx5MwG_GiBeKOuMNwv4QbO8OqA7XFdzn7fzzI7AQDg">[链接]</a></p>

<p>随着深度学习的蓬勃发展，GPU 共享逐渐成为了 K8s 社区的一个<a href="https://github.com/kubernetes/kubernetes/issues/52757">热门话题</a>。目前 NVIDIA 官方提供的<a href="https://github.com/NVIDIA/k8s-device-plugin">设备插件</a>可以申请的最小资源粒度还是 1 个 GPU，但很多时候资源是浪费的。为了提升 GPU 的资源利用率，社区已经出现了多种解决方案，例如分别来自<a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender">阿里云</a>、<a href="https://github.com/tkestack/gpu-manager">腾讯云</a>以及 <a href="https://github.com/awslabs/aws-virtual-gpu-device-plugin">AWS</a> 的实现。现在 NVIDIA 官方终于在新一代的 Ampere 架构硬件上原生支持了共享，也就是标题中的 MIG（Multi-Instance GPUs）。这篇文档来自 NVIDIA 团队，首先介绍了当前是如何在 K8s 中管理 GPU 资源的，然后介绍了 MIG 的一些概念，最后提议了 4 个支持 MIG 的可能的解决方案。整体感觉 GPU 共享还是没有 CPU 灵活，不少地方设置了限制，但毕竟这是 NVIDIA 官方迈出的第一步。</p>

<h2>How to read deep learning papers?</h2>

<p><a href="https://www.reddit.com/r/MachineLearning/comments/gi3ihe/d_how_to_read_deep_learning_papers">[链接]</a></p>

<p>Reddit 上一个有趣的讨论：如何阅读深度学习的论文？我们常常调侃机器学习就是在「炼丹」，没有人能解释为什么结果就是有效的，反正<a href="https://www.youtube.com/watch?v=YPN0qhSyWy8">「it just works」</a>。最高票的评论说你不需要接受论文中的每一个观点，只要把注意力集中在作者提供的证据并有选择性地调整你的想法就好了。正好前段时间前微软执行副总裁沈向洋博士做了一个主题名为<a href="https://www.bilibili.com/video/BV1df4y1m74k">「You are how you read」</a>的演讲，主要内容就是一些阅读论文的经验（有趣的是沈博士在几年前还写过一篇叫做<a href="https://www.linkedin.com/pulse/you-what-write-harry-shum">「You are what you write」</a>的博客）。</p>

<h2>Farewell, TensorFlow</h2>

<p><a href="https://mrry.github.io/2020/05/10/farewell-tensorflow.html">[链接]</a></p>

<p>TensorFlow 核心开发者、Google Brain 团队的 Derek Murray 宣布离开，这位大哥在 GitHub 和 Stack Overflow 上都很活跃，如果你经常浏览社区应该对他的头像不陌生。在这篇告别文中 Murray 提到了很多 Google 内部帮助工程师解决问题的工具，并详细介绍了近期对 TensorFlow 底层运行时进行的一项性能优化的过程，在部分评测中可以提升 10% 的推理性能。这个优化目前已经合入 master，并将在 TensorFlow 2.3 发布。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #2]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2/"/>
    <updated>2020-06-02T09:25:45+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>In Search of an Understandable Consensus Algorithm (Extended Version)</h2>

<p><a href="https://raft.github.io/raft.pdf">[链接]</a></p>

<p>终于有机会仔细阅读一遍 Raft 的论文，如果你还不了解 Raft 是什么可以看看我过去的一篇介绍分布式系统基础概念的<a href="https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5/">文章</a>。</p>

<p>Raft 为节点定义了三种状态：leader、follower 和 candidate（以及一个非正式状态 learner 或者叫做 non-voting member）。一个集群只会有 1 个 leader，其余节点都是 follower。Leader 负责处理所有的读写请求，如果请求 follower 会失败并告知客户端 leader 的地址。</p>

<p>每个节点都有一个自己的 log，log 中每个条目都有一个下标（index）。这个 log 基本算是 append-only 的，通常也需要持久化到可靠的存储上（例如磁盘）。当处理写请求时 leader 会首先更新自己的 log，然后通过 RPC 复制到其它节点，只要大多数（majority）节点更新成功 leader 就会认为这个请求已经 committed，此时会更新自己的状态机（state machine）并返回给客户端。如果 RPC 请求失败 leader 会不断重试直到成功。</p>

<p>如果出现异常，如 leader 宕机、网络故障等，就可能触发 leader 重新选举。选举过程是所有 follower 为 candidate 投票，只要获得多数票 candidate 就会升级为 leader。如果投票失败会继续新一轮选举，选举过程通常是毫秒级的。每一轮新的选举都会产生一个对应的 term（任期），Raft 在协议上保证了重新选举后的新 leader 一定是包含之前所有 term 已经 committed 的 log，这样就避免了新 leader 选举成功以后需要首先补上缺失的数据。</p>

<p>当集群需要伸缩时，leader 会首先将旧集群配置（configuration）和新集群配置合并到一起并通过 log 的形式复制到 follower。成功收到这个合并后配置的节点会用这个配置替代老的配置。一旦这个合并后的配置 committed，leader 就会创建一个只包含新配置的 log 继续复制到 follower。等到新的配置 committed，旧配置将不再生效，需要下线的节点也可以被安全关闭。</p>

<p>随着时间增长，log 的容量会越来越大，Raft 引入了快照（snapshot）机制，定期将 log 压缩到快照文件。这个快照文件同时也可以帮助新加入的节点快速补上缺失的数据。</p>

<p>总结一下 Raft 算法保证了以下几个属性始终成立：</p>

<ul>
<li><strong>Election Safety</strong>：在一个特定的任期最多只能有一个 leader 被选举出来</li>
<li><strong>Leader Append-Only</strong>：leader 永远不会覆盖或者删除 log 中的条目，只会追加新的条目。</li>
<li><strong>Log Matching</strong>：如果两份 log 同时包含一个具有相同任期数和下标的条目，那么这两份 log 中这个下标之前的所有条目都应该是一致的。</li>
<li><strong>Leader Completeness</strong>：如果某个任期中的一个 log 条目已经 committed，那么在之后任期中选举出的新 leader 一定包含这个条目。</li>
<li><strong>State Machine Safety</strong>：如果一个节点已经将一个给定下标的 log 条目更新到自己的状态机，那么其它节点上同样的下标一定不会是不同的条目，也就是说不会更新一个不同的条目到自己的状态机。</li>
</ul>


<p>更多有关 Raft 的信息可以查看它的<a href="https://raft.github.io">官网</a>，强烈建议初次接触一致性协议的朋友看看网站上的动画演示，非常有助于建立一个形象直观的认知。</p>

<h2>Scaling Raft</h2>

<p><a href="https://www.cockroachlabs.com/blog/scaling-raft">[链接]</a></p>

<p>作为前面介绍 Raft 的一篇衍生阅读，原始的 Raft 实现是将所有节点看作一个 group，这种设计在某些场景（例如集群规模很小）是可行的。但是当集群规模大到一定程度，或者类似 <a href="https://github.com/cockroachdb/cockroach">CockroachDB</a> 和 <a href="https://github.com/tikv/tikv">TiKV</a> 这种将数据划分为非常多的 range，多个 range 组成一个 Raft group 的场景（通常叫做 Multi-Raft），就会发现 Raft 的基础网络通信已经足以影响单节点的性能（比如过多的心跳请求）。因此社区已经针对这样的问题有了一些优化方案，比如 <a href="https://github.com/cockroachdb/cockroach/issues/357">CockroachDB 的方案</a>和 <a href="https://github.com/tikv/tikv/pull/4591">TiKV 的方案</a>。这两个方案都很类似，基本思想是暂停那些不活跃的 Raft group 的网络通信，等到需要的时候再唤醒。</p>

<h2>Why Generics?</h2>

<p><a href="https://blog.golang.org/why-generics">[链接]</a></p>

<p>这篇文章是 Ian Lance Taylor 在 GopherCon 2019 演讲的文字版（文章中也附带了视频），主要介绍了目前 Go 的核心开发者关于泛型（generics）的一些思考。总的来说 Go 核心团队的设计思想还是保持 Go 语言一贯的简洁，不希望引入过多的概念和复杂性。大部分新增的语法特性都由提供泛型接口的开发者来学习，对于使用者来说和调用普通接口几乎没有区别。早在 2016 年社区就已经有了 <a href="https://github.com/golang/go/issues/15292">#15292</a> 这个关于泛型的讨论，并且还在持续更新中，目前已经有了 710 条评论，Ian Lance Taylor 也在其中积极回复。虽然这个 issue 打上了 Go2 的标签，但泛型特性是否能在 Go 语言的 2.0 版本中出现现在还是个未知数。</p>

<h2>The Open Application Model from Alibaba’s Perspective</h2>

<p><a href="https://www.infoq.com/articles/oam-alibaba">[链接]</a></p>

<p>阿里云和微软在去年<a href="https://cloudblogs.microsoft.com/opensource/2019/10/16/announcing-open-application-model">共同宣布</a>了 Open Application Model（OAM），OAM 组织的<a href="https://github.com/orgs/oam-dev/people">核心成员</a>同时也是前 CoreOS 团队成员以及 etcd、K8s Operator 的创造者。简单理解 OAM 就是希望将传统的 K8s YAML 配置抽象成两部分：开发者和运维，开发者的配置中只包含与业务最相关的内容，而运维的配置中则包含与运行环境相关的内容。本质上是希望将开发者和运维的界线分得更清楚，让不同的角色更专注于自己的领域。在我看来 OAM 的好处当然是降低了普通开发者接入 K8s 的门槛，所谓大道至简，但这种表面上的「简」背后隐藏的复杂性也是不能忽略的。理想情况是某个云服务商能够完全包办所有跟运维有关的事情，用户只需要负责业务开发就好了。但现状还是不管多小的公司都肯定会有专人在负责运维工作。很多年前 Google App Engine 刚诞生时让所有人都眼前一亮，都认为这才是软件开发的未来啊，但即使是 Google 也没能让这个趋势持续下去。最近几年这个趋势又开始回潮，只不过换了一个名字叫做「Serverless」，希望这一次能够持续下去，虽然还有很长的路要走。</p>

<h2>Lightweight coscheduling based on back-to-back queue sorting</h2>

<p><a href="https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/kep/20200116-lightweight-coscheduling-based-on-back-to-back-queue-sorting.md">[链接]</a></p>

<p>自从 K8s 1.15 新增了 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework">Scheduling Framework</a> 以后，原生调度器的扩展性有了很大程度的增强。这个 KEP 来自阿里云团队，提出了基于 Scheduling Framework 来实现 coscheduling（或者叫做 gang scheduling）。Coscheduling 这个特性对于机器学习任务来说是非常重要的，一个任务通常包含多个 pod，只有当多个 pod 能够同时运行时这个任务才算是正常运行，如果只有部分 pod 可以运行其实是一种资源的浪费。因此 coscheduling 保证的就是一个任务必须满足一定数量的 pod 都能够被调度时才会实际分配资源。这个特性在 K8s 社区早有讨论，也诞生了一些相关联的项目，如 <a href="https://volcano.sh">Volcano</a>（前身是 <a href="https://github.com/kubernetes-sigs/kube-batch">kube-batch</a>）。5 月初这个插件的第一版已经被 <a href="https://github.com/kubernetes-sigs/scheduler-plugins/pull/4">merge</a> 到 scheduler-plugins 项目。</p>

<h2>Scheduler Support for Elastic Quota Management</h2>

<p><a href="https://docs.google.com/document/d/1ViujTXLP1XX3WKYUTk6u5LTdJ1sX-tVIw9_t9_mLpIc/edit?usp=sharing">[链接]</a></p>

<p>同样是与 K8s 相关的一个讨论，也同样来自阿里云团队。<code>ResourceQuota</code> 是 K8s 目前提供的一种限制某个 namespace 最大资源使用量的方式，但是在实际的多租户场景中，<code>ResourceQuota</code> 往往显得不够灵活。很多时候我们是希望给每个租户一个可以保证（guarantee）的最小资源量，以及一个超卖的最大资源量。当某个租户的资源比较空闲时，就允许其它租户临时租用。但是调度器也要保障这个租户有能力在必要的时候可以拿回这些被租用的资源，这通常是通过抢占（preemption）的方式来实现。这个提案就提出了扩展 <code>ResourceQuota</code> 来实现类似功能的想法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何设计与实现一个分布式索引框架（五）：分布式]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5/"/>
    <updated>2020-05-25T11:21:49+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5</id>
    <content type="html"><![CDATA[<blockquote><p>这是一个<a href="https://blog.xiaogaozi.org/categories/htdadif/">系列文章</a>，大部分内容都来自我过去在小红书发现 Feed 团队工作期间的实践和经验。在介绍的过程中我会尽量不掺杂过多的业务细节，而专注于这背后我个人一些浅薄的设计思想，希望你在阅读完这些文章以后能够直接或者间接地拓展到不同的场景。</p></blockquote>

<p>前面几篇文章介绍的技术都是在单机上实现的，但如果做不到分布式那整个系统的扩展性将会受到非常大的限制。本篇文章将会围绕分布式这个话题讨论。</p>

<!-- more -->


<h2>数据分割（Partition）</h2>

<p>分布式存储很大一个目的是为了将数据分布到多个节点上，以突破单机的存储限制，实现水平扩展（horizontal scaling）。因此这就涉及到一个很重要的问题：要如何将数据分布到不同的节点上？可能的几种做法有：</p>

<ol>
<li>随机：每一条数据都随机分配到某个节点上</li>
<li>轮询（round-robin）：通过轮询的方式将数据分配到节点上，例如第 1 条数据分配到节点 1，第 2 条数据就分配到节点 2，以此类推。</li>
<li>哈希（hash）：通过某种哈希算法将数据中的某个 key 映射到一个固定的值，根据这个值来分配节点。</li>
<li>范围（range）：划定一些范围，并将这些范围与节点进行映射，当数据中的某个 key 属于某个范围时就分配到对应的节点上。</li>
</ol>


<p>方案 1 显然是最简单的，但也是最不可行的。这个方案有两个大问题：因为数据是随机分配的，因此在查询某一条数据时必须请求所有节点；同样因为随机分配的关系，不同节点之间的数据量可能是非常不均衡的。</p>

<p>方案 2 相比方案 1 稍微改进了一点，轮询的方式可以基本保证数据分布是均衡的，但是在查询时还是必须请求所有节点。</p>

<p>方案 3 基本解决了前面提到的两个问题，哈希算法通常是稳定的，也就是说通过某个 key 得到的哈希值是固定的。比如最简单的哈希算法取模运算，将 key 模上集群的节点数 <code>key mod N</code>，就可以算出这个 key 应该分配的节点。不过取模运算虽然简单但也存在一些问题，最明显的就是当添加新节点或者删除老节点的时候会造成大量的数据重新分配（rebalance）。因此比较常见的改进方案是采用<a href="https://en.wikipedia.org/wiki/Consistent_hashing">一致性哈希</a>（consistent hashing），一致性哈希可以显著降低数据重新分配这个过程需要迁移的数据量。Amazon 的 <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo</a> 便是采用一致性哈希进行数据分割的一个很好的例子，Cassandra 的<a href="https://cassandra.apache.org/doc/latest/architecture/dynamo.html">官方文档</a>里也介绍了类似的内容。但是一致性哈希也不是没有缺点，当集群节点数较少时还是有可能造成数据分布不均衡，因此 Dynamo 提出了通过增加虚拟节点（virtual node）的方法来解决这个问题，细节可以参考论文或者 Cassandra 的文档。</p>

<p>方案 4 也能实现稳定查询，例如将数据 key 的首字母限定在 a-z 这 26 个字母中，再将 a-z 等分为几个范围（range），那么就能根据 key 的首字母确定属于哪个范围。同时每个节点会包含 1 个或多个范围，便能将 key 分配到某个节点上。HBase 便是采用范围分割数据的一个案例，但是由于 HBase 不会预先为所有节点绑定范围，因此在实践中通常还要结合 <a href="https://hbase.apache.org/book.html#tricks.pre-split">pre-split</a> 来避免数据都集中在少数节点中。因为每个范围都是连续的，所以方案 4 相比方案 3 的一个优势是对于范围扫描（range scan）的支持更好。</p>

<p>综合来看方案 3 和方案 4 都是可行的方案，它们也都有各自的一些优缺点，如何选择还得看具体的使用场景。</p>

<h2>数据复制（Replication）</h2>

<p>数据分布到多个节点上以后，虽然扩展性（scalability）得到了满足，但是随着节点数的增多，可用性（availability）的重要性会逐渐凸显出来。节点因为各种原因下线是非常普遍的，一旦节点下线那这台节点上的数据将无法访问。因此为了保障可用性，通常会通过冗余存储的方式来解决，也就是为每一份数据新增多个副本（replica），然后将副本分散到不同的节点上，只要还有至少 1 个副本存在那即使部分节点下线也能继续访问数据。为了实现多副本也有几种可能的方案：</p>

<ol>
<li>节点组（node group）：为每个节点创建多个副本节点，这些节点共同组成一个节点组。一个节点组内部的数据是完全相同的，不同节点组之间的数据是不同的。</li>
<li>混合（hybrid）：每个节点不仅有属于自己的数据，同时还存储了其它节点数据的副本。</li>
</ol>


<p>方案 1 中节点组之间的数据因为是相互独立的，因此实现和维护相对来说都会比较简单，新增副本就只需要在每个节点组中新增节点即可。我们在数据库系统中经常见到主（master）从（slave）节点的概念，这里可以把 1 个主节点和多个从节点看作是一个节点组。</p>

<p>方案 2 是目前主流分布式存储的实现方案，在存储数据时通过某种算法选择多个副本节点，并时刻检查当前数据的副本数是否符合用户设定的值。这种方案因为在一个节点上同时包含了原始数据和副本，相比方案 1 节点的资源利用率会更高，但代价就是维护成本会有所提升。</p>

<p>不论是选择前面介绍的哪种方案都会涉及到一个问题：如何将原始数据同步到副本上？这里就必须提及在分布式系统中非常重要的一个概念「一致性（consistency）」<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>，所谓一致性就是用于描述分布式系统中不同实体间状态（state）一致程度的概念。一致性从强到弱大致可以分为以下 4 种类别：</p>

<ol>
<li>线性一致性（Linearizability）或者强一致性（Strong consistency）</li>
<li>顺序一致性（Sequential consistency）</li>
<li>因果一致性（Causal consistency）</li>
<li>最终一致性（Eventual consistency）</li>
</ol>


<p>一致性越强的算法对数据的一致要求也越高，当然实现成本也越高。线性一致性的代表有 <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> 和 <a href="https://raft.github.io">Raft</a>，最终一致性的代表有 Dynamo<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。为什么一致性如此重要呢？因为分布式系统天然存在的并发和延迟，要如何把一个集群的状态更新最终实现得看起来就像一台单机一样，这是一致性算法要解决的问题。</p>

<p>具体细分状态复制的实现方式有两种：一种是传统的 replicated state machine（或者叫做 active replication），另一种是 primary-backup（或者叫做 primary-copy、passive replication）。前者的代表有 Paxos 和 Raft，后者的代表有 <a href="http://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/viewstamped.pdf">Viewstamped Replication</a> 和 <a href="https://marcoserafini.github.io/papers/zab.pdf">Zab（ZooKeeper Atomic Broadcast）</a>。有关这两种状态复制方案的区别可以看看 Raft 作者的<a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf">博士毕业论文</a><sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>和 <a href="https://arxiv.org/abs/1309.5671">Vive la Différence: Paxos vs. Viewstamped Replication vs. Zab</a> 这篇论文。</p>

<p>因此回到最开始的那个问题「要如何将原始数据同步到副本」，这取决于你需要哪种程度的一致性，你甚至可以说我不需要一致性<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>。对于推荐系统的场景，线性一致性属于杀鸡用牛刀<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>，所以我们只要追求最终一致性就够了。</p>

<h2>集群成员管理</h2>

<p>一个分布式系统必然是由多个节点构成的，那这些节点之间要如何互相感知呢？关于这个问题可以分为两类方案：中心化和去中心化。</p>

<p>所谓中心化就是存在一个（或一组）集中管理的服务，这个中心服务负责接收并存储集群所有节点上报的信息，以及反向分发这些信息，相当于一个集群的信息枢纽。在微服务领域有另外一个词用于表示类似的功能：服务注册与发现。常见的可以实现这种中心服务的开源组件有 <a href="https://zookeeper.apache.org/">ZooKeeper</a>、<a href="https://etcd.io">etcd</a> 和 <a href="https://www.consul.io">Consul</a>。</p>

<p>而去中心化顾名思义就是不存在一个中心服务，完全依靠集群内各个节点之间的通信来实现拓扑发现。最著名的去中心化协议恐怕就是 <a href="https://en.wikipedia.org/wiki/Gossip_protocol">gossip 协议</a>，这是一个可以实现点对点（P2P）通信的协议，很多开源系统里也使用到了 gossip，比如 <a href="https://cassandra.apache.org/doc/latest/architecture/dynamo.html#distributed-cluster-membership-and-failure-detection">Cassandra</a> 和 <a href="https://www.consul.io/docs/internals/gossip.html">Consul</a>。</p>

<p>至于是中心化还是去中心化好那只能是见仁见智了，没有哪个方案是绝对完美的。</p>

<h2>数据重新分配（Rebalance）</h2>

<p>前面的「数据分割」小节已经介绍了如何将数据分布到不同的节点上，如果一个集群的节点数永远不变那这不会带来任何问题，但是如果存在新增或者删除节点的情况呢？不论是哈希还是范围分割的方法，都必须要重新分配数据，以保持集群节点间的数据均衡。为了不影响已有的节点，数据重新分配通常的实现都是在一个后台线程中执行，同时也要控制数据同步的带宽和速率。当数据重新分配这个过程完成以后就可以上线或者下线对应的节点。</p>

<p>当然数据重新分配也不一定就只是由集群节点伸缩触发的，某些系统也会实时地根据当前每个节点的负载而动态调整数据的分布，目的是为了避免出现热点导致整体系统的稳定性受影响。</p>

<h2>整体设计</h2>

<p>综合前面介绍的所有内容，现在如果让你来设计分布式索引你会如何设计？这里提供一个我们的实现方案，但是请记住一定不存在一个完美的方案，任何架构设计都是权衡（trade-off）的结果。</p>

<p><img src="https://blog.xiaogaozi.org/images/posts/rec_sys_distributed_design.png" alt="recommendation system distributed design" /></p>

<p>简单总结上面这个方案的一些特点：</p>

<ul>
<li>通过哈希来进行数据分割</li>
<li>通过节点组的方式进行数据复制，一致性的要求是最终一致性<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>。</li>
<li>通过 Consul 来进行服务注册和发现，并封装一个库供客户端使用。</li>
</ul>


<p>以上就是关于分布式的介绍，下一篇文章的内容会相对轻松一些，聊一聊所谓的端到端（end-to-end）用户体验。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>如果想系统了解一致性，推荐阅读 Jepsen 的<a href="https://jepsen.io/consistency">系列文档</a>、普林斯顿大学的 <a href="https://www.cs.princeton.edu/courses/archive/fall19/cos418">COS 418</a> 课程以及 The Morning Paper 的<a href="https://blog.acolyer.org/2015/03/01/cant-we-all-just-agree">系列解读</a>。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Amazon 的 CTO 也写过<a href="https://www.allthingsdistributed.com/2008/12/eventually_consistent.html">一篇博客</a>讲解最终一致性<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>具体位置在「11.6 Replicated state machines vs. primary copy approach」章节<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>那你爱咋同步咋同步<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>用户根本不关心（或者说根本察觉不出）不同推荐结果之间有什么一致性问题<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>实践中我们是通过 HDFS 作为索引数据源，每个节点自行拉取（pull）的方式实现。<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #1]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1/"/>
    <updated>2020-05-21T17:34:22+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="https://blog.xiaogaozi.org/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>CFS: A Distributed File System for Large Scale Container Platforms</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3299869.3314046">[链接]</a></p>

<p>跟<a href="https://blog.xiaogaozi.org/2020/04/26/weekly-reading-list-issue-1/">上次介绍</a>的 FoundationDB Record Layer 一样，这篇来自京东团队的论文也是发表在 SIGMOD 2019，介绍了一个为大规模容器平台设计的分布式文件系统。</p>

<p>系统整体由 3 部分组成：元数据子系统（metadata subsystem）、数据子系统（data subsystem）、资源管理器（resource manager）。元数据子系统负责维护 inode 和 dentry（directory entry），数据子系统负责存储数据块，资源管理器负责处理客户端的各种文件操作请求以及维护前面两个子系统的状态。元数据子系统和数据子系统都是多 partition 的分布式系统，多个元数据和数据的 partition 逻辑上共同组成一个卷（volume），这个卷即是对客户端（容器）可见的存储单元并且可以被挂载，通过传统的 POSIX 接口访问。</p>

<p>因为上述 3 部分组件内部其实都是一个分布式系统，因此都用到了 Raft 作为一致性协议，资源管理器还用到了 RocksDB 作为本地持久化存储。稍微特殊的是数据子系统根据不同类型的写操作选择了不同的复制方案，论文里把这个叫做 Scenario-Aware Replication，具体讲就是顺序写操作（比如 append）用的是 primary-backup，而覆盖（overwrite）操作用的是 Raft。</p>

<p>系统的另一个亮点是基于资源利用率的 partition 分配策略，论文中叫做 Utilization-Based Placement。传统的 partition 分配策略通常是哈希，这种策略的优点是简单但是当扩缩容时必须进行 rebalance。CFS 的做法是元数据和数据子系统定期上报内存、磁盘使用率到资源管理器，当需要创建新的 partition 时根据资源利用率选择最低的那个节点，这样设计的好处是不再需要 rebalance。但是对于这种设计方案是否会造成数据不均衡表示存疑，论文中也没有做过多论述。</p>

<p>为了尽量减少客户端的网络交互，不让某个系统组件成为瓶颈，客户端会缓存元数据子系统、数据子系统和资源管理器的信息到本地，当执行文件操作时会优先读取本地缓存。当然某些组件（比如资源管理器）还是有可能在某一天成为瓶颈，但是基于京东的经验这件事情基本上不会发生。</p>

<p>在与 Ceph 的评测中，CFS 平均有 3 倍的 IOPS 提升，特别是多客户端和随机读写的场景。这很大程度上得益于元数据和数据节点分离的设计，且 CFS 的元数据是全内存存储，而 Ceph 并不是。</p>

<p>分布式文件系统一直都是比较重要的基础组件，在分布式数据库、大数据、机器学习领域有广泛应用。常见的分布式文件系统如 HDFS、Ceph，在如今这个全面推行容器化的时代越来越显得捉襟见肘。容器化一个很大的特点是快速扩缩容，传统的存储系统在这一点上是非常不友好的，因此才会有越来越多针对容器化场景的基础组件诞生（具体可以访问 <a href="https://www.cncf.io">CNCF</a> 查看），这里介绍的 CFS 是一个例子，另一个类似的是 <a href="https://juicefs.com">JuiceFS</a>。</p>

<p>CFS 目前属于 CNCF 下的 <a href="https://www.cncf.io/sandbox-projects">sandbox 项目</a>，且已经<a href="https://github.com/chubaofs/chubaofs">开源</a>，使用 Go 语言编写。</p>

<h2>tensorflow/community #237: RFC: Sparse Domain Isolation for Supporting large-scale Sparse Weights Training</h2>

<p><a href="https://github.com/tensorflow/community/pull/237">[链接]</a></p>

<p>推荐系统大规模稀疏特征分布式训练一直是工业界一件有挑战的事情，大公司内部自研的训练框架大多已经解决了这个问题，但是在开源社区问题仍然存在。TensorFlow 作为也许目前最流行的深度学习训练框架，社区里也早有相关的讨论（比如 <a href="https://github.com/tensorflow/tensorflow/issues/19324">#19324</a>、<a href="https://github.com/tensorflow/tensorflow/issues/24539">#24539</a>、<a href="https://github.com/tensorflow/tensorflow/pull/24915">#24915</a>），但基本都以烂尾告终。最新的 RFC #237 来自腾讯，区别于现有的一些开源实现（比如阿里巴巴的 <a href="https://github.com/alibaba/x-deeplearning">XDL</a>、字节跳动的 <a href="https://github.com/bytedance/byteps">BytePS</a>、蚂蚁金服的 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a>）完全自己重新造了一个 parameter server，腾讯的方案最大限度复用了 TensorFlow 现有的组件，对用户的代码侵入也最小。目前这个 RFC 还在讨论中，有兴趣可以订阅 PR。</p>

<h2>深入云原生 AI：基于 Alluxio 数据缓存的大规模深度学习训练性能优化</h2>

<p><a href="https://mp.weixin.qq.com/s/2Pj8erPbYuMo7mBJvweJgQ">[链接]</a></p>

<p>机器学习模型训练由于依赖大量的数据作为输入，因此数据 I/O 的性能会直接影响模型训练的效率。有时间会发现计算设备的算力升级了，但是数据 I/O 跟不上了，反而拖慢了整个训练流程。阿里云团队分享的这篇文章便是他们在使用 Alluxio（试图）加速数据 I/O 的过程中的经验，虽然最后的优化结果性能指标其实也只是基本跟本地读取持平。</p>

<h2>Rob Pike interview: “Go has indeed become the language of cloud infrastructure”</h2>

<p><a href="https://evrone.com/rob-pike-interview">[链接]</a></p>

<p>没啥好介绍的了，值得一读的一篇采访。文中有两个有趣的问题：</p>

<ul>
<li><strong>对于 Rust 宣称的「没有 GC」的设计有什么看法</strong>：Rob Pike 只是表示了他对 Rust 很感兴趣，其它意见不便发表。</li>
<li><strong>如果可以时间旅行到最初设计 Go 的时候想给自己一个什么忠告</strong>：无视那些仇恨者（haters），只需要聆听那些理解以及和你有共同目标的人的声音。不可能每一个人都认同你正在做的事情，但是那些鼓励你前进的人会是提供给你非常棒（fantastic）的想法、能量和灵感的源泉。</li>
</ul>


<h2>孤芳「自赏」：盯鞋音乐的前世与今生</h2>

<p><a href="https://www.gcores.com/articles/121368">[上]</a> <a href="https://www.gcores.com/articles/123770">[下]</a></p>

<p>这两篇文章来自「竟然还能聊游戏」的机核，相对系统地介绍了「盯鞋（shoegaze）」这种音乐风格，作为目前可能是除了后朋克以外我最喜欢的音乐风格非常高兴能够有人科普，稍微欠缺的是文中没有提到任何中国的乐队。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何设计与实现一个分布式索引框架（四）：索引更新]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/13/how-to-design-a-distributed-index-framework-part-4/"/>
    <updated>2020-05-13T14:55:43+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/13/how-to-design-a-distributed-index-framework-part-4</id>
    <content type="html"><![CDATA[<blockquote><p>这是一个<a href="https://blog.xiaogaozi.org/categories/htdadif/">系列文章</a>，大部分内容都来自我过去在小红书发现 Feed 团队工作期间的实践和经验。在介绍的过程中我会尽量不掺杂过多的业务细节，而专注于这背后我个人一些浅薄的设计思想，希望你在阅读完这些文章以后能够直接或者间接地拓展到不同的场景。</p></blockquote>

<p><a href="https://blog.xiaogaozi.org/2020/04/24/how-to-design-a-distributed-index-framework-part-3/">上一篇文章</a>介绍了如何实现正排索引和二级索引，但要创建索引也得先有数据才行，本篇将会介绍数据是如何更新的。</p>

<!-- more -->


<h2>全量索引</h2>

<p>所谓「全量索引（full index）」就是指需要索引的数据的全集，通常全量索引的数据量都是一个比较大的量级<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>，离线构建一次全量索引的时间成本也比较高，因此更新频率不会特别频繁<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。全量索引的更新很简单，一般就是覆盖线上已经存在的那份旧的全量索引，当然这个更新流程不会是直接替换，而是先把新的数据加载好再进行替换，也就是说在更新的过程中需要保证内存中能够同时存放两份数据。</p>

<p>全量索引有几个比较严重的问题：</p>

<ul>
<li>索引的数据量决定了它的更新频率不会很快，而且有变化的数据在这个全集中必定是少数，每次都更新全部数据有点浪费。</li>
<li>索引更新过程中需要临时存储双份数据，会有大量新对象产生，对 GC 的压力也会很大。很多时候我们选择不频繁更新全量索引也是这个原因，这就进一步加剧了上一个问题的影响。</li>
</ul>


<p>解决思路其实也很直接，既然需要更新的数据是少数，那每次索引更新就只更新这部分数据好了，这也就是下一章节要着重介绍的内容。</p>

<h2>增量索引</h2>

<p>与「全量索引」一起经常被提及的另一个词就是「增量索引（incremental index）」，顾名思义增量索引是只针对增量数据构建的集合，因此索引的数据量也会小非常多，自然更新频率就可以很快了。构建增量索引并不是一件特别复杂的事情，只需要有办法获取到最近一段时间有变化的内容就行<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>。但是构建好的增量索引要如何更新到线上是一个值得认真思考的问题，有两种方案可以选择：</p>

<ol>
<li>直接修改全量数据的倒排索引和正排索引</li>
<li>单独为增量数据创建倒排索引和正排索引</li>
</ol>


<p>第 1 种方案如果是新增的内容比较简单，在倒排索引和正排索引中插入新的条目即可。但如果是旧的内容被更新或者删除，那就需要在这两种索引中找到对应的条目并全部更新或者删除。直接原地更新或者删除对于倒排索引来说因为需要扫描整个索引条目列表，时间复杂度会随着列表长度以及增量更新的数据量线性增长；对于正排索引来说堆外内存不可避免会产生空间碎片，必须定期清理碎片以免造成空间浪费。</p>

<p>第 2 种方案创建索引的逻辑跟全量索引是一样的，只不过是针对增量数据。但是此时相当于就存在了多个倒排索引和正排索引，查询逻辑应该怎样实现呢？由于正排索引是一一映射，因此如果有多个相同 primary key 的索引，那在查询时选择最新的那个索引即可。查询倒排索引稍微复杂一点，同一个倒排索引 key 可能在多个索引中都存在，查询时需要同时从这些索引中遍历，最终选取出 top N 的条目<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>。遍历时除了用户提供的过滤器以外，还需要过滤那些已经被删除的条目，这可以通过一个全局的已删除条目集合来实现。随着增量索引数量的增多，不同索引间冗余的数据会变得越来越多，浪费存储空间的同时也会增加查询的时间复杂度。因此我们需要不定期合并这些索引，去除那些重复或者被删除的条目。</p>

<p>我们最终选择了方案 2，因为整体上更倾向于把存储的数据结构设计成 append-only 的模式，简化底层存储的实现<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>。熟悉数据库系统设计的朋友可能已经发现方案 2 同现在流行的 <a href="https://github.com/google/leveldb">LevelDB</a>、<a href="https://rocksdb.org">RocksDB</a> 有一些相似的地方，事实上我们在设计时也的确借鉴了它们的部分思想。这两者底层都是 <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">LSM tree</a> 的数据结构，简单介绍 LSM tree 就是将数据分为多个 level，每个 level 的数据都是只读的且可能存在冗余，不同 level 之间会通过压缩（compaction）来去掉这些冗余。下图是增量索引的设计示意图。</p>

<p><img src="https://blog.xiaogaozi.org/images/posts/incremental_index_design.png" alt="incremental index design" /></p>

<p>我们限定最大的 level 数（即增量索引数），如果超过这个限定值就会触发合并。大部分情况下都会是增量索引之间进行合并，但如果合并之后的大小已经超过全量索引大小的某个比例，就会触发 1 次同全量索引的合并。</p>

<p>有了增量索引之后索引的更新频率最快可以控制在分钟级，相比全量索引动辄小时级甚至天级的频率已经快了不少。索引更新更快也意味着内容可以更快地被用户消费，促进了整个社区的信息流动。</p>

<p>以上就是本篇要介绍的全部内容，简单回顾一下：</p>

<ul>
<li>全量索引虽然构建成本很高但也是不可或缺的，它有着最全的业务数据。</li>
<li>增量索引的目的是为了加快索引更新频率，设计上借鉴了部分 LSM tree 的思想。</li>
</ul>


<p>注意过这个系列文章标题的朋友可能很好奇讲了这么久为啥感觉跟分布式一点儿关系都没有，的确前面几篇文章都是在重点介绍索引相关的技术，下一篇文章将会开始聊聊分布式这个话题，敬请期待。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>当然数据量有多大取决于你的业务数据有多少<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>小时级、天级、周级都有可能<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>如何获取有非常多的方案，比如 MySQL 的 binlog，MongoDB 的 oplog。基础服务做得比较好的公司还会将不同存储的更新消息聚合到类似消息队列的系统中，方便下游业务消费。<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>假设现在有 3 个倒排索引，那是不是得从这 3 个倒排索引中都选出 top N 以后才能得到最终的结果呢（即总共需要查询 3 x N 个条目）？答案是不用，一种优化的实现方案是同时比较 3 个倒排索引的头部，挑选最大的那个条目，然后一直重复这个步骤直到满足选出 N 个条目，这样总共需要查询的条目数仍然是 N。<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>比如堆外内存从设计上就不用考虑更新和删除操作<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #0]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/11/maybe-news-issue-0/"/>
    <updated>2020-05-11T14:44:52+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/11/maybe-news-issue-0</id>
    <content type="html"><![CDATA[<blockquote><p>前言：从这一期开始这个系列将会有一个正式的名字「Maybe News」，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。本身我分享的内容也很有可能是一些旧闻，只不过对于我来说是还未了解的知识罢了。<a href="https://blog.xiaogaozi.org/2020/04/26/weekly-reading-list-issue-1/">上一期</a>的名字还是维持原样就不做修改。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>这个系列的文章。</p></blockquote>

<!-- more -->


<h2>LightRec: a Memory and Search-Efficient Recommender System</h2>

<p><a href="http://staff.ustc.edu.cn/~liandefu/paper/lightrec.pdf">[链接]</a></p>

<p>这篇论文由微软亚洲研究院与中科大共同发表在 <a href="https://www2020.thewebconf.org">WWW 2020</a> 会议上，提出了一种新的表示物品向量的方法，大幅降低存储向量所需空间的同时还显著提升了召回效果。一个直观的数据：LightRec 将 1 千亿 256 维双精度向量的内存占用从 9.5 GB 降到了 337 MB，这是非常惊人的！现在工业界常用的 <a href="https://github.com/nmslib/nmslib">nmslib</a> 和 <a href="https://github.com/facebookresearch/faiss">Faiss</a> 都无法实现如此高的压缩比，因此很多时候都需要借助分布式存储来满足业务场景，如果真的如论文中所描述的一样那单机存储在未来很长一段时间来说都是完全足够的。</p>

<p>这里简单解释一下为什么向量召回对于当下的推荐系统如此重要，传统的召回是基于倒排索引的方式，正如我在<a href="https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1/">之前的一篇文章</a>中介绍的那样，召回与模型优化目标之间的差异较大导致召回效果始终较差。自从 <a href="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a> 这篇论文（同样也是由微软研究院发表）提出 DSSM（Deep Structured Semantic Models）以后，将召回与 DNN 进行结合，显著提升了召回的效果，在很多公司的实践中也的确论证了 DSSM 是一个非常有效的召回方式。DSSM 的核心是分别为物品和用户生成向量，再通过 ANN（Approximate Nearest Neighbors）查询相似向量从而实现召回。因此向量的存储和查询效率决定了在线请求的效果和性能，如何平衡向量索引的空间占用和召回效果是非常重要的。</p>

<p>微软研究院的微信公众号有一篇简短的针对这篇论文的<a href="https://mp.weixin.qq.com/s/E43gc16A3OVWgxyfdUxr7g">中文版介绍</a>，有兴趣也可以先看这篇文章。</p>

<h2>TFRT: A new TensorFlow runtime</h2>

<p><a href="https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html">[链接]</a></p>

<p>Google 近期开源了新的 TensorFlow 运行时 TFRT（TensorFlow Runtime），这是一个介于上层用户代码和底层设备之间的执行环境。项目的愿景是实现一个统一的、可扩展的、性能首屈一指（best-in-class）的，同时可跨越多种领域硬件（domain specific hardware）的运行时。未来 TFRT 会成为 TensorFlow 默认的运行时，目前还在集成中。从 ResNet-50 的 inference 测试结果上看平均提升了 28% 的性能。</p>

<h2>Why We Need DevOps for ML Data</h2>

<p><a href="https://tecton.ai/blog/devops-ml-data">[链接]</a></p>

<p>虽然这是一篇产品推广软文（在文章最后一节），但是文章中普及的关于 DevOps 与机器学习之间的关系还是非常有价值的。很多人可能以为机器学习就只是模型算法而已，诚然这是学术研究的基石，但是要真正把机器学习应用到工业界光有算法是远远不够的。Google 著名的 <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf.">Hidden Technical Debt in Machine Learning Systems</a> 论文已经论述了那些隐藏在模型背后的往往被人忽略的技术，模型规模越大需要付出的工程努力也是越大的（所以很多时候大公司才需要自己造轮子）。作为衍生阅读也可以同时看看 <a href="https://towardsdatascience.com/how-linkedin-uber-lyft-airbnb-and-netflix-are-solving-data-management-and-discovery-for-machine-9b79ee9184bb">How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions</a> 这篇文章。</p>

<h2>Mid-stack inlining in Go</h2>

<p><a href="https://dave.cheney.net/2020/05/02/mid-stack-inlining-in-go">[链接]</a></p>

<p>Dave Cheney 继续科普 Go 的一些实现细节，这次的主题是编译器如何实现 mid-stack inlining。所谓 mid-stack inlining 就是将那些调用了其它函数的函数变成 inline，相对的还有 leaf inlining，即不调用任何其它函数。有兴趣了解 leaf inlining 的可以看 Dave Cheney 的<a href="https://dave.cheney.net/2020/04/25/inlining-optimisations-in-go">上一篇文章</a>。</p>

<h2>Why We Leverage Multi-tenancy in Uber’s Microservice Architecture</h2>

<p><a href="https://eng.uber.com/multitenancy-microservice-architecture">[链接]</a></p>

<p>Uber 介绍了他们在微服务领域实践的一个经验「多租户」，简单讲就是让请求链路上的所有组件和系统都能够感知「租户」这个概念，比如租户可以分为生产环境和测试环境。Uber 列举了两个应用场景：集成测试和 Canary 部署，这两个场景都依赖生产环境的请求，有了租户的概念就可以自动进行请求路由和数据隔离。愿景其实挺美好，但「代价」也是不容忽视，前面讲了要让所有组件和系统都感知就非常依赖基础组件的统一，要解决这个问题很多时候并不单纯是一个技术问题。如何做好不同环境的数据隔离也是一个难题，关于这一点文章并没有做特别详细的介绍。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weekly Reading List Issue #1]]></title>
    <link href="https://blog.xiaogaozi.org/2020/04/26/weekly-reading-list-issue-1/"/>
    <updated>2020-04-26T12:22:08+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/04/26/weekly-reading-list-issue-1</id>
    <content type="html"><![CDATA[<!-- more -->


<h2>FoundationDB Record Layer: A Multi-Tenant Structured Datastore</h2>

<p><a href="https://arxiv.org/abs/1901.04452">[链接]</a></p>

<p>FoundationDB 2015 年被 Apple <a href="https://techcrunch.com/2015/03/24/apple-acquires-durable-database-company-foundationdb">收购</a>并于 2018 年<a href="https://www.foundationdb.org/blog/foundationdb-is-open-source">开源</a>，作为 Apple 为数不多的开源项目受到广泛关注。简单介绍 FoundationDB 是一个基于 Paxos 的分布式 KV 存储，底层存储结构是 B-tree（是的，并不是 LSM tree），定位上跟 Google 的 Spanner 非常相似。这篇论文发表在 <a href="https://sigmod2019.org/sigmod_industry_list">SIGMOD 2019</a>，介绍的是基于 FoundationDB 的 record-oriented 结构化存储框架（也已经<a href="https://github.com/FoundationDB/fdb-record-layer">开源</a>）。<a href="https://apple.github.io/foundationdb/layer-concept.html">Layer</a> 是 FoundationDB 一个很有特色的概念，在最基本的 KV 上无限扩展更加复杂的数据模型。这个框架整体上有几个亮点：</p>

<ul>
<li>基于 Protocol Buffers 的数据模型定义</li>
<li>丰富的索引类型支持（单字段索引、嵌套字段索引、列表字段索引、聚合索引、rank 索引、全文索引、多字段联合索引等），并且索引是可以跨表的（这里简单将 record type 理解为表）。</li>
<li>基于 Java 的查询 API（并不是 SQL）</li>
</ul>


<p>目前已经被应用在 <a href="https://developer.apple.com/icloud/cloudkit">CloudKit</a>，替代旧的 Cassandra + Solr 架构（旧架构也有<a href="https://dl.acm.org/doi/10.1145/3164135.3164138">一篇论文</a>介绍）。CloudKit 作为一个庞大的存储服务供所有 Apple 生态的应用和用户使用，这也就是论文标题中 Multi-Tenant 的含义。</p>

<h2>Introducing Dispatch</h2>

<p><a href="https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072">[链接]</a></p>

<p>Incident 管理一直是 DevOps 领域比较热门的话题，Netflix 开源了他们自己的 incident 管理工具 <a href="https://github.com/Netflix/dispatch">Dispatch</a>，更早之前 LinkedIn 也<a href="https://engineering.linkedin.com/blog/2017/06/open-sourcing-iris-and-oncall">开源</a>过类似的东西。</p>

<h2>Agent57: Outperforming the human Atari benchmark</h2>

<p><a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark">[链接]</a></p>

<p>大众对于 DeepMind 的认知恐怕就是<a href="https://en.wikipedia.org/wiki/AlphaGo">下下围棋</a>、<a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">打打星际</a>，最近又搞起了雅达利的游戏，可以说是把强化学习玩儿出花儿了。最新一代的 Agent57 已经可以在全部 57 个游戏里战胜人类玩家。</p>

<h2>An Illustrated Guide to Graph Neural Networks</h2>

<p><a href="https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783">[链接]</a></p>

<p>Graph Neural Networks（GNN）最近几年已经火得不行，Amazon 也开源了相关的框架 <a href="https://www.dgl.ai/">DGL</a>。这篇文章以一种简单的示意图的形式介绍什么是 GNN，帮助不了解 GNN 的人建立一个简单的认知。</p>

<h2>Debugging with Delve</h2>

<p><a href="https://tpaschalis.github.io/delve-debugging">[链接]</a></p>

<p><a href="https://github.com/go-delve/delve">Delve</a> 是一个 Go 语言的 debugger，Go 官方也<a href="https://golang.org/doc/gdb">推荐</a>优先考虑使用它而不是 GDB。这篇文章简单介绍了 Delve 的基本功能，其实跟 GDB 的使用方式很类似，但是 Delve 的亮点在于可以理解 Go 语言的语义以及调试 goroutine。</p>

<h2>gofiber/fiber</h2>

<p><a href="https://github.com/gofiber/fiber">[链接]</a></p>

<p>Fiber 是（又）一个 Go 语言的 HTTP 框架，设计上很大程度受了 Node.js 中非常流行的 <a href="https://expressjs.com">Express</a> 启发（API 非常相似）。得益于底层使用的 <a href="https://github.com/valyala/fasthttp">fasthttp</a> 库，在 Fiber 自己的<a href="https://docs.gofiber.io/benchmarks">评测</a>中超越了很多市面上现有的框架。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何设计与实现一个分布式索引框架（三）：正排索引]]></title>
    <link href="https://blog.xiaogaozi.org/2020/04/24/how-to-design-a-distributed-index-framework-part-3/"/>
    <updated>2020-04-24T16:29:40+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/04/24/how-to-design-a-distributed-index-framework-part-3</id>
    <content type="html"><![CDATA[<blockquote><p>这是一个<a href="https://blog.xiaogaozi.org/categories/htdadif/">系列文章</a>，大部分内容都来自我过去在小红书发现 Feed 团队工作期间的实践和经验。在介绍的过程中我会尽量不掺杂过多的业务细节，而专注于这背后我个人一些浅薄的设计思想，希望你在阅读完这些文章以后能够直接或者间接地拓展到不同的场景。</p></blockquote>

<p><a href="https://blog.xiaogaozi.org/2020/04/22/how-to-design-a-distributed-index-framework-part-2/">上一篇文章</a>介绍了如何定义 schema、查询 API 以及怎样实现倒排索引，本篇将会着重介绍另一种重要的索引类型「正排索引」，以及跟正排索引密切相关的「二级索引」。</p>

<!-- more -->


<h2>正排索引</h2>

<p>正排索引是主键（primary key）到条目的一一映射，在推荐系统中使用正排索引的场景是获取模型计算所需的原始特征（raw feature）。为什么说是原始特征呢？因为这些数据还需要经过特征提取（feature extraction）以后才能作为最终输入给模型的参数，特征提取不在本系列文章的讨论范畴。</p>

<p>这里先简单讲讲什么是<a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)">特征</a>。最早我们提到机器学习的时候讲过模型是首先经过离线训练产生，然后用于在线预测去预估用户的喜好。在离线训练阶段算法工程师需要先从训练数据<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>人工筛选出一批对于当前想要训练的模型有意义、有价值的、可衡量的属性，这个过程叫做<a href="https://en.wikipedia.org/wiki/Feature_engineering">「特征工程（feature engineering）」</a>。特征工程考验的是一个算法工程师对于业务数据的理解、经验、数据敏感度以及统计分析能力，很多时候还需要结合大量的 A/B 实验才行。近年来深度学习的兴起已经将特征工程的复杂度降低不少，但特征工程依然是一个非常重要的步骤。这些被筛选出来的属性就是特征，举个直观的例子下面这些都可以作为模型特征使用：用户的地理位置、用户性别、用户看过的内容总曝光/点击/赞/评论的次数等。</p>

<p>正排索引中的条目存储的就是大量的原始特征，这些特征也是在 schema 中定义，基本上 schema 中除了跟倒排索引有关的字段其它都属于特征。因此可以看到正排索引条目的大小是远大于倒排索引条目的。为了保证查询的性能我们依然选择了将正排索引存储在内存中，但是这会带来一个问题，因为正排索引占用的空间可能会很大，我们也是明确知道这些数据是需要常驻在内存中的，对于类似 Java 这种带有 GC 的语言来说这部分数据反而会增加垃圾回收器的压力。这些数据会长期存储在 old generation 中，不仅浪费空间也降低了 GC 的性能。那么我们的目标便是尽量不要让这部分数据对 GC 造成太大的影响，最好是对 GC 不可见的，毕竟「眼不见心不烦」。</p>

<p>一种比较常见的解决方案是「堆外内存（off-heap）」<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>。所谓堆外内存就是通过某些特殊的 API 分配独立的内存空间，且这个内存空间对于 GC 是不可见的，当然也就不会影响 GC。听起来这个方法似乎很简单直接，但凡事有好也有坏，绕开 GC 的副作用是你需要自己管理这块儿内存，如何高效地使用堆外内存是一个比较关键的问题。HBase 的 <code>BucketCache</code><sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>是一个值得参考的实现，我们在调研阶段也仔细研究过 <code>BucketCache</code> 的设计，但最终没有直接照搬，有一个非常重要的原因：<code>BucketCache</code> 是为了解决之前 <code>BlockCache</code> 这种 on-heap 缓存方案造成的 GC 性能问题而诞生，本质上也还是一个缓存，既然是缓存就必定要考虑缓存数据的驱逐，因此 <code>BucketCache</code> 的设计方案中包含如何释放内存以及合并 bucket 的逻辑。但是正排索引不是缓存，也就不存在驱逐数据的问题<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>，<code>BucketCache</code> 中的这部分设计对于我们的场景来说其实是多余的，如果完全照搬反而是在系统中引入了一个不必要的复杂组件，增加维护成本<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>。因此我们最终借鉴了一部分 <code>BucketCache</code> 的设计思想同时再结合推荐系统的业务特点实现了一个只读版本的堆外内存，下图是具体的实现方案。</p>

<p><img src="https://blog.xiaogaozi.org/images/posts/off_heap_design.png" alt="off-heap design" /></p>

<p>上图中最左边蓝色的部分是一个 hash map，key 是正排索引的主键，value 是一个包含与堆外内存地址有关的元信息对象。这个元信息对象中主要有 3 个成员变量：</p>

<ol>
<li>BB Index：BB 是 <a href="https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html"><code>java.nio.ByteBuffer</code></a> 的缩写，是 Java 中创建堆外内存的底层 API。在应用的初始化阶段我们会提前申请一块儿大的物理内存空间作为堆外内存，假设这块儿内存的大小是 10GB，在其中会按照一个固定的大小（默认是 10MB）再分割成多个小的 bucket。BB Index 即是某个 bucket 的索引。</li>
<li>Offset：每个 bucket 中存储了很多正排索引条目，offset 是某个条目在当前 bucket 中的偏移。</li>
<li>Length：这个很好理解，就是索引条目的长度。</li>
</ol>


<p>上图中蓝色和黄色的部分还是存储在堆内，只有绿色部分属于堆外。写入数据的流程就是根据索引条目的长度找到空闲的 bucket，然后通过 <code>ByteBuffer.put()</code> 方法将数据存放到堆外内存，并在 hash map 中新增相应的元信息。读取数据的流程是首先查找元信息，然后通过 HBase 中封装的 <a href="https://github.com/apache/hbase/blob/master/hbase-common/src/main/java/org/apache/hadoop/hbase/util/UnsafeAccess.java"><code>UnsafeAccess.copy()</code></a> 方法将数据从堆外拷贝到堆内。当然数据拷贝出来以后并不能直接使用，因为这还只是序列化后的字节流，还需要经过反序列化步骤<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>。</p>

<h2>二级索引</h2>

<p>二级索引的概念在很多数据库系统中也存在，特别是分布式数据库，通常主键用来查询分片的位置，而二级索引用来在某个具体的分片中查询特定的字段。</p>

<p>在推荐系统场景中二级索引的功能类似，只不过不是因为这是一个分布式系统，而是为了查询某些特殊的特征。在传统的机器学习模型中有一类特征是非常重要的，那便是内容在不同维度的统计值。举个例子，我们不仅会统计一篇笔记的总曝光数，还会统计这篇笔记在不同城市、不同性别、不同类型设备的曝光数，这里的城市、性别、设备类型就是维度。并且这些维度是允许交叉的，也就会产生非常多的维度组合。所有这些维度组合起来的统计值是一个大的集合，每次查询时并不需要这个集合中的所有值，而是根据当前用户的画像选取与这个用户相符的值。</p>

<p>如果每次查询时都把所有值从堆外内存中拷贝出来显然是很浪费的，因此我们需要一种方法直接从堆外内存中查询部分值，这就是二级索引的作用。二级索引的字段在 schema 中会标记 <code>secondary_key</code> 属性，上一篇文章中示例的字段类型是 <code>[BreakdownStats]</code>，那这个 <code>BreakdownStats</code> 的定义是什么呢？如下所示：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>table BreakdownStats {
</span><span class='line'>  key:SecondaryKey (id: 0);
</span><span class='line'>  value:NoteEngagementStats (id: 1);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>上面 <code>key</code> 字段的数据类型是 <code>SecondaryKey</code>，这是一个由框架预定义的类型，具体定义如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>table SecondaryKey {
</span><span class='line'>  type:int (id: 0);
</span><span class='line'>  value:string (id: 1);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>因此我们可以知道一个二级索引 key 由两部分组成：<code>type</code> 和 <code>value</code>，<code>type</code> 是 key 的类型（通常是可枚举的），<code>value</code> 是具体的值（不可枚举）。继续拿前面的例子举例，「城市」是一种 key 的类型，「上海」是具体的值。于是查询流程相比前面介绍的区别之处在于，通过主键查找以后还需要通过二级索引 key 才能获取到元信息对象，相当于增加了一次 hash map 的查找。一个优化的细节点是在框架内部我们还将二级索引 key 映射到了一个整数，这样便可以将这个整数作为 hash map 的 key 来使用。于是通过刚才的流程便实现了直接获取某个维度（或者维度组合）的统计值的需求。</p>

<p>以上就是本篇要介绍的全部内容，简单回顾一下：</p>

<ul>
<li>基于堆外内存的正排索引</li>
<li>通过二级索引实现查询部分特征</li>
</ul>


<p>至此两种索引已经全部介绍完毕，下一篇文章将会围绕一个更加上层的问题「索引如何快速更新」进行讨论。数据不可能是一成不变的，更新的效率也会直接影响产品体验和业务指标。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>训练数据往往是用户的历史行为数据，例如用户看过、点过、赞过、评论过的所有内容。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>除了堆外内存这种方案还有一些其它可参考的解决方案，例如阿里巴巴曾经<a href="https://blog.csdn.net/alitech2017/article/details/80133021">分享</a>过的一些经验（文章中提到的 AliGC 多租户功能近期已经在 <a href="https://github.com/alibaba/dragonwell8/wiki/Alibaba-Dragonwell8-Release-Notes">Alibaba Dragonwell 8.3.3-GA</a> 中开源）；Netflix 的开源框架 <a href="https://hollow.how/advanced-topics/#in-memory-data-layout">Hollow</a>。<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><code>BucketCache</code> 的详细设计可以参考 <a href="https://issues.apache.org/jira/browse/HBASE-7404">HBASE-7404</a> 这个 issue<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>本质上索引是只读的<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>也许有人会问索引数据难道是不更新的吗？答案是需要更新，有关如何更新索引数据会在下一篇文章中介绍。<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>频繁从堆外拷贝大量数据并反序列化可能会是一个比较耗时的过程，因此我们在实际使用时还在正排索引上增加了一层堆内的缓存。<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何设计与实现一个分布式索引框架（二）：Schema、API 及倒排索引]]></title>
    <link href="https://blog.xiaogaozi.org/2020/04/22/how-to-design-a-distributed-index-framework-part-2/"/>
    <updated>2020-04-22T18:16:55+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/04/22/how-to-design-a-distributed-index-framework-part-2</id>
    <content type="html"><![CDATA[<blockquote><p>这是一个<a href="https://blog.xiaogaozi.org/categories/htdadif/">系列文章</a>，大部分内容都来自我过去在小红书发现 Feed 团队工作期间的实践和经验。在介绍的过程中我会尽量不掺杂过多的业务细节，而专注于这背后我个人一些浅薄的设计思想，希望你在阅读完这些文章以后能够直接或者间接地拓展到不同的场景。</p></blockquote>

<p>在<a href="https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1/">上一篇文章</a>中简单介绍了什么是推荐系统以及实现一个推荐系统的核心组件有哪些，文章最后引入了一个非常重要的概念「索引」，本篇将会首先从框架使用者的角度介绍如何定义索引，框架有哪些 API 可以使用以及从设计者的角度介绍如何实现一个简单的倒排索引。</p>

<!-- more -->


<h2>Schema</h2>

<p>在传统的数据库系统中，当我们提到 schema 时通常是指表（table）的逻辑定义，这个定义中会包含这些信息：表名、有哪些列（column）、列名、列的数据类型、主键（primary key）、索引名、索引的列等。非常类似的，在推荐系统中我们也需要这样的信息。框架的使用者需要首先定义好存储的数据实体，如实体名（表名）、实体有哪些字段（列）、字段的名称和数据类型、哪个字段是主键、哪些字段需要创建倒排索引。正如传统数据库系统中通过 SQL 来定义 shcema，我们也需要一种类似的 DDL<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>。经过一番调研和比较以后，我们选用了 <a href="https://google.github.io/flatbuffers">FlatBuffers</a> 作为定义 schema 的语言。同 <a href="https://developers.google.com/protocol-buffers">Protocol Buffers</a>（以下简称 PB）一样，FlatBuffers 也是 Google 开源的一种序列化协议，支持多种主流语言。为什么要选用 FlatBuffers 呢？FlatBuffers 的主页上列举了几个特点，我选取了几个最重要的翻译过来<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>，如果你熟悉 PB、Thrift 这一类 IDL 应该能很明显看出区别。</p>

<ul>
<li><strong>无需反序列化即可访问序列化后的数据</strong>：将 FlatBuffers 同其它协议区分开来的一个重要原因是 FlatBuffers 通过平展的二进制缓冲区（flat binary buffer）表示层级数据（hierarchical data），因此无需反序列化（parsing/unpacking）即可直接访问数据。同时依然支持数据结构的演变（evolution），保持向前和向后兼容性。</li>
<li><strong>高效的内存空间和访问性能</strong>：当访问数据时唯一需要分配的内存就只有数据本身的缓冲区（buffer），不需要任何额外的内存空间（C++ 语言支持，其它语言可能有变化）。FlatBuffers 也非常适合用于 mmap（或者流式处理），允许只有部分缓冲区在内存中。访问序列化后的数据基本等价于访问原始的结构体（struct），只会增加一次额外的跳转（一种虚表）来实现数据格式的演变（evolution）和可选字段。FlatBuffers 旨在应用于那些不接受耗费大量时间和空间访问或者构建序列化数据的项目，例如游戏或者任何其它对性能敏感的应用。点击查看<a href="https://google.github.io/flatbuffers/flatbuffers_benchmarks.html">性能测试</a>了解更详细的信息。</li>
</ul>


<p>有兴趣进一步了解设计细节的朋友可以看看官网的 <a href="https://google.github.io/flatbuffers/flatbuffers_internals.html">FlatBuffers Internals</a> 文档，简单总结就是 FlatBuffers 通过一种特殊的序列化格式（针对更小的内存开销和访问性能设计）相比传统 IDL 更加高性能，同时又兼具传统 IDL 的大部分特性（语言无关、强类型、schema evolution）。当然 FlatBuffers 也不是没有缺点，最明显的一个问题就是为了实现高性能，FlatBuffers 的原始 API 对开发者及其不友好，手动编写序列化或者读取数据<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>的代码非常容易出错。不过好在这些问题都可以通过自动生成的代码和框架隐藏起来，不需要直接暴露给用户<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>。前面列举的几个特点为什么对于索引框架如此重要呢？笼统讲当然是为了高性能，不过后面介绍倒排索引的设计时会详细说明一些细节点。</p>

<p>说了这么多还是不知道具体的 schema 长什么样子，下面以一个实际的例子来说明。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>table NoteInfo {
</span><span class='line'>  note_id:string (id: 0, primary_key);
</span><span class='line'>  ...
</span><span class='line'>  note_gender:NoteGender (id: 29, index_attribute);
</span><span class='line'>  taxonomies:[KeyValueEntry] (id: 30, index_key);
</span><span class='line'>  ...
</span><span class='line'>  breakdown_stats:[BreakdownStats] (id: 47, secondary_key);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>上面是一个完整的索引实体定义，也就是小红书里用户创建的笔记（note）。每一行定义了实体中的字段名称、数据类型以及可选的属性标记。例如 <code>note_id</code> 这个字段是笔记的 ID，数据类型是 <code>string</code>，<code>id: 0</code> 是字段在 FlatBuffers 中的唯一 ID，<code>primary_key</code> 表示这个字段是主键。类似的后面列举的几个字段也具有某些特殊含义，例如 <code>NoteGender</code> 是一个枚举值，<code>index_attribute</code> 表示这是一个索引属性；<code>[KeyValueEntry]</code> 是一个 <code>KeyValueEntry</code> 类型的数组，<code>index_key</code> 表示这是一个倒排索引；<code>secondary_key</code> 表示这是一个二级索引。可以看到语法上 FlatBuffers 跟传统 IDL 类似，某种意义上可能还略微简洁一些。定义里有些是 FlatBuffers 官方的语法（如 <code>id: 0</code>），还有一些是我们扩展的（如 <code>primary_key</code>）<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>。这里扩展性是非常有必要的，否则这个 IDL 就只能用于序列化而没法作为一种数据的逻辑定义语言来使用了。这些扩展的语法具体是什么意思之后的几篇文章会逐渐展开。</p>

<h2>API</h2>

<p>有了 schema 框架就可以理解索引的数据结构了，但是对于使用者来说其实更加关心的是如何「查询」数据。推荐系统的业务特点是一个读远大于写的场景，且在线请求中只会涉及读数据而不涉及写数据，即请求都是只读的。结合上一篇文章的介绍，使用者真正需要用到的 API 基本就是下面几种：</p>

<ol>
<li>查询正排索引</li>
<li>查询倒排索引</li>
<li>查询二级索引</li>
</ol>


<p>以 Java 语言为例，实际的 API 大概长这样：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">QueryApi</span><span class="o">.</span><span class="na">queryByPrimaryKey</span><span class="o">(</span><span class="n">Object</span> <span class="n">primaryKey</span><span class="o">)</span>
</span><span class='line'><span class="n">QueryApi</span><span class="o">.</span><span class="na">queryByIndexKey</span><span class="o">(</span><span class="n">String</span> <span class="n">indexKeyName</span><span class="o">,</span> <span class="n">Object</span> <span class="n">indexKey</span><span class="o">,</span> <span class="kt">long</span> <span class="n">limit</span><span class="o">,</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">IndexPayload</span><span class="o">&lt;?&gt;,</span> <span class="n">Boolean</span><span class="o">&gt;</span> <span class="n">filter</span><span class="o">)</span>
</span><span class='line'><span class="n">QueryApi</span><span class="o">.</span><span class="na">queryBySecondaryKey</span><span class="o">(</span><span class="n">Object</span> <span class="n">primaryKey</span><span class="o">,</span> <span class="n">String</span> <span class="n">secondaryKeyName</span><span class="o">,</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">SecondaryKey</span><span class="o">&gt;</span> <span class="n">secondaryKeys</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>第 1 个 API 通过主键查询正排索引；第 2 个 API 通过倒排索引的字段 key 来查询倒排索引，同时还限定了查询的索引条目数以及一个用户自定义的过滤器；第 3 个 API 通过主键和二级索引 key 查询二级索引。</p>

<p>当然除了以上列举的最基本的 API 以外我们还提供了一些额外的接口，例如为了优化批量查询性能的批量查询接口，为了监控和可视化的索引统计信息查询接口。</p>

<h2>倒排索引</h2>

<p>假设给你一份序列化好的索引数据，要怎么创建倒排索引呢？这里有几个关键的问题需要思考：</p>

<ol>
<li>如何解析序列化的数据？</li>
<li>如何知道哪些字段需要创建倒排索引？</li>
<li>如何在运行时读取需要创建倒排的字段的值？</li>
<li>倒排索引在内存中的数据结构是什么？</li>
<li>倒排索引的条目列表如何排序？</li>
<li>如何实现在查询倒排索引的同时对条目进行过滤？</li>
</ol>


<p>第 1 个和第 2 个问题结合前面介绍 schema 时的知识应该很容易解答，只要框架能够提前获取到数据的 schema<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>，就能对索引数据有一个全局的了解，并能够事先知道哪些字段需要创建倒排索引。</p>

<p>第 3 个问题需要通过 FlatBuffers 提供的<a href="https://github.com/google/flatbuffers/blob/master/reflection/reflection.fbs">反射 API</a> 来解决<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>，配合 shcema 就能够从实际的数据中获取某个字段的值。还记得前面没有细讲的一个问题吗？为什么我们选用了 FlatBuffers 作为序列化协议，一个非常重要的原因就是<strong>无需反序列化即可访问序列化后的数据</strong>。在创建倒排索引时这个需求尤其强烈，一个完整的定义有可能包含几十甚至上百个字段，每个字段的大小都是不同的，但是这其中可能只有个位数的字段需要创建倒排索引，如果使用传统的 IDL 反序列化整个对象的时间和空间开销将会非常大，特别是对于有 GC 的语言来说<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>。因此在这一点上 FlatBuffers 基本完美解决了这个问题。</p>

<p>第 4 个问题思考的角度需要从查询性能出发，既然是索引那必然追求的是查询时间复杂度最小，那就没有比 O(1) 更小的复杂度了。能够实现 O(1) 查找的数据结构最常见的就是 hash map<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>，在不同语言中这都是非常基础的数据结构，基本不用操心是否需要自己从头开始实现。Hash map 的 key 就是倒排索引 key，value 就是索引的条目列表。而 value 应该用什么数据结构呢？倒排索引的 value 一定是有序的，且通常是倒序排列，最简单的场景用 array 其实就够了，如果需要动态增删那你可能会想到类似 <a href="https://en.wikipedia.org/wiki/Skip_list">skip list</a> 这样的数据结构。这里有一个细节点需要注意，同一个条目是有可能同时出现在不同的倒排索引中的，因此做好对象复用是节省内存非常关键的点。</p>

<p>回答第 5 个问题前可以先回到介绍 schema 时举的例子，倒排索引的字段是一个特殊的数据结构 <code>[KeyValueEntry]</code>，那么这个 <code>KeyValueEntry</code> 具体是什么呢？</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="n">table</span> <span class="n">KeyValueEntry</span> <span class="o">{</span>
</span><span class='line'>  <span class="nl">key:</span><span class="n">string</span> <span class="o">(</span><span class="n">key</span><span class="o">);</span>
</span><span class='line'>  <span class="nl">value:</span><span class="kt">double</span><span class="o">;</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>这是一个由用户自定义的数据结构，只有两个字段 <code>key</code> 和 <code>value</code>，前者即是倒排索引 key，而后者即是倒排索引条目的 score，同一个倒排索引 key 下的条目列表将会根据这个 score 从大到小逆序排序。这个特殊的数据结构是框架约定俗成的，只要符合一定条件就可以作为倒排索引的字段类型。</p>

<p>最后一个问题是在推荐系统的业务场景中相当常见的需求，通常查询时会限定查询 top N 的条目，但是对于不同用户这个 top N 可能是不一样的。例如需要过滤掉每个用户历史上曾经有过曝光（impression）的条目，需要根据某些用户画像属性过滤条目等。出于节省内存的原因我们不可能将一个完整定义中的所有字段都直接存放在内存中<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup>，因此限定了只有某些标记了特殊属性的字段才会存储在索引条目中，这也是前面示例中 <code>index_attribute</code> 这个标记的作用。因此一个完整的索引条目数据结构大概是这样（以 Java 语言为例）：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kd">class</span> <span class="nc">IndexPayload</span><span class="o">&lt;</span><span class="n">T</span> <span class="kd">extends</span> <span class="n">Comparable</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;&gt;</span> <span class="kd">implements</span> <span class="n">Cloneable</span> <span class="o">{</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">T</span> <span class="n">primaryKey</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">Object</span> <span class="n">indexKey</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kd">final</span> <span class="kt">double</span> <span class="n">score</span><span class="o">;</span>
</span><span class='line'>    <span class="kd">private</span> <span class="kd">final</span> <span class="n">Map</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;</span> <span class="n">attributes</span><span class="o">;</span>
</span><span class='line'>
</span><span class='line'>    <span class="kd">public</span> <span class="n">Object</span> <span class="nf">getAttribute</span><span class="o">(</span><span class="n">String</span> <span class="n">attrName</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">return</span> <span class="n">attributes</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">attrName</span><span class="o">);</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>上面的 <code>attributes</code> 成员变量即是索引属性，key 是标记了索引属性的字段名，value 是对应的值，可以通过 <code>getAttribute()</code> 方法查询这个值。前面介绍的 <code>QueryApi.queryByIndexKey()</code> 接口中有一个 <code>filter</code> 参数，数据类型是 <code>Function&lt;IndexPayload&lt;?&gt;, Boolean&gt;</code>，也就是说这个参数是一个函数，输入参数的数据类型是 <code>IndexPayload</code>，返回值的数据类型是 <code>Boolean</code>。用户需要自己实现过滤器的逻辑，通过 <code>IndexPayload</code> 提供的接口来判断是否需要过滤当前条目。</p>

<p>以上就是本篇要介绍的全部内容，简单回顾一下：</p>

<ul>
<li>基于 FlatBuffers 的 schema 定义</li>
<li>根据不同索引类型提供不同的查询 API</li>
<li>如何在运行时创建倒排索引</li>
</ul>


<p>下一篇文章依然是围绕索引来介绍，不过重点将会是正排索引，看似一个 hash map 即可解决的问题其实有很多玄机。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>为什么不直接用 SQL 呢？首先 SQL 的语法很复杂，很多原语是多余的，这对于使用者来说是不必要的负担。其次我们是实现一个推荐系统而不是一个完备的 DBMS，没必要硬套。最后这个 DDL 需要足够的扩展性来满足针对推荐系统的一些定制需求，关于这一点后面会提到。<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>需要查看原文和所有特点的朋友请转到 FlatBuffers 的官网<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>这里我刻意没有用「反序列化」这个词，理论上 FlatBuffers 是没有反序列化这个概念的，buffer is data（缓冲区即数据）。<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>框架使用者甚至不需要知道底层用的是 FlatBuffers<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>同样的设计思想在 <a href="https://github.com/FoundationDB/fdb-record-layer/blob/master/docs/Overview.md">FoundationDB Record Layer</a> 里也有所体现，只不过它使用的是 PB 作为 DDL，相比之下 FlatBuffers 的语法会更加简洁。<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>实际在实现时是通过框架暴露的注册 schema 的 API 由用户来提供这些信息<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
<li id="fn:7">
<p>截止 2020 年 4 月 FlatBuffers 官方依然没有提供 Java 语言的反射 API，有需要的朋友可以参考 <a href="https://github.com/google/flatbuffers/pull/4019">#4019</a> 这个 PR，虽然这个 PR 也烂尾了。<a href="#fnref:7" rev="footnote">&#8617;</a></p></li>
<li id="fn:8">
<p>如果你使用的是 Java 语言，即使用对象池这个问题也是没法优化的，类似 PB 这样的协议对于对象池的支持可以说是相当不友好。<a href="#fnref:8" rev="footnote">&#8617;</a></p></li>
<li id="fn:9">
<p>这里暂时忽略掉哈希碰撞<a href="#fnref:9" rev="footnote">&#8617;</a></p></li>
<li id="fn:10">
<p>至于完整的数据存放在哪里后续的文章中会介绍<a href="#fnref:10" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[如何设计与实现一个分布式索引框架（一）：概览]]></title>
    <link href="https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1/"/>
    <updated>2020-04-21T17:08:24+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1</id>
    <content type="html"><![CDATA[<blockquote><p>这是一个<a href="https://blog.xiaogaozi.org/categories/htdadif/">系列文章</a>，大部分内容都来自我过去在小红书发现 Feed 团队工作期间的实践和经验。在介绍的过程中我会尽量不掺杂过多的业务细节<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>，而专注于这背后我个人一些浅薄的设计思想，希望你在阅读完这些文章以后能够直接或者间接地拓展到不同的场景。</p></blockquote>

<p>在介绍什么是索引框架之前先了解一下我们当时面临的业务场景<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>，业界现在的 <a href="https://en.wikipedia.org/wiki/Activity_stream">feed 流</a>产品已经逐步从非个性化全面过渡到个性化，所谓的个性化 feed 其实就是<strong>基于机器学习的推荐系统</strong>。</p>

<!-- more -->


<p>先讲讲什么是推荐系统，用一个词概括就是「投其所好」。当你遇到一个跟你志趣相投的人时，那这个人感兴趣的东西很有可能也是你感兴趣的，这是「基于人」的维度进行推荐，微信的「朋友圈」就是这么一个简单的思路<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>。也有可能一个人他从来没见过你，你们也不互相认识，但是如果他能够知道你过去看过、喜欢过的东西，那他也很有可能可以推断出你未来感兴趣的东西，这是「基于历史行为」的维度进行推荐。我们可以通过制定一些人工的规则来实现推荐，但是用户的喜好是千奇百怪的<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>，有大量长尾的需求是人工规则无法覆盖的<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>。因此我们需要让计算机学习如何制定这些规则，这是对「机器学习」这个概念非常浅显的解释。一个完整的机器学习流程简单概括包含「离线」和「在线」两部分，离线部分是通过大量的用户数据来让计算机找寻其中的规律和共性，最终产出「模型」；在线部分是通过输入当前用户的数据给模型，让模型计算出一个预测值，这个预测值用来衡量我们想要推荐的内容是否符合这个用户的兴趣。这个系列的文章将会主要围绕在线部分，离线部分如果有机会会在以后的文章中介绍。</p>

<p>前面提到在线部分的核心逻辑是模型计算<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>，但在计算之前还有一个非常重要的工作是筛选候选集，通常叫做「召回（recall）」。所谓召回就是从一个很大的集合中通过一定的条件选取一个子集，为什么要有召回这一步呢？本质上是因为模型计算是一个非常耗费时间及资源的过程，如果每次用户请求都对整个集合中的条目进行计算，不仅浪费资源，所需的时间对于用户来说也是无法接受的<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>。大部分情况<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>下我们对于推荐系统一次请求的时间要求是控制在 100~200ms 左右，如果超过这个时间对业务指标一定会有负面影响。因此有针对性地进行召回就非常关键了，召回需要尽量确保筛选出来的候选集是符合当前用户兴趣的，但同时耗时又是非常短的<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>。总结一下一次推荐请求的流程如下图所示。</p>

<p><img src="https://blog.xiaogaozi.org/images/posts/recommendation_system_arch.png" alt="recommendation system architecture" /></p>

<p>实现快速召回的关键是「索引（index）」，正如大部分数据库系统一样，索引是为了实现快速查找的重要组件。在推荐系统中主要有两类索引：正排索引（forward index）和倒排索引（inverted index）<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup>。正排索引通常是用来通过一个主键（primary key）查询一个条目，是「一对一的映射」；倒排索引是用来通过跟条目关联的某些属性查询多个条目，是「一对多的映射」。举个实际的例子，小红书上用户发布的内容叫做「笔记」，每一篇笔记都会生成一个唯一的 ID，这个 ID 就是这篇笔记的主键，正排索引即是一个从笔记 ID 到笔记的映射。而每篇笔记都会有一些同笔记本身相关的属性，比如分类（category），一些常见的分类有：旅行、美妆、摄影、美食等。倒排索引即是一个从多个属性到多篇笔记的映射，如「旅行」分类可以映射到所有属于这个类别的笔记列表。对于召回来说主要依赖倒排索引，而正排索引将会在模型计算的前置步骤特征提取中用到<sup id="fnref:11"><a href="#fn:11" rel="footnote">11</a></sup>。</p>

<p><img src="https://blog.xiaogaozi.org/images/posts/rec_sys_index.png" alt="recommendation system index" /></p>

<p>讲到这里也基本上把索引框架需要实现的功能介绍得差不多了，其实需求很简单：给定一个集合然后在这个集合上创建正排和倒排索引，并暴露相应的查询接口。下一篇将会详细介绍如何定义索引、框架的 API 应该有哪些以及如何实现一个简单的倒排索引。</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>但其实能真正应用到业务中才是检验设计的唯一标准<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><del>刚说完不聊业务就打脸</del><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>当然前提是你的好友数得像<a href="https://baike.baidu.com/item/%E5%BD%AD%E7%A3%8A/6238051">彭磊</a>一样少<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>这几年有一个很恶心的词叫「千人千面」也是同样的意思<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>满足好长尾需求也是推荐系统面临的一大挑战<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>你可能看到的表示模型计算的术语有：推理（inference）、预测（prediction）<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
<li id="fn:7">
<p>想象一下你打开某个 app 的首页需要等待数分钟才能显示出来<a href="#fnref:7" rev="footnote">&#8617;</a></p></li>
<li id="fn:8">
<p>大部分情况 = P95/P99<a href="#fnref:8" rev="footnote">&#8617;</a></p></li>
<li id="fn:9">
<p>召回的耗时通常比模型计算小一到两个数量级<a href="#fnref:9" rev="footnote">&#8617;</a></p></li>
<li id="fn:10">
<p>如果你接触过搜索引擎，对于这两类索引也不会感到陌生。<a href="#fnref:10" rev="footnote">&#8617;</a></p></li>
<li id="fn:11">
<p>特征提取（feature extraction）是一个非常重要的步骤，这里暂时不会过多介绍。<a href="#fnref:11" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Little Throught About Microservices]]></title>
    <link href="https://blog.xiaogaozi.org/2015/03/22/a-little-throught-about-microservices/"/>
    <updated>2015-03-22T23:25:05+08:00</updated>
    <id>https://blog.xiaogaozi.org/2015/03/22/a-little-throught-about-microservices</id>
    <content type="html"><![CDATA[<p>知乎在 4 年前已经开始尝试服务化，至今也经历了好几个架构的变迁演化。我大约是 2013 年开始在知乎负责服务化的工作，对服务化的理解也从最初的模糊逐渐变得清晰，前段时间看了一篇叫做 <a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">Microservices &ndash; Not A Free Lunch!</a> 的文章，也想趁着这个机会梳理总结目前为止我的一些感悟和想法。</p>

<!-- more -->


<h2>SOA 与 Microservices</h2>

<p>SOA（Service Oriented Architecture）是一个很「古老」的概念，而 microservices 似乎是这两年才开始流行起来的。很多人把 microservices 看作一个全新的概念（我们都是喜新厌旧的人），Martin Fowler 觉得它跟 SOA <a href="http://martinfowler.com/articles/microservices.html#MicroservicesAndSoa">差别非常大</a>，Netflix 也把他们目前的架构<a href="http://nginx.com/blog/microservices-at-netflix-architectural-best-practices">称作</a> microservices architecture。但是有人站出来<a href="http://service-architecture.blogspot.co.uk/2014/03/microservices-is-soa-for-those-who-know.html">说</a> microservices 根本就是 SOA 很多年前已经提出的概念嘛，甚至还有一份相当冗长的<a href="https://www.oasis-open.org/committees/download.php/19679/soa-rm-cs.pdf">标准文档</a>，SOA 在互联网界的流行很大程度上可能也要归功于 <a href="http://www.infoq.com/news/Amazon-CTO-Werner-Vogels-on-SOA">Amazon</a>。在 microservices（这个单词真的好长。。）这个名词流行之前，我对服务化的理解一直就是 SOA，不过我并不是想说 SOA 跟 microservices 是两个完全不同的东西，对于后者我的理解是它是 SOA 的一个 <a href="http://en.wikipedia.org/wiki/Dialect_(computing)">dialect</a>，很多核心的思想还是来源于 SOA，只不过随着时代的发展必然会产生差异（也可以说是标准制定得太慢）。至于 microservices 的标准定义，我想目前应该没有，就连 Wikipedia 的<a href="http://en.wikipedia.org/wiki/Microservices">条目</a>也讲得不清不楚（还不如看前面提到的 Netflix 的文章，里面与 SOA 比较的文字我也觉得有待商榷），每个人、每个团队、每个公司都应该有自己的理解，后文提到的知乎目前的服务化架构姑且用 microservices 指代。</p>

<h2>Microservices 的代价</h2>

<p>服务化的好处可能很多人都了解了，你可以在任何一篇相关文章中很轻易地找到关于服务化的各种优点，很多人选择服务化的时候也正是被这个「看起来」很美好的概念打动。一切模块都是天然解耦的，这简直就是软件工程的理想境界。但凡事有利必有弊，告诉你这个东西很好的人并不一定会告诉你背后隐含的一些注意事项（所以我特别欣赏那些可以把不管优点缺点都告诉你的开源项目）。文章开头提到的那篇文章就讲述了几个在实践过程中才会真正发现的「问题」，我也大概循着作者的思路，以及附上其它一些在工作中体会到的事情。</p>

<h3>显著增加运维（DevOps）成本</h3>

<p>这里的成本包括人力和物力成本。先说说物力，在服务化之前，一个项目的所有代码应该都在一个代码仓库里，在部署的时候很自然地我们把代码 clone 下来，可能还会编译打包，最后把整个项目放到生产环境。采用服务化意味着你的项目可能会从一个变成几十个（曾经有新同事来了之后惊讶于知乎内部居然有这么多项目，其实里面有很多都是一个个小的服务），想象一下此时你的部署流程会变成什么样子？当然我们并不会每次部署都要把这几十个项目挨个部署一遍，但最坏情况下你需要关心的项目的确变多了。比如所有项目依赖的一个特殊的服务有变化，需要依赖方重启，这将会是一场「浩大」的工程，不同项目大部分情况下拥有不同的维护者，通知到所有人并且完成这件事情本身就变得比较困难（难度取决于团队大小，当然这个例子并不会是经常发生的事情）。</p>

<p>在 microservices 的思想里不同的服务应该拥有完全「独立」的资源，包括代码、机器、存储等，理论上每台机器应该只运行一个服务，存储也应该只供这一个服务读写。如果再考虑不同服务的负载和高可用，那么需要为每个服务分配 2 至多台机器。此时从运维角度上来看已经增加了「数量庞大」的机器，不过考虑到成本问题，我们可能会把服务都部署在虚拟机里，而单个存储实例也可能是被多个服务共享。但这只是物理机器的数目变少了，实际上需要管理的机器还是很多。不过现在越来越流行的容器（container）的概念也许是一个不错的解决方案，有效利用了集群的资源，同时还能做到自动伸缩（auto scaling，前提是你的服务必须是无状态的）。</p>

<p>有了这么多服务，找到它们变成了一件困难的事情。这个时候我们需要一个 proxy，它的功能很简单，帮你找到你想使用的服务，再高级一点的，也许还会帮你完成负载均衡。但有网络必有开销，即使是内网，何况还是单点，有一天你会发现某个服务的调用量已经大到无法忽视 proxy 带来的网络开销。于是我们把一个 proxy 变成多个，来分担压力。但维护这些 proxy 的信息其实也是一件麻烦事，服务的机器可能会调整，可能会有很多新的服务出现，对于运维来说是不容忽视的成本。也许你在某个地方看到了另一种方案，我们其实可以不需要 proxy，直连服务岂不更好？直连减少了多余的网络开销，同时也意味着你需要自己做负载均衡和高可用，以及发现新的或者死掉的服务。其实很多人已经想到了一个解决的办法：服务发现（service discovery）。这是一个已经很成熟的方案，你甚至可以找到<a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud">很</a><a href="https://github.com/Netflix/eureka">多</a><a href="https://www.consul.io">开</a><a href="https://coreos.com">源</a>实现，这里有一篇<a href="http://jasonwilder.com/blog/2014/02/04/service-discovery-in-the-cloud">文章</a>比较详细地介绍、对比了服务发现相关的技术。当然这些开源实现各有利弊，也许最终你会选择自己开发。但服务发现终归引入了一个新的概念，意味着你需要单独为它部署、配置、管理，也许还会与你的代码耦合。</p>

<p>另一个必须关心的事情就是监控，当然你说监控本来就是运维需要做的事情，但无形中增加了这么多机器监控肯定值得关注。并且监控不仅仅是指服务是否正常运行，还包括服务的请求量、负载、响应时间，这些都不是现成的，需要额外统计。</p>

<p>然后就是人力成本。前面提到的架构已经比服务化之前复杂了许多，这也许就不是一个人能完成的事情。还有很多组件并不一定是现成的，于是运维同学还需要具备一定的开发能力，DevOps 这个称谓其实是一个蛮高的要求。伴随而来的就是招人的标准也得提高，考虑到我们是家小公司，技术团队规模也不会太大，必须在招人上做出取舍。</p>

<h3>接口</h3>

<p>有了服务之后接口变成了一件很重要的事情。我们需要制定一些接口规范，讲究一点的可能还会要求命名风格；需要考虑接口的粒度，不能过细，尽量通用；不能让接口的使用者对服务内部产生太大影响，比如调用一个非常消耗服务资源的接口，这时服务的开发者就需要对接口参数进行必要的检验；最重要的，接口一旦发布，之后的任何改动都必须向后兼容。Protocol Buffers 就是一个很好的例子，因为是强类型，所以接口参数验证可以很方便地完成，Google 还给出了一套<a href="https://developers.google.com/protocol-buffers/docs/proto#updating">更新接口的准则</a>，例如新增的参数必须是 <code>optional</code> 或者 <code>repeated</code>，不能删除 <code>required</code> 参数。但有时候难免会做出不兼容的改动或者发布了新的接口，这时就需要告知所有服务的调用者。但你会发现找到服务是一个难题，找到服务的调用者其实也是一个难题。糙一点的可能就是发邮件给所有人或者通过经验来逐一排查，智能一点的就得在服务的框架里做些统计，自动生成服务的调用关系图。总之接口是一个你不可避免需要考虑的问题。</p>

<h3>重复逻辑</h3>

<p>软件工程一个比较重要的思想就是要避免重复代码，有这样一句耳熟能详的话：当你第二次写下同样的代码的时候就得思考是否可以抽象出一段新的代码。在一个项目里这件事很容易，可以是封装好一些函数、mixin 或者类。服务化之后有好几种方案可以选择：</p>

<ul>
<li>抽象出一个新的服务</li>
<li>把这段逻辑封装为一个库</li>
<li>管他的，我们就直接复制粘贴了吧</li>
</ul>


<p>每一个其实都有优缺点，挨个说一下。抽象新服务有滥用服务化的嫌疑，并且新的服务意味着更多的网络开销，多个服务也跟这个新服务显式地绑在了一起，稳定性有待商榷。封装库少了刚才提到的不稳定因素，但同时带来了维护成本，只要维护过库的同学应该都了解版本更新是一件很麻烦的事情，在迭代速度上肯定要逊于第一种方案。最后一种，嗯。。就像武侠小说中的锦囊一样，不到万不得已千万不要用。目前我们更倾向于第二种。</p>

<h3>分布式系统带来的复杂性</h3>

<p>服务化打破了长久以来的三层架构（3-tier architecture），有人称之为<a href="http://nginx.com/blog/time-to-move-to-a-four-tier-application-architecture">四层架构</a>。分层在软件工程里是一件好事，可以有效减少单层实现的复杂度，但同时也会给整个系统产生额外的代价。网络开销、网络的不稳定性、架构的容错性、消息的序列化和反序列化、不同服务之间负载的变化等等。四层架构里多了很重要的一层「服务层」，这一层内部的网络通信需要与上层隔离，客户端需要对服务的某些异常进行捕获，必要的时候重发请求，服务如何做到 graceful 部署，分布式事务（如果你真的需要事务），不要因为某个服务挂掉而导致整个系统宕机，序列化是采用二进制还是 JSON，序列化程序的性能如何，服务层内部又如何分层，如何避免循环调用。前面这些都必须考虑。</p>

<p>还有一个问题可能很多人刚开始并不一定会想到，那就是分布式系统的整体跟踪（tracing）。这是干嘛的？当一个问题出现时，你需要准确判断是哪一层出了问题，而不是靠猜或者逐一排查；当你需要优化整体性能时，你需要判断是哪次调用拖了后腿。早在 2010 年 Google 发表了 <a href="http://research.google.com/pubs/pub36356.html">Dapper 的论文</a>，之后 Twitter 开源了他们的实现 <a href="https://twitter.github.io/zipkin">Zipkin</a>，目前知乎也是在 Zipkin 的基础上针对我们自己的服务化框架定制了一套 tracing 系统。</p>

<p>系统架构的变化也会影响到开发者的某些设计，在以前这就是一些普通的函数调用，我们可以自由地控制调用的顺序、处理相关的异常，现在我们需要考虑到网络调用的因素，时序性也是一个问题，什么时候需要重试，什么时候又不行。某些问题是一个好的服务框架可以解决的，但某些不可以。</p>

<h3>异步</h3>

<p>由于<a href="http://en.wikipedia.org/wiki/Global_Interpreter_Lock">众所周知的原因</a>，Python 的多线程并不是一个效率很高的方案（事实上多线程本身也不是一个很好的方案），于是异步大行其道。但是 Python 的异步毕竟不是语言级别的，虽然有很多实现，但都不是特别好用（Python 3 这货也不知道要何年何月才能普及）。某些异步实现也会带来编程习惯上的改变，在使用的时候需要特别注意，否则可能会遇到一些看似「莫名其妙」的 bug。异步也不是银弹，当一个服务既有异步请求又有同步请求的时候，异步请求的性能反而会因为同步请求变差，因此一个服务最好是完完全全的异步。</p>

<h3>开发与测试</h3>

<p>这可能是比较容易忽视的一块，毕竟是给自己用的东西，不好用也许还可以忍忍，但我觉得这反而是最影响开发效率的环节。当一个项目的运行需要依赖十几个、几十个服务的时候，开发、测试就成了一个难题。我们也许可以分别在开发环境和测试环境将这些服务部署好，但是维护这些服务的可用性和稳定性会变成新的问题。比较理想的情况是项目的开发和测试不依赖任何第三方服务，从单元测试的角度上来讲你也只需要测试自己代码的逻辑就够了，这也许就得从服务框架的角度入手，不管开发还是测试都要保证接口正常调用，必要的时候把接口 mock 掉（但不能滥用 mock），不过集成测试还是不可避免大量服务的依赖。</p>

<h3>RPC 框架</h3>

<p>知乎从一开始就没有使用 Thrift 这样现成的 RPC 框架，而是基于 Protocol Buffers 自己搞了一个，后来又有了 JSON 序列化的框架，也逐渐从 Python 版扩展到 Node.js、Java 等语言。对于使用 HTTP 协议或者现成框架的团队来说可能这不是什么问题，但对于我们来说几乎是从零开始。凡事有利有弊，但知乎技术团队还是更倾向于简单的解决方案，服务化是一个生态，每个组件都需要完成自己的工作，框架可能是其中的胶水，把各个组件连接起来。2015 年我们会继续完善最新一版的 RPC 框架，同时还有整个服务化的生态。</p>

<h2>组织结构与 Microservices</h2>

<p>这是一个比较有趣的话题，我也是在看 Martin Fowler 的文章时才第一次了解到。引用一段著名的理论——Conway&rsquo;s Law（这个理论因《The Mythical Man-Month》而得名）：</p>

<blockquote><p>Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization&#8217;s communication structure.</p><footer><strong>Conway&#8217;s Law</strong> <cite><a href='http://www.melconway.com/Home/Conways_Law.html'>www.melconway.com/Home/&hellip;</a></cite></footer></blockquote>


<p>翻译过来就是：一个组织设计的系统（广义的指代，并不一定指软件系统）往往就是公司管理组织结构的翻版。在大公司通常是这样的，设计师、产品经理、工程师、测试、运维分别属于不同的团队，但是他们又共同负责一个产品，于是这个产品可能就变成产品经理想好需求，设计师负责界面交互，弄好之后交给工程师，工程师弄好之后交给测试，测试通过最后交给运维部署，软件架构上每个角色负责的东西都是独立的。而 microservices 更加强调小团队，每个团队的成员可以承担多种角色（感觉跟敏捷开发好像），microservices 往往又跟 <a href="http://en.wikipedia.org/wiki/Continuous_integration">CI</a> 和 <a href="http://en.wikipedia.org/wiki/Continuous_delivery">CD</a> 联系紧密，因此这样的组织结构能够更加契合新的软件架构。前段时间知乎内部也有关于这个话题的讨论，最后觉得软件架构和组织结构其实是相互影响的，任何一方不契合另外一方，都会造成这两个的融合。</p>

<h2>一点总结</h2>

<p>写了这么多，并不是想说服务化有多么可怕，相反，在我看来在如今移动互联网的时代，microservices 架构是一个趋势，但每个团队应该根据自己当前的需要合理选择服务化架构，这个架构可以很复杂也可以很简单，没有必要完全相同，适合的就是最好的，这绝对是软件工程第一准则。</p>

<h2>相关文章</h2>

<ul>
<li><a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html">Microservices &ndash; Not A Free Lunch!</a></li>
<li><a href="http://martinfowler.com/articles/microservices.html">Microservices</a></li>
<li><a href="http://nginx.com/blog/time-to-move-to-a-four-tier-application-architecture">It’s Time to Move to a Four-Tier Application Architecture</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Best Practices: Scheduling in YARN]]></title>
    <link href="https://blog.xiaogaozi.org/2014/12/27/hadoop-best-practices-scheduling-in-yarn/"/>
    <updated>2014-12-27T19:01:55+08:00</updated>
    <id>https://blog.xiaogaozi.org/2014/12/27/hadoop-best-practices-scheduling-in-yarn</id>
    <content type="html"><![CDATA[<blockquote><p>这篇文章基本上是对<a href="https://www.safaribooksonline.com/library/view/hadoop-the-definitive/9781491901687/ch04.html#YARNScheduling">《Hadoop: The Definitive Guide, 4th Edition》第 4 章</a>的转述，版权归作者所有。</p></blockquote>

<p>YARN 提供了三种任务调度策略：FIFO Scheduler，Capacity Scheduler 和 Fair Scheduler，下面会分别详细介绍。</p>

<!-- more -->


<h2>FIFO Scheduler</h2>

<p>顾名思义，FIFO Scheduler 就是将所有 application 按照提交顺序来执行，这些 application 都放在一个队列里，只有在执行完一个之后，才会继续执行下一个。</p>

<p>这种调度策略很容易理解，但缺点也很明显。耗时的长任务会导致后提交的任务一直处于等待状态，如果这个集群是多人共享的，显然不太合理。因此 YARN 提供了另外两种调度策略，更加适合共享集群。下图是 FIFO Scheduler 执行过程的示意图：</p>

<p><img src="https://farm8.staticflickr.com/7562/16118318715_13c5427d15_o.png" alt="FIFO Scheduler" /></p>

<h2>Capacity Scheduler</h2>

<p>既然需要多人共享，那 Capacity Scheduler 就为每个人分配一个队列，每个队列占用的集群资源是固定的，但是可以不同，队列内部还是采用 FIFO 调度的策略。下图是 Capacity Scheduler 执行过程的示意图：</p>

<p><img src="https://farm8.staticflickr.com/7559/15495992404_c03bc4d9a8_o.png" alt="Capacity Scheduler" /></p>

<p>可以看到，队列 A 和 B 享有独立的资源，但是 A 所占的资源比重更多。如果任务在被执行的时候，集群恰好有空闲资源，比如队列 B 为空，那么调度器就可能分配更多的资源给队列 A，以更好地利用空闲资源。这种处理方式被叫做「queue elasticity」（弹性队列）。</p>

<p>但是弹性队列也有一些副作用，如果此时队列 B 有了新任务，之前被队列 A 占用的资源并不会立即释放，只能等到队列 A 的任务执行完。为了防止某个队列过多占用集群资源，YARN 提供了一个设置可以控制某个队列能够占用的最大资源。但这其实又是跟弹性队列冲突的，因此这里有一个权衡的问题，这个最大值设为多少需要不断试验和尝试。</p>

<p>Capacity Scheduler 的队列是支持层级关系的，即有子队列的概念。下面是一个示例配置文件：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="cp">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
</span><span class='line'><span class="nt">&lt;configuration&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.queues<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>prod,dev<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.dev.queues<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>eng,science<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.prod.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>40<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.dev.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>60<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.dev.maximum-capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>75<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.dev.eng.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'>  <span class="nt">&lt;property&gt;</span>
</span><span class='line'>    <span class="nt">&lt;name&gt;</span>yarn.scheduler.capacity.root.dev.science.capacity<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>    <span class="nt">&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/property&gt;</span>
</span><span class='line'><span class="nt">&lt;/configuration&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>所有队列的根队列叫做 <code>root</code>，这里一共有两个队列：<code>dev</code> 和 <code>prod</code>，<code>dev</code> 队列之下又有两个子队列：<code>eng</code> 和 <code>science</code>。<code>dev</code> 和 <code>prod</code> 分别占用了 60% 和 40% 的资源比重，同时限制了 <code>dev</code> 队列能够伸缩到的最大资源比重是 75%，换句话说，<code>prod</code> 队列至少能有 25% 的资源分配。<code>eng</code> 和 <code>science</code> 队列各占 50%，但因为没有设置最大值，所以有可能出现某个队列占用整个父队列资源的情况。</p>

<p>除了设置队列层级关系和资源分配比重之外，Capacity Scheduler 还提供了诸如控制每个用户或者任务最大占用资源、同时执行的最大任务数，以及队列的 ACL 等配置，详细请参考<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">官方文档</a>。</p>

<h3>队列放置</h3>

<p>分配好了队列，要怎么控制任务在指定队列执行呢？如果是 MapReduce 程序，那么可以通过 <code>mapreduce.job.queuename</code> 来设置执行队列，默认情况是在 <code>default</code> 队列执行。注意指定的队列名不需要包含父队列，即不能写成 <code>root.dev.eng</code>，而应该写 <code>eng</code>。</p>

<h2>Fair Scheduler</h2>

<p>Fair Scheduler 试图为每个任务均匀分配资源，比如当前只有任务 1 在执行，那么它拥有整个集群资源，此时任务 2 被提交，那任务 1 和任务 2 将平分集群资源，以此类推。</p>

<p>当然 Fair Scheduler 也支持队列的概念，下图是执行过程的示意图：</p>

<p><img src="https://www.safaribooksonline.com/library/view/hadoop-the-definitive/9781491901687/images/yarn_fair_scheduling.png" alt="Fair Scheduler" /></p>

<p>队列 A 首先执行任务，任务 1 拥有整个集群资源，随后队列 B 增加任务 2，这两个队列均分资源，接着任务 3 被提交到队列 B，但这并不会影响队列 A，任务 3 将会跟任务 2 一起均分资源。</p>

<h3>开启 Fair Scheduler</h3>

<p>设置 <code>yarn.resourcemanager.scheduler.class</code> 为 <code>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code>（在 <code>yarn-site.xml</code>），如果你使用的是 CDH，那默认就是 Fair Scheduler（事实上，CDH 也<a href="http://www.cloudera.com/content/cloudera/en/documentation/cdh5/v5-1-x/CDH5-Installation-Guide/cdh5ig_mapreduce_to_yarn_migrate.html#concept_nqs_pmy_xl_unique_3">不支持 Capacity Scheduler</a>）。</p>

<h3>队列设置</h3>

<p>Fair Scheduler 通过 <code>fair-scheduler.xml</code> 文件来进行各种设置，这个文件的位置可以通过 <code>yarn.scheduler.fair.allocation.file</code> 属性来控制（在 <code>yarn-site.xml</code>）。如果没有这个文件，Fair Scheduler 采取的策略将是：每个任务都放在以当前用户命名的队列中，如果这个队列不存在，将会自动创建。</p>

<p>Fair Scheduler 也支持显式定义队列，就像 Capacity Scheduler 那样，下面是示例文件：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="cp">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
</span><span class='line'><span class="nt">&lt;allocations&gt;</span>
</span><span class='line'>  <span class="nt">&lt;defaultQueueSchedulingPolicy&gt;</span>fair<span class="nt">&lt;/defaultQueueSchedulingPolicy&gt;</span>
</span><span class='line'>
</span><span class='line'>  <span class="nt">&lt;queue</span> <span class="na">name=</span><span class="s">&quot;prod&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;weight&gt;</span>40<span class="nt">&lt;/weight&gt;</span>
</span><span class='line'>    <span class="nt">&lt;schedulingPolicy&gt;</span>fifo<span class="nt">&lt;/schedulingPolicy&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/queue&gt;</span>
</span><span class='line'>
</span><span class='line'>  <span class="nt">&lt;queue</span> <span class="na">name=</span><span class="s">&quot;dev&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;weight&gt;</span>60<span class="nt">&lt;/weight&gt;</span>
</span><span class='line'>    <span class="nt">&lt;queue</span> <span class="na">name=</span><span class="s">&quot;eng&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;queue</span> <span class="na">name=</span><span class="s">&quot;science&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/queue&gt;</span>
</span><span class='line'>
</span><span class='line'>  <span class="nt">&lt;queuePlacementPolicy&gt;</span>
</span><span class='line'>    <span class="nt">&lt;rule</span> <span class="na">name=</span><span class="s">&quot;specified&quot;</span> <span class="na">create=</span><span class="s">&quot;false&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;rule</span> <span class="na">name=</span><span class="s">&quot;primaryGroup&quot;</span> <span class="na">create=</span><span class="s">&quot;false&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;rule</span> <span class="na">name=</span><span class="s">&quot;default&quot;</span> <span class="na">queue=</span><span class="s">&quot;dev.eng&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/queuePlacementPolicy&gt;</span>
</span><span class='line'><span class="nt">&lt;/allocations&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>这里自定义了两个队列：<code>prod</code> 和 <code>dev</code>，权重比是 40:60，也就是说不采用均分的策略。每个队列可以有不同的调度策略，默认都是 <code>fair</code>，此外还有 FIFO、Dominant Resource Fairness（<code>drf</code>，后面会讲到）。详细的配置信息可以查看<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html">官方文档</a>。</p>

<h3>队列放置</h3>

<p>不同于 Capacity Scheduler，Fair Scheduler 是通过规则来决定放置的队列，即前面配置文件中的 <code>queuePlacementPolicy</code> 设置。第一个规则 <code>specified</code> 代表如果任务自己指定了队列，就放置到这个队列，如果没有指定，或者指定的队列不存在，就采用下一条规则。<code>primaryGroup</code> 规则的意思是试图将任务放置到当前用户的主要 Unix 组，如果这个队列不存在则继续下一条规则。<code>default</code> 规则会匹配所有任务，示例文件的意思是放置到 <code>dev.eng</code> 队列中。</p>

<p><code>queuePlacementPolicy</code> 可以省略，如果不设置，那么默认的规则如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;queuePlacementPolicy&gt;</span>
</span><span class='line'>  <span class="nt">&lt;rule</span> <span class="na">name=</span><span class="s">&quot;specified&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;rule</span> <span class="na">name=</span><span class="s">&quot;user&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'><span class="nt">&lt;/queuePlacementPolicy&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>也就是说除非显式指定队列，那么将会使用当前用户名作为队列，并且如果队列不存在将会自动创建。</p>

<h3>中断（Preempt）</h3>

<p>当一个任务被提交到一个空队列，但是集群不太空闲的时候，这个任务不会被立即执行，需要等待其它任务执行完毕让出资源。为了等待时间更加可控，Fair Scheduler 支持「中断」（preemption）。</p>

<p>中断的意思是调度器会通过强行结束 container 执行的方式来释放资源，在满足某些条件的情况下。注意中断是以牺牲集群性能为代价的一种做法，因为被强行结束的 container 需要重新执行。</p>

<p>通过设置 <code>yarn.scheduler.fair.preemption</code> 为 <code>true</code> 来开启中断（在 <code>yarn-site.xml</code>），同时还需要设置另外两个超时属性中的至少一个（在 <code>fair-scheduler.xml</code>），超时的单位都是秒。</p>

<ul>
<li><code>defaultMinSharePreemptionTimeout</code> 或 <code>minSharePreemptionTimeout</code>：如果一个队列等待当前设置的超时时间之后还是没有分配到应该分配的最小资源，那么调度器就会去中断其它 container。</li>
<li><code>defaultFairSharePreemptionTimeout</code> 或 <code>fairSharePreemptionTimeout</code>：如果一个队列等待当前设置的超时时间之后还是没有分配到应该分配的资源的一半以上，那么调度器就会去中断其它 container。<code>defaultFairSharePreemptionThreshold</code> 或 <code>fairSharePreemptionThreshold</code> 可以用来调节阈值，默认是 0.5。</li>
</ul>


<h2>延迟调度</h2>

<p>以上三种调度都遵从 locality 原则。在一个繁忙的集群里，当一个任务请求一个节点的时候有很大概率这个节点正被其它 container 占用，比较显而易见的做法可能是立即寻找同一机柜里的其它节点。但是经过实际观察，如果稍微等待一段时间（秒级），分配到当前请求节点的概率将显著增加。这种策略叫做「延迟调度」（delay scheduling），Capacity Scheduler 和 Fair Scheduler 都支持这种策略。</p>

<p>每一个 node manager 会定期发送心跳给 resource manager，这其中就包含了该 node manager 正在运行的 container 数量以及可以分配给新 container 的资源。当采用延迟调度策略时，调度器并不会立即使用收集到的信息，而会等待一段时间，以达到遵从 locality 的目的。</p>

<p>Capacity Scheduler 的延迟调度通过 <code>yarn.scheduler.capacity.node-locality-delay</code> 来配置，这是一个正整数，假设是 n，表示调度器将会放弃前 n 条心跳信息。</p>

<p>Fair Scheduler 的延迟调度通过 <code>yarn.scheduler.fair.locality.threshold.node</code> 来设置，这是一个 0~1 之间的浮点数，例如是 0.5，表示调度器将会等待超过一半的节点发送心跳信息之后再决定。</p>

<h2>Dominant Resource Fairness (DRF)</h2>

<p>如果只有一种资源类型需要调度，例如内存，那资源容量的概念将会很简单，比如均分资源，就代表均分内存。但是如果有多种资源类型，例如再加上 CPU，事情就变得复杂了。如果一个任务需要很多的 CPU，但是很少的内存，而另一个任务需要很少的 CPU，很多的内存，这两个任务要如何比较呢？</p>

<p>Dominant Resource Fairness（DRF）就是用来干这种事情的，下面举例说明是什么意思。</p>

<p>假设一个集群总共有 100 个 CPU，10 TB 内存。任务 A 需要 2 个 CPU，300 GB 内存。任务 B 需要 6 个 CPU，100 GB 内存。那么 A 所需资源占集群的比重是 2% 和 3%，因为内存的比重更大，那么就可以以 3% 这个比重来整体衡量 A。同理，比较之后 B 的最终比重是 6%。因此任务 B 需要两倍于任务 A 的资源（6% 比 3%），如果是均分（fair）策略，那么 B 的 container 数量将会是 A 的一半。</p>

<p>DRF 没有默认使用，因此在计算资源的时候只考虑了内存，而忽略了 CPU。Capacity Scheduler 需要设置 <code>yarn.scheduler.capacity.resource-calculator</code> 为 <code>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</code>（在 <code>capacity-scheduler.xml</code>）；Fair Scheduler 需要设置 <code>defaultQueueSchedulingPolicy</code> 为 <code>drf</code>。</p>

<h2>总结</h2>

<p>FIFO Scheduler 显然不适用于生产环境；Capacity Scheduler 概念简单，但缺乏灵活性；Fair Scheduler 最复杂，但具有足够的灵活性以及更好的资源利用率。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[香港帆船培训记录]]></title>
    <link href="https://blog.xiaogaozi.org/2014/12/15/saling-in-hk/"/>
    <updated>2014-12-15T05:28:58+08:00</updated>
    <id>https://blog.xiaogaozi.org/2014/12/15/saling-in-hk</id>
    <content type="html"><![CDATA[<p><img class="center" src="https://farm4.staticflickr.com/3947/14961313994_9fea8b7503_z.jpg"></p>

<p>曾经对香港的印象就是便宜的苹果电脑和遍地的茶餐厅，竟忘记了这是一个靠海的岛屿。作为一个在西部长大的孩子，对于海总是有很多憧憬。从小到大见过很多地方的海，有浑浊的，有碧蓝的，有挤满游客的，也有波涛汹涌的。其实海不一定就是蓝色的，只是人们习惯性地把自己的愿望加诸在别的东西身上，所以如果某一天你见到了不是蓝色的海，请不要抱怨它。</p>

<!-- more -->


<p>听说厂里要组织去香港培训帆船的时候很兴奋，想象在海上漂泊一周，应该会遇到很多有趣的事吧，虽然对于帆船其实毫无概念。照例准备好各种东西，通行证、睡袋、手套、薄外套这些，翻箱倒柜居然找出了以前用过的八达通。同事帮忙买了香港的上网卡，想到以前去只能蹭酒店 Wi-Fi 的窘境。</p>

<p>恍然来到香港，跟同事会合，一路在港铁上打趣，穿越拥挤的街道，坐在街边的茶餐厅看店员交谈，排着长队上太平山，俯瞰星光点点的维港，再来一份糖水，第一天的香港，还是老样子。</p>

<p>为了准时到达跟教练约好的地点，第二天起了个大早，幸好还有时间品尝热粥。见面的地点是香港帆船俱乐部的会客厅，墙上一张很大的地图绘制着香港岛屿周围的海域，以及各种不了解含义的符号，后来教练有介绍上面大部分的标识。教练是个苏格兰人，我们习惯称他 Cameron，接下来的五天我们一点一点了解着这个中年男人，互相交谈，互相倾听。</p>

<p>帆船在英文里叫做 yacht，如果你查字典的话会发现还有一个含义是游艇，可能在大多数人的印象中只有游艇，一个象征有钱人的东西。但真正的帆船运动远不是点燃发动机，操控船舵那么简单。也许这五天对我来说最大的意义就是了解到世界上还有这样一种运动，需要丰富的知识和经验，需要团队协作，需要良好的体力，还需要极大的热情。</p>

<p>上船的第一天对我来说应该是不太好的，在讲完必要的安全须知之后，我们正式从铜锣湾起航，一点一点远离维港。在驶到相对宁静的海湾之后，开始学习船员落水后的救援措施。首先发现落水的人需要大喊一声，扔下救生器具，同时死死盯住落水者。这是很关键的一步，Cameron 讲到在海上其实很难发现那里有一个人，尤其是人浮在水面上的部分很有限，而救生器具除了帮助落水者以外，其实还有标记的作用。这时船长需要选择一个逆风的路线逐渐靠近落水者，之所以要逆风是为了尽量控制船的行进，千万不能顺风，对于帆船来说顺风就是噩梦。最后其他的船员要负责捞起落水者，而船长始终由第一个发现的人指挥方向，因为只有他知道落水者的实际位置，以及跟船之间的距离，这需要对指挥者的充分信任。如此练习几次之后，我们向着更广阔的海域驶去。</p>

<p>「天气不错，让我们起帆吧。」 Cameron 说道。虽然这艘船有发动机，但是只要天气合适 Cameron 都更愿意使用帆来航行，「这会让你不断思考需要怎样控制船，注意风的变化，而且也更环保。」我们的帆船一共有两个帆，分别叫做 mainsail 和 genoa。首先需要升起 mainsail，也就是主帆，是个体力活，但也需要技巧和配合。帆船上的很多工作都需要很好的体力，但看似简单的步骤如果掌握了技巧会让你轻松很多。接下来是 genoa，这是一个在主帆前面的帆，比主帆大很多，根据不同的风向，我们需要控制它的大小。此时的我已经开始晕船，这真是一种不太好的感觉，于是接下来的练习我也基本上没有参与，静静躺在船上随着波浪起伏。在太阳下山前我们赶到了浅水湾，今晚将会在这里过夜，但不会靠岸。趁着夕阳我拍下了文章开头的那张照片，浅水湾真是一个适合停靠的地方，夜晚看着对面灯火辉煌的楼宇，安静的海面，时而波动。</p>

<p>第二天 Cameron 告诉了我们一个不幸的消息，今天似乎没风，这意味着原计划的航行只能作罢。「但是没关系，即使没风我们也可以做很多其它的练习。」Cameron 不放过任何练习的机会，他告诉我们虽然我们报名的课程里没有，但是他很愿意教授我们更多帆船的知识。既然没风，那就练习怎么使用发动机吧。如何原地转弯，如何快速调头，如何掌舵，如何观察水的流向，如何将船固定在港口的浮标上，这些都要一遍一遍地练习。午餐之后，幸运的我们又迎来了风。「让我们起帆出海吧！」这可能是 Cameron 最喜欢说的一句话，这个男人对于大海总是有着极大的热情。依旧是类似昨天的练习，但要更熟练，更迅速。</p>

<p>帆船依靠风来航行，因此对于不同风向，帆船的方向以及帆的角度和大小都至关重要。通常情况下正对风左右各 45 度角的区域是无法航行的，称为「no-go zone」，这是一个绝对不能进入的区域，否则就会失去动力。有时我们需要转向，此时风向也会从船的一边变到另外一边，因此帆的角度也要同时变化，这个过程叫做 tacking 或者 jibing。你需要不断观察风向，以及船与风的夹角，因为风向随时可能会变化。而对于帆船来说最好的位置是航向与风向呈 90 度角。「要怎么判断风向呢？」「用你的脸去感觉」虽然我们有风向仪，但 Cameron 更喜欢原始的方法，他总是说你得学会在仪器坏了的情况继续航行。</p>

<p>第三天我们迎来了距离最远的一次航行，从深水湾到西贡，意味着我们将有夜航的课程。夜航是一种完全不同的体验，你无法看清海面，也没有明显的参照物，因此船上的灯光显得尤为重要。到了晚上任何船只都会分别在左舷和右舷亮起红色和绿色的灯，这样就能够很方便地判断某一艘船与你的方位。远处的灯塔指引着你的方向，也提醒你这里可能会有礁石，不同的灯塔会有不同颜色、不同形式的灯光，便于区分。夜航带来了更多挑战，也更加危险，不过 Cameron 说道虽然帆船跟飞机有很多相似的地方，但最大的好处是如果遇到紧急情况帆船可以立即停下来。</p>

<p>之后的两天是对于前几天的复习，很快五天就这样过去了，想到周一我们还对帆船一无所知，如今已是可以控制它航行的船员了。我想我以后也许不会继续参与这项运动，但帆船运动的精神会一直伴随着我，至于 Cameron 的传奇经历，只能等以后再写了。</p>
]]></content>
  </entry>
  
</feed>
