<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Maybe News | Freedom]]></title>
  <link href="https://blog.xiaogaozi.org/categories/maybe-news/atom.xml" rel="self"/>
  <link href="https://blog.xiaogaozi.org/"/>
  <updated>2021-02-04T11:15:59+08:00</updated>
  <id>https://blog.xiaogaozi.org/</id>
  <author>
    <name><![CDATA[xiaogaozi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #8]]></title>
    <link href="https://blog.xiaogaozi.org/2021/02/04/maybe-news-issue-8/"/>
    <updated>2021-02-04T10:07:06+08:00</updated>
    <id>https://blog.xiaogaozi.org/2021/02/04/maybe-news-issue-8</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>The Google File System</h2>

<p><a href="https://research.google/pubs/pub51">[链接]</a></p>

<p>2003 年的 SOSP 会议上作为一家刚成立 5 年的创业公司，Google 发表了这篇影响深远的论文。论文的第一作者 Sanjay Ghemawat 相比他的同事 Jeff Dean 可能不太为外界所知，但看过他的履历以后就会发现早在 DEC 工作期间他就已经与 Jeff Dean 共事，当 Jeff Dean 在 1999 年加入 Google 后不久 Sanjay Ghemawat 也随即加入，并一起研发了 Google File System（以下简称 GFS）、MapReduce、Bigtable、Spanner、TensorFlow 这些每一个都鼎鼎大名的系统，是当之无愧的 Google 元老。</p>

<p>18 年后的今天再来回顾这篇论文依然能发现很多值得借鉴的地方，作为 GFS 最著名的开源实现，HDFS 近年来虽然已经有了很多自己的改进，但核心架构依然沿用的是这篇论文的思想。让我们回到十几年前，去探求为什么 Google 当时要研发这样一个分布式文件系统。</p>

<blockquote><p>GFS shares many of the same goals as previous distributed file systems such as performance, scalability, reliability, and availability. However, its design has been driven by key observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system design assumptions.</p></blockquote>

<p>论文开篇的第一段话已经很好地概括了 GFS 设计的初衷，这是一个完全基于 Google 业务特点设计的系统。回想一下 Google 的业务是什么？搜索引擎。搜索引擎依靠的是爬虫抓取大量数据，通过用户输入的关键词在这个庞大的数据库中检索，最后通过 Google 独有的<a href="https://en.wikipedia.org/wiki/PageRank">排序算法</a>把搜索结果展示给用户。GFS 面对的业务场景有下面几个特点：</p>

<ul>
<li><strong>组件故障随处可见</strong>：存储集群由成百上千台普通商用机器组成（与之对应的是昂贵的超级计算机），再加上应用程序和操作系统的 bug、人为错误、各种硬件故障，系统随时都面临着很多不稳定的因素。因此持续监控、错误检测、容错以及自动恢复就显得尤为重要。</li>
<li><strong>大文件为主</strong>：GB 级文件非常常见，每个文件通常包含很多应用对象（application objects），比如 web 文档。对于数十亿对象的 TB 级数据集来说，把文件切分成 KB 级大小会使得管理变得非常复杂，即使系统能够支撑这样的量级。因此系统设计的假设、文件块的大小都需要重新衡量。</li>
<li><strong>大多数文件都只是追加写而不是覆盖</strong>：随机写的场景完全不存在，文件一旦写入，只会涉及读操作，且通常是顺序读。多种类型的数据都具有这样的特征，例如某些数据是被数据分析程序批量扫描、某些数据是由数据流持续生成、某些数据是归档数据、某些数据属于中间结果（由某一台机器生成然后被另一台机器处理）。</li>
</ul>


<p>Google 当时已经部署了多个 GFS 集群，最大的一个集群有超过 1000 个存储节点以及超过 300TB 的磁盘，同时被数百个客户端访问。</p>

<p>论文的第二章节详细介绍了 GFS 的设计假设，除了前面提到的 3 个以外还包括：</p>

<ul>
<li><strong>业务场景主要包含两种读取模式：大批量的流式读取和小量的随机读取。</strong>对于前一种模式，每次请求一般读取数百 KB 或者 MB 级的数据，同一个客户端的连续请求一般也是读取某个文件的连续区域。而后一种模式通常从文件任意偏移位置读取几 KB 数据，对于那些性能敏感的应用会把多个随机读请求排序后批量发送，避免在单个文件中来来回回。</li>
<li><strong>系统需要针对并发追加写同一个文件的场景设计好的语义</strong>：典型的应用场景是把 GFS 作为消息队列，数百个生产者并发追加数据到同一个文件；或者多路合并文件，想象一下 MapReduce 的 reduce 阶段。这个文件有可能是边写边读，也有可能是写完以后再读。因此用最小的同步开销保证原子性是非常有必要的。</li>
<li><strong>高吞吐比低延时更重要</strong>：GFS 面对的大多数应用追求的是高速率批量处理数据，只有少部分应用对于点查有严格的延时要求。</li>
</ul>


<p>像传统的文件系统一样，GFS 提供包括创建、删除、打开、关闭、读取、写入这样的接口，但是 GFS 并不提供 POSIX 这样的标准 API。文件通过目录结构组织，可以通过路径名来标识某一个文件。除此之外，GFS 还提供快照（snapshot）和原子追加写（record append）功能。</p>

<p>在介绍完 GFS 的设计背景以及假设以后，接下来是详细的 GFS 架构讲解。<strong>GFS 服务端由一个 master 和多个 chunkserver 组成，通过特定的 client 库（实现了 GFS 的文件系统 API）与应用集成。</strong>GFS 是非常经典的分布式系统架构，影响了后来很多系统的设计。</p>

<p>Master 负责维护整个文件系统的元数据（metadata），包括命名空间（namespace）、访问控制（access control）信息、文件到 chunk 的映射以及每一个 chunk 的具体位置（location）。命名空间可以理解为目录结构、文件名等信息。除此之外，master 还承担一些系统级的活动，例如 chunk 的租约（lease）管理、垃圾回收无效 chunk、在不同 chunkserver 之间迁移 chunk。Master 会周期性地与每一个 chunkserver 进行心跳通信，心跳信息中同时还会包含 master 下发的指令以及 chunkserver 上报的状态。元数据都是保存在 master 的内存中，因此 master 的操作都非常快。每个 chunk 的元数据大约会占用 64 字节内存空间，每个文件的命名空间信息也是占用 64 字节左右（因为 master 针对文件名进行了前缀压缩），相对来说内存的开销是很小的，随着文件数的增加对 master 节点进行纵向扩展即可。比较重要的元数据信息（比如命名空间、文件到 chunk 的映射）还会同步持久化操作日志（operation log）到 master 的本地磁盘以及复制到远端机器，保证系统的可靠性，避免元数据丢失。当操作日志增长到一定大小，master 会生成一个检查点（checkpoint）用于加快状态恢复，检查点文件是一个类似 B 树的结构，可以不经过解析映射到内存直接查询。每个 chunk 的具体位置不会被持久化，master 每次启动时会通过请求所有 chunkserver 来获取这些信息。最初设计时其实考虑过持久化 chunk 的位置信息，但是后来发现在 chunkserver 拓扑经常变化（比如宕机、扩缩容）的情况下如何保持 master 和 chunkserver 之间的数据同步是一个难题。此外 GFS 还提供仅用于只读场景的影子（shadow）master，影子 master 的数据不是实时同步，因此不保证是最新的数据。</p>

<p>采用单 master 的架构极大地简化了 GFS 的设计（也成为了之后被人诟病的因素），master 作为掌握全局信息的唯一入口，必须确保最小程度影响读写操作，否则就会变成整个系统的瓶颈。<strong>因此 GFS 的设计是 client 读写数据永远不会经过 master。</strong>实现方式很简单，client 请求 master 获取到具体需要通信（不管是读还是写）的 chunkserver 列表，把这个列表缓存在本地，之后就直接请求 chunkserver。</p>

<p>Chunkserver 这个名字的来历其实是因为 GFS 把文件分割成了多个固定大小的 chunk。每个 chunk 的大小是 64MiB，相比传统文件系统的块（block）大小大了很多（比如 ext4 默认的块大小是 4KiB），同时 master 会为每一个 chunk 分配一个全局唯一的 64 位 ID。Chunkserver 除了将 chunk 存储到本地磁盘上，还会复制到其它 chunkserver，GFS 默认会存储 3 个副本，当然用户也可以为不同的目录指定不同的复制等级。为什么 GFS 会选择 64MiB 这么大的 chunk 大小呢？论文中列举了几个原因：</p>

<ul>
<li><strong>减少 client 与 master 的交互</strong>：前面提到 client 不管是读还是写数据都需要首先与 master 通信，GFS 的业务场景通常都是顺序读写大文件，chunk 大小越大 client 就能在 1 次请求中获取到更多的信息。即使是随机读的场景，client 也能更多地缓存 chunk 位置信息。</li>
<li><strong>降低 chunkserver 的网络开销</strong>：更大的 chunk，client 越可能执行更多的操作，因此可以降低 client 与 chunkserver 之间的 TCP 长连接的网络开销。</li>
<li><strong>减少 master 维护的元数据大小</strong>：chunk 越大，master 就可以在内存中保存更多元数据。</li>
</ul>


<p>每个 chunk 以及它的副本有两种角色：主副本（primary replica）和从副本（secondary replica），主副本只有 1 个，其它的都是从副本，至于具体哪个是主副本是由 master 决定的。master 会授权一个租约（lease）给主副本，租约的初始超时时间是 60 秒，但是只要 chunk 还在被修改，主副本可以无限续租，master 也可以随时废除租约。基于主从副本和租约的概念，数据写入 GFS 的流程是：</p>

<ol>
<li>Client 请求 master 获取当前 chunk 所有副本所在的 chunkserver 列表，如果目前还没有租约，master 会授权给其中一个副本（也就是说这个副本升级为主副本）。</li>
<li>Master 将主副本的 ID 以及从副本的位置回复给 client，client 会将这些信息缓存在本地，只有当主副本无法通信或者租约失效时才会再次请求 master。</li>
<li>Client 发送<strong>数据</strong>给主从副本所在的全部 chunkserver。发送顺序无所谓，一般是发送给离 client 最近的一个 chunkserver，然后这个 chunkserver 会传递给离它最近的下一个 chunkserver，依此类推。Chunkserver 之间的距离是通过 IP 地址估算出来的，之所以采用这种线性传递数据的方式，目的是最大化网络吞吐。Chunkserver 不会等到一个 chunk 全部接收完毕才发送出去，而是采用管道（pipeline）的方式，只要接收到一定的数据就立即发送。<strong>值得一提的是，当时 Google 的网络带宽是 100Mbps，而现在（2021 年）AWS 上的机器网络带宽能达到 25Gbps，是当年的 250 倍。</strong></li>
<li>一旦所有副本都回复收到了数据，client 就发送<strong>写请求</strong>给主副本。这个请求包含了上一步发送的所有数据的标识符，主副本会分配连续的序列号给写请求，并按照序列号的顺序修改它的状态。</li>
<li>主副本转发写请求给其它从副本，从副本也会按照相同的序列号顺序修改状态。</li>
<li>当所有从副本都回复给主副本，即表示这次写请求已经完成。</li>
<li>主副本回复请求给 client。如果任何副本发生了错误也会一并回复，GFS 的客户端会尝试重试。步骤 3~步骤 7 执行时也会有一定的重试机制，避免每次都从头开始。</li>
</ol>


<p>原子追加写（record append）的流程大体上和上面介绍的一样，区别在于第 4 步时主副本会检查写入以后是否会超过最后一个 chunk 的大小（64MiB），如果没超过就追加到后面，如果超过了会把最后一个 chunk 填充（pad）满，并回复 client 重试。</p>

<p>快照（snapshot）功能基于 copy-on-write 实现，master 通过仅仅复制元数据的方式能够在短时间内完成快照的创建。当 client 需要修改快照数据时，master 会通知所有 chunkserver 本地复制对应的 chunk，新的修改会在复制后的 chunk 上进行。</p>

<p>限于本期的篇幅，还有很多 GFS 的特性没有介绍，例如命名空间管理与锁、副本放置策略（placement policy）、chunk 重新复制（re-replication）、数据均衡（rebalancing）、垃圾回收、高可用等。最后是一个彩蛋，如果你仔细看论文最后的感谢名单，会发现一个熟悉的名字（当然不是 Jeff Dean）。</p>

<h2>Colossus: Successor to the Google File System</h2>

<p><a href="https://www.systutorials.com/colossus-successor-to-google-file-system-gfs">[链接]</a></p>

<p>自从 GFS 的论文发布以来，Google 的数据已经增长了好几个数量级，很显然 GFS 的架构已经无法支撑如此大规模的数据存储。那 Google 下一代的文件存储是什么呢？答案就是 Colossus。这个神秘的项目直到目前为止都没有在公开场合被全面正式地介绍过，我们只能通过很多碎片的信息来拼凑出它的模样，上面链接中的内容即是通过这些信息整理出来的。一些有趣的信息是：元数据服务（Curators）基于 Bigtable；相比 GFS 至少可以横向扩展 100 倍；GFS 依然存在，只不过是用来存储文件系统元数据的元数据（metametadata）；Colossus 可以基于另一个 Colossus 构建，就像俄罗斯套娃一样无限嵌套（让我想到了分形）；存储数据的服务叫做 D server；默认使用 Reed-Solomon 编码存储数据，也就是通常所说的纠删码（erasure code）。建议配合这个 2017 年的 <a href="http://www.pdsw.org/pdsw-discs17/slides/PDSW-DISCS-Google-Keynote.pdf">slide</a> 以及这篇<a href="https://levy.at/blog/22">中文博客</a>一起阅读。</p>

<h2>Storage Reimagined for a Streaming World</h2>

<p><a href="https://blog.pravega.io/2017/04/09/storage-reimagined-for-a-streaming-world">[链接]</a></p>

<p>流式计算这几年应该算是红到发紫？看看 Flink 社区的发展便可知晓。不过本期要介绍的不是流式计算，而是流式存储。说到与流式计算有关的存储，首先想到的可能是 Kafka，作为实时数据流的消息总线，Kafka 承担着非常重要的角色。但是 Kafka 也不是完美的，它的诞生其实比流式计算更早。Kafka 2011 年<a href="https://blog.linkedin.com/2011/01/11/open-source-linkedin-kafka">开源</a>，Spark <a href="https://spark.apache.org/news/spark-0-7-0-released.html">v0.7.0</a> 2013 年发布开始支持 streaming，Flink <a href="https://flink.apache.org/news/2014/11/04/release-0.7.0.html">v0.7.0</a> 2014 年发布开始支持 streaming（跟 Spark 是同一个版本号不知是否是巧合）。因此 Kafka 的很多设计并不是针对流式计算场景优化。比如 topic partition 这个概念，本质上是为了提高读或者写的并发，但是 partition 本身是一个静态配置，并不能做到动态伸缩。再比如 Kafka 的数据存储，目前只支持内存和本地磁盘两种，消费新数据都是从内存，如果是旧数据就可能读磁盘，但是 Kafka 集群的存储容量上限毕竟还是受限于磁盘空间，在流式计算越来越重以及云计算大行其道的今天集群运维是一个难题（某些公司已经自研了 Kafka on HDFS 的方案，比如<a href="https://cloud.tencent.com/developer/news/599446">快手</a>）。Pravega 便这样应运而生，这是一个来自戴尔的<a href="https://github.com/pravega/pravega">开源项目</a>，一些设计亮点是动态 partition 以及自动数据分层（Apache BookKeeper + HDFS）。</p>

<h2>Why We Built lakeFS: Atomic and versioned Data Lake Operations</h2>

<p><a href="https://lakefs.io/why-we-built-lakefs-atomic-and-versioned-data-lake-operations">[链接]</a></p>

<p>在数据库领域 ACID 和 MVCC 已经不是什么新鲜的概念，但是文件系统领域似乎还是一个属于比较「早期」的阶段，虽然过去已经有类似 <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a>、<a href="https://en.wikipedia.org/wiki/Btrfs">Btrfs</a> 这样创新的设计，但它们并不是广泛被大众了解以及使用的技术。特别是当云计算以及 S3 这样的「傻瓜」方案出现后，人们似乎已经习惯了开箱即用的产品。数据湖（data lake）这个词汇不知道从什么时候开始流行，对象存储的角色变得越来越重（至少云厂商是这样希望的？）。人们对这个「万能」的存储有着越来越多的期望，但是对象存储并不是万能的。为了解决对象存储的各种问题（这里不赘述具体问题）或者说填补它的一些缺失，越来越多基于对象存储的项目诞生。<a href="https://lakefs.io">lakeFS</a> 即是其中一个，lakeFS 希望通过提供类似 Git 的体验来管理对象存储中的数据，并且保证 ACID。比如创建一个数据的「分支」即可实现多版本管理。lakeFS 的开发团队来自以色列（<a href="https://www.treeverse.io">公司官网</a>挺有意思），项目使用 Go 语言实现。一些类似的项目还有 <a href="https://dvc.org">DVC</a>、<a href="https://github.com/quiltdata/quilt">Quilt</a> 以及 <a href="https://github.com/tensorwerk/hangar-py">Hanger</a>。</p>

<h2>Magnet: A scalable and performant shuffle architecture for Apache Spark</h2>

<p><a href="https://engineering.linkedin.com/blog/2020/introducing-magnet">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6/">第 6 期</a> Maybe News 曾经介绍过 Facebook 的 Cosco，一个给 Hive/Spark 使用的 remote shuffle service 实现。本期介绍的 Magnet 来自 LinkedIn，也是一个 shuffle service。跟 Cosco 的区别在于 Magnet 不是存算分离架构，不依赖外部存储，核心思想是 mapper 把 shuffle 数据先写到本地的 shuffle 服务，然后这些 shuffle 数据会根据某种负载均衡算法推到远端的 shuffle 服务上，远端 shuffle 服务会定期合并（merge）数据，最后 reducer 从远端 shuffle 服务读取数据。这里的「远端」其实是一个相对的概念，有可能 reducer 跟 shuffle 服务在同一个节点上，那就不需要发送 RPC 请求而是直接读取本地磁盘的数据。更多技术细节可以参考 VLDB 2020 的<a href="http://www.vldb.org/pvldb/vol13/p3382-shen.pdf">论文</a>，另外 LinkedIn 的工程师也在积极将 Magnet 贡献给 Spark 社区，目前已经合入了几个 PR，具体请参考 <a href="https://issues.apache.org/jira/browse/SPARK-30602">SPARK-30602</a>。</p>

<h2>支付宝研究员王益：Go+ 可有效补全 Python 的不足</h2>

<p><a href="https://tech.antfin.com/community/articles/993">[链接]</a></p>

<p><a href="https://github.com/wangkuiyi">王益</a>目前是蚂蚁集团研究员，同时也是开源项目 <a href="https://github.com/sql-machine-learning/sqlflow">SQLFlow</a> 和 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a> 的负责人（这两个项目也很有意思，有兴趣的同学可以去了解了解）。这里介绍的 <a href="https://github.com/goplus/gop">Go+</a> 是七牛创始人许式伟发起的开源项目，从 Go+ 的 slogan「the language for data science」就能看出项目的设计初衷。如果说目前什么编程语言在数据科学和机器学习领域最受欢迎，那可能就是 Python 了。但是 Python 的语言特性决定了它可能并不是最适合的，Go+ 依托 Go 语言作为基础，很好地弥补了 Python 的缺失。推荐对机器学习感兴趣的同学看看这篇文章，其中提到的一些八卦历史也很有趣。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #7]]></title>
    <link href="https://blog.xiaogaozi.org/2020/11/15/maybe-news-issue-7/"/>
    <updated>2020-11-15T16:37:20+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/11/15/maybe-news-issue-7</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores</h2>

<p><a href="https://databricks.com/research/delta-lake-high-performance-acid-table-storage-overcloud-object-stores">[链接]</a></p>

<p>14 年前，Amazon 发布了 EC2（Elastic Compute Cloud）和 S3（Simple Storage Service）这两个划时代的产品，从此「云计算」这个词开始进入大众的视野，经过十几年的发展已经逐渐被大众所认知与接受。「云」意味着近乎无限的资源，EC2 为用户提供了计算资源，S3 为用户提供了存储资源。传统基于 Hadoop 的大数据平台是将这两种资源绑定在一起的，而迁移到云端以后非常自然地会想到将存储资源转到类似 S3 的对象存储中，从而真正实现存储计算分离的架构，能够更加弹性地管理计算和存储这两种天生异构的资源，既大幅节约了成本还省去了运维 HDFS 集群的各种烦恼。</p>

<p>作为 Spark 的发明者，Databricks 这家商业公司的很多客户同时也是 AWS 的客户，因此有着非常丰富的在大数据场景使用 S3 的经验。这些经验暴露了 S3（或者类似的对象存储）作为 HDFS 替代者的种种缺陷。</p>

<p>对象存储（object store）的用户可以创建很多 bucket，每个 bucket 中存储了很多对象（object），每个对象都会有一个唯一的 key 作为标识。因此对象存储本质上是一个 K/V 存储，这一点非常重要，因为通常的认知都会将对象存储等同于文件系统（file system）。对象存储中的「目录」其实是通过 key 的前缀模拟出来的，虽然对象存储提供类似 LIST 目录这样的 API，底层实现却是遍历相同前缀的对象，这个操作在文件系统中是 O(1) 的时间复杂度，但在对象存储中是 O(n)。更加严重的情况是，S3 的 LIST API 每次请求最多返回 1000 个 key，单次请求延时通常为几十到几百毫秒，因此当处理超大规模的数据集时单单花在遍历上的时间就可能是分钟级。重命名对象或者目录也是一样，文件系统是一个原子操作，对象存储是先拷贝到新路径，再删除原路径的对象，代价非常高。</p>

<p>另一个对象存储严重的问题是一致性模型，S3 的一致性模型是<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">最终一致性</a>。当某个客户端上传了一个新的对象以后，其它客户端并不一定保证能立即 LIST 或者读取这个对象。当一个对象被更新或者删除以后也会发生同样的现象，即使是负责写入的这个客户端自己也有可能遇到。S3 能确保的一致性是 read-after-write，也就是说 PUT 请求产生以后的 GET 请求是保证一定能返回正确数据的。</p>

<p>论文概述了目前大数据存储的 3 种方案：分区目录、自定义存储引擎、元数据在对象存储中，下面分别介绍。</p>

<p><strong>分区目录</strong>顾名思义就是将数据按照某些属性进行分区，比如日期。这是大数据领域非常普遍的做法，好处是可以根据分区过滤不需要的数据，也就能减少 LIST 请求的数量。这个方案并没有解决前面提到的对象存储的问题，因此缺点也很明显：不支持跨多个对象的原子操作、最终一致性、低性能、不支持多版本和审计日志。</p>

<p><strong>自定义存储引擎</strong>的意思是在云上实现一个独立的元数据服务，类似 Snowflake、<a href="https://juicefs.com">JuiceFS</a> 的做法。对象存储只是被当作一个无限容量的块存储，一切元数据操作都依赖这个单独的元数据服务。这个方案的挑战是：</p>

<ul>
<li>所有 I/O 操作都需要经过元数据服务，这会带来额外的请求开销，降低性能和可用性。</li>
<li>实现一个与现有计算引擎互通的连接器（connector）需要更高的工程成本</li>
<li>用户会因为元数据服务而绑定在某一个特定的服务供应商上，没法直接访问对象存储中的数据。</li>
</ul>


<p><strong>元数据在对象存储中</strong>是 Databricks 提倡的方案，即今天介绍的 Delta Lake。这个方案和前一个的本质区别是不存在一个中心化的元数据服务，元数据是通过「日志」的形式直接存放在对象存储中。从目录结构上来看，Delta Lake 定义了一种特殊的存储格式，例如对于更新或者删除的数据会产生很多小的 delta 文件。这一点上其实跟 <a href="https://issues.apache.org/jira/browse/HIVE-5317">Hive 实现 ACID</a> 的设计很像，后者在 2013 年就已经开始开发，而 Delta Lake 项目是 2016 年启动，很难说有没有借鉴的成分。更加类似 Delta Lake 的是另外两个项目：<a href="https://hudi.apache.org">Apache Hudi</a> 和 <a href="https://iceberg.apache.org">Apache Iceberg</a>，关于这几个项目的异同后面会有一个更详细的介绍，Databricks 目前宣传的一些 Delta Lake 独有的特性（比如 Z-order clustering）其实并没有开源。</p>

<p>Delta Lake 的思想其实很好理解，本质上是把所有操作都通过日志的形式记录下来，当读取时需要重放这些日志来得到最新的数据状态，最终实现 ACID 的语义。优化的点在于怎么加速整个流程，比如定期合并日志为一个 checkpoint、索引最新的 checkpoint 等。这种把日志作为元数据的设计解决了前面提到的对象存储最终一致性的问题，即只依赖日志来确定具体读取的文件，而不是简单通过 LIST 一个目录。但是从论文描述的场景来看还是有可能因为最终一致性踩坑（因为依然会用到 LIST API），至于这个概率有多大就不知道了，因此我对于是否能根本性解决一致性问题存疑。</p>

<p>写数据的时候有一个地方需要特别注意，日志文件的文件名是递增且全局唯一的 ID，因为写入存在并发，所以需要在这一步保证操作的原子性。根据不同的对象存储有不同的解决方案：</p>

<ul>
<li>Google Cloud Storage 和 Azure Blob Store 因为支持原子 put-if-absent 操作，因此可以通过这个 API 实现。</li>
<li>对于支持原子 rename 的文件系统（比如 HDFS、Azure Data Lake Storage），可以通过这种方式实现。</li>
<li>如果以上功能都不支持（比如 S3），在 Databricks 的企业版本里是通过一个独立的轻量级协调服务（coordination service）来确保 ID 递增的原子性。在开源版本的 Spark 连接器里是通过 Spark driver 来统一分配 ID，这样也能保证在 1 个 Spark 任务里可以并发写。你也可以通过 <code>LogStore</code> 这个接口实现一个类似 Databricks 提供的协调服务。因为依赖了一个中心化的服务（虽然只是在写数据时），也一定程度上破坏了 Delta Lake 宣扬的去中心化思想。</li>
</ul>


<p>由于日志中记录了所有的历史操作，并且数据和日志都是不可变的（immutable），因此 Delta Lake 可以很轻松实现时间旅行（Time Travel）功能，也就是重现某个历史时刻的数据状态。Delta Lake 通过类似 <code>TIMESTAMP AS OF</code> 这种语法的 SQL 可以让用户指定读取某个时间的数据，不过这个 SQL 语法目前在开源版本中还<a href="https://github.com/delta-io/delta/issues/128">不支持</a>。</p>

<p>Delta Lake 也可以很好地跟流式计算进行结合，不管是生产者还是消费者都可以利用 Delta Lake 的 API 来实现流式写和读数据。当然毕竟因为是 Databricks 开发的产品，目前结合得最好的肯定是 Spark Structured Streaming。你问支持 Flink 吗？至少 Databricks 员工的<a href="https://github.com/delta-io/delta/issues/156#issuecomment-552503730">回答</a>是还在计划中，短期内估计没戏。</p>

<p>最后是性能评测部分。首先评测的是 LIST 大量文件的场景，通过对同一张表进行不同程度地分区来模拟不同量级的文件，评测的引擎是 Hive、Presto、Databricks Runtime（企业版 Spark，以下简称 DR），其中 Hive 和 Presto 读取的数据格式是 Parquet，DR 读取的格式是 Parquet 和 Delta Lake。Hive 在有 1 万个分区时总时间已经超过 1 小时；Presto 稍好一些在 10 万个分区时才超过 1 小时；DR + Parquet 在 10 万个分区时的耗时是 450 秒（得益于并发执行 LIST 请求）；DR + Delta Lake 在 1 百万分区时的耗时才 108 秒，如果启用了本地缓存可以进一步缩短到 17 秒，可以看出来优化效果非常明显。这个结果也基本符合预期，毕竟 Delta Lake 主要目标之一就是优化 list 的性能（以及一致性），对象存储在元数据性能上肯定没有优势。</p>

<p>接下来是更接近真实场景的 TPC-DS 测试，数据集大小是 1 TB，测试结果取的是 3 次运行时间的平均值。最后的数据是 Presto + Parquet 耗时 3.76 小时，社区版 Spark + Parquet 耗时 1.44 小时，DR + Parquet 耗时 0.99 小时，DR + Delta Lake 耗时 0.93 小时。DR + Parquet 相比社区版 Spark 快的主要原因是 DR 做了很多运行时和执行计划的优化，相比之下 DR + Delta Lake 并没有比直接读取 Parquet 提升太多，论文中的解释是 TPC-DS 的表分区都不大，不能完全体现 Delta Lake 的优势。</p>

<p>总结一下，Delta Lake 的思想其实并不复杂，也是工业界为了解决对象存储诸多问题的一种尝试，虽然并不能完全解决（比如原子重命名和删除）。在大数据存储上实现 ACID 这一点对于构建实时数仓至关重要，Delta Lake 通过一种简单统一的方式实现了这个需求，而不用像传统的 <a href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda 架构</a>一样再单独部署一套存储系统（比如 HBase、Kudu）。但现在流式计算领域的风头已经从 Spark 逐渐转向了 Flink，像 Delta Lake 这种只对 Spark 支持的技术在某种程度上也会限制它的普及，相比之下 Iceberg 和 Hudi 似乎更有竞争力。</p>

<h2>Delta Engine: High Performance Query Engine for Delta Lake</h2>

<p><a href="https://www.youtube.com/watch?v=o54YMz8zvCY">[链接]</a></p>

<p>前面介绍了 Delta Lake，算是 Databricks 今年一个重量级的开源产品，但其实真正的杀手锏并没有开放出来，也就是这里要介绍的 Delta Engine。简单介绍这是一个在 Delta Lake 之上，基于 Spark 3.0 的计算引擎。Delta Engine 主要包含 3 部分：原生执行引擎（Native Execution Engine），查询优化器（Query Optimizer）以及缓存（Caching）。这个视频重点介绍了原生执行引擎，这个引擎的代号是 Photon，它使用 C++ 编写，并且实现了目前在 OLAP 领域很火的向量化（vectorization）功能，感兴趣的同学强烈建议阅读 <a href="http://cidrdb.org/cidr2005/papers/P19.pdf">MonetDB/X100: Hyper-Pipelining Query Execution</a> 这篇论文，Databricks 厉害的地方在于是跟论文作者 Peter Boncz 一起合作设计。在 30 TB 的 TPC-DS 测试中，Photon 带来了 3.3 倍的性能提升。关于查询优化器以及缓存功能的介绍可以参考 Delta Engine 的<a href="https://docs.databricks.com/delta/optimizations/index.html">文档</a>。</p>

<h2>A Thorough Comparison of Delta Lake, Iceberg and Hudi</h2>

<p><a href="https://databricks.com/session_na20/a-thorough-comparison-of-delta-lake-iceberg-and-hudi">[链接]</a></p>

<p>Iceberg 和 Hudi 是另外两个会经常拿来跟 Delta Lake 做比较的对象，Iceberg 是 Netflix 开源，而 Hudi 是 Uber 开源。它们之间有着诸多相似之处，又有着很多截然不同的设计思想。这个视频来自腾讯云数据湖团队的陈俊杰，比较系统地对比了这 3 种技术。相对来说 Iceberg 的设计是这 3 个里面最中立的，不跟某种特定的格式和引擎绑定，这也是腾讯选择 Iceberg 的原因之一，具体可以看<a href="https://www.infoq.cn/article/59lbbuvcrzlusmdowjbb">「为什么腾讯看好 Apache Iceberg？」</a>这篇文章。</p>

<h2>Bringing HPC Techniques to Deep Learning</h2>

<p><a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce">[链接]</a></p>

<p>深度学习的核心之一是 <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>（Stochastic Gradient Descent），通过把数据集拆分成若干小的集合（mini-batch），再基于这些小集合反复进行前向传播（forward propagation）和反向传播（backpropagation）计算，不断获取新的梯度（gradient）和权重（weight）。分布式训练本质上要解决的问题就是怎么让多机计算的效率线性提升，即所谓的「线性加速比」，理论值当然是 100%，但是实际情况往往差了很多。传统的同步 SGD 在每一轮计算完以后需要把所有梯度汇总，再重新计算新的权重，类似一个 MapReduce 的过程，此时 reducer 需要等待所有 mapper 计算完成，计算性能会随着 mapper 数量的增加而线性下降。怎么解决这个问题呢？这篇 2017 年的旧文介绍的便是影响至今的 Ring Allreduce 算法，作者 Andrew Gibiansky 之前在百度硅谷 AI 实验室工作，后来联合创办了语音合成公司 <a href="https://www.voicery.com">Voicery</a>（不过悲剧地发现这家公司今年 10 月份已经关了）。基于 Andrew Gibiansky 的成果，Uber 开源了目前公认的 Ring Allreduce 标准框架 <a href="https://github.com/uber/horovod">Horovod</a>。</p>

<h2>Introducing TensorFlow Recommenders</h2>

<p><a href="https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html">[链接]</a></p>

<p>推荐系统是一直都是机器学习一个重要的应用领域，如果你不了解什么是推荐系统可以看我之前写的<a href="https://blog.xiaogaozi.org/2020/04/21/how-to-design-a-distributed-index-framework-part-1/">一篇简介</a>。使用 TensorFlow 可以很方便地训练一个推荐系统模型，不管是召回模型还是排序模型。现在 TensorFlow 官方将这个流程进一步简化，推出了 TensorFlow Recommenders（TFRS）库，旨在让训练、评估、serving 推荐系统模型更加容易，并且融合一些 Google 自己的经验，对于初学者来说会是一个好的入门指南。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #6]]></title>
    <link href="https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6/"/>
    <updated>2020-09-15T18:54:00+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/09/15/maybe-news-issue-6</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Presto: SQL on Everything</h2>

<p><a href="https://prestosql.io/Presto_SQL_on_Everything.pdf">[链接]</a></p>

<p>Presto 是 Facebook 2012 年开始开发并于 2013 年开源的分布式查询引擎。和<a href="https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3/">第 3 期</a>介绍的 Kudu 一样，主要应用在 OLAP 场景，但跟 Kudu 不一样的地方是，Presto 仅仅是一个查询引擎，并不负责数据存储。这也是论文标题「SQL on Everything」的含义，这里的「Everything」指代的即是任意类型的存储，比如 HDFS、MySQL 等。</p>

<p>论文开篇先总结了 Presto 几个值得关注的特点：</p>

<ul>
<li>Presto 是一个自适应的多租户系统（adaptive multi-tenant system），可以很容易扩展到上千节点的同时，还能有效利用集群资源。这里的「自适应」很重要，是将 Presto 和其它系统进行比较的要点之一。</li>
<li>Presto 可以很方便地和多种数据源进行集成，甚至在 1 条查询语句里就可以同时查询多个数据源。Presto 通过连接器（connector）的概念统一底层存储的访问。</li>
<li>通过不同的配置可以让 Presto 同时适配不同的场景。关于这一点在后续介绍 Facebook 的查询场景时也有体现。</li>
<li>Presto 通过很多关键特性实现了一个高性能的查询引擎。多个并行的查询在同一个 JVM 中运行，虽然可以降低响应时间，但同时也需要在调度、资源管理、隔离这些方面特别注意。</li>
</ul>


<p>接下来介绍 Facebook 目前使用 Presto 的几个主要场景：</p>

<ul>
<li><strong>交互式分析（Interactive Analytics）</strong>：这是将 Facebook 数据仓库作为数据源的查询场景。这个场景通常查询的数据量较小，压缩后大概 50GB-3TB 左右。单集群需要支持的并发查询量在 50-100 左右，秒级或者分钟级返回结果。用户对于查询时间非常敏感，但同时对于查询所需的资源量没有特别精确的判断。在进行探索式分析时，用户通常不需要返回所有结果集，只要有初步的结果或者满足 <code>LIMIT</code> 的限制整个查询就可以提前终止。</li>
<li><strong>批量 ETL（Batch ETL）</strong>：这个场景的用户一般是数据工程师，目前已经是 Facebook 内部一个很大的 Presto 应用场景。相比交互式查询，ETL 需要的资源也更多，不管是 CPU 还是内存，特别是当涉及到聚合或者 join 很多大表的时候。在这个场景查询时间反而没有那么重要，更加重要的是资源利用率和整体集群的吞吐。</li>
<li><strong>A/B 测试（A/B Testing）</strong>：为了满足用户对产品验证越来越快的需求，A/B 测试的结果需要在小时级（而不是天级）内得到，并保证数据完整且精确。当用户需要进行更深层次的分析时，查询结果需要在 5-30 秒左右返回。很难通过预聚合（pre-aggregating）的方式满足这些查询需求，因此必须通过在线计算来解决。查询会涉及到 join 多个大的数据集，同时查询语句的特征是相对固定的。</li>
<li><strong>开发者／广告主分析（Developer/Advertiser Analytics）</strong>：这是面向外部开发者或者广告主的分析场景，比如 <a href="https://analytics.facebook.com">Facebook Analytics</a>。同 A/B 测试场景一样，这个场景的查询语句特征也是相对固定的。虽然总的数据规模很大，但是用户查询时因为会限制在他自己的数据里相对来说查询量会小很多。数据接入（data ingestion）的时延大概是分钟级，查询时延需要严格限定在 50 毫秒到 5 秒左右。因为应用在外部商业产品，Presto 集群的可用性需要保证在 99.999%，并且支持上百并发查询。</li>
</ul>


<p>以上这些场景可能除了 ETL 以外，也是目前很多公司使用 Presto 的主要场景，总的来说主要还是应用在交互式查询上。</p>

<p>然后是 Presto 整体的架构介绍，集群分为两种类型的节点：coordinator 和 worker。Coordinator 节点负责解析、规划以及优化查询，通常只会有 1 个。Worker 节点负责处理查询请求，根据集群规模可以横向扩展。</p>

<p>当客户端通过 HTTP 请求将 SQL 发送给 coordinator 时，经过解析和分析，coordinator 会生成一个分布式执行计划（distributed execution plan）。这个执行计划由多个 stage 连接而成，类似一个 DAG 的形式。因为这是一个分布式执行计划，stage 会被分发到不同的 worker，因此 stage 之间需要通过 shuffle 来交换数据。每个 stage 内部由多个 task 组成，一个 task 可以被看作一个处理单元（processing unit）。Task 内又由多个 pipeline 构成，一个 pipeline 内包含一系列的 operator。到这里，operator 已经是最小的处理单位，通常只负责某一类单一计算任务。</p>

<p>Coordinator 很大一部分工作是负责调度，调度分为三个维度：stage、task 和 split。Stage 调度决定 stage 的执行顺序；task 调度决定多少任务需要被调度以及应该分配给哪些 worker；split 调度决定 split 会被分配给哪些任务（关于 split 这个概念后面会详细介绍）。</p>

<p>调度 stage 分为两种策略：all-at-once 和 phased。All-at-once 很好理解就是所有 stage 并行执行，这个策略可以最大化执行效率，适合时延敏感的场景（如交互式分析）。而 phased 策略就是只并行执行那些强关联的组件，整体任务分阶段执行，这个策略可以有效降低内存占用，适合 ETL 场景。</p>

<p>当 stage 调度成功，coordinator 即会开始分配 task。任务调度器将 stage 分为两类：leaf 和 intermediate。Leaf stage 负责从连接器中读取数据，intermediate stage 负责处理来自其它 stage 的中间结果。对于 leaf stage，任务调度器会根据如网络拓扑、数据本地性这些因素来决定应该把 task 分配给哪些 worker 节点，这个过程依赖连接器实现的 Data Layout API。如果没有任何限制，Presto 倾向于把 leaf stage 的任务分散到整个集群，以加快数据读取效率。Intermediate stage 的任务可以被分配到任意节点上，但是调度器仍然需要决定当前每个 stage 有多少任务需要被调度，且这个任务数是可以在运行时动态调整的。</p>

<p>当 leaf stage 的任务分配好以后，这个 worker 节点便可以开始接收来自 coordinator 分配的 split。Split 是对底层数据的逻辑封装，例如底层存储是 HDFS，那一个 split 通常包含的信息有文件路径、文件偏移等。Leaf stage 的任务必须至少分配一个 split 才能开始运行，而 intermediate stage 的任务是一直可运行的。Split 的创建由连接器负责，并且懒分配给 leaf stage 的任务，也就是说并不会等到所有 split 都创建完毕。这样做有几个好处：</p>

<ul>
<li>将连接器创建 split 的时间从查询中解耦。某些连接器（如 Hive）可能需要花费很长时间去遍历分区和 list 文件。</li>
<li>查询可以尽快开始执行而不用等到所有数据处理完毕。在交互式分析场景很有可能查询会被提前中断。</li>
<li>每个 worker 维护了一个 split 的队列，coordinator 分配 split 时会优先选择队列长度最短的节点。</li>
<li>不用一次保存所有 split 的元信息。对于 Hive 连接器来说很有可能会产生上百万个 split，这会直接导致 coordinator 内存不足。</li>
</ul>


<p>介绍完了 coordinator 的工作接下来就是 worker。前面已经提到最小的执行单位是 operator，operator 负责处理输入数据，同时输出处理完的数据。Operator 输入输出的数据单元叫做 page，一个 page 是连接器将 split 中的多行数据转为列式存储以后产生的数据结构。Shuffle 也是 worker 的主要工作之一，区别于传统的 Hadoop 组件，Presto 是基于全内存的 shuffle 实现，这也是 Presto 性能更优的原因之一。Shuffle 的数据会暂存在内存缓冲区（buffer）中，简单理解 map 端的缓冲区为输出缓冲区，reduce 端的为输入缓冲区。这两个缓冲区都是有容量限制的，会根据数据消费的速率动态调整生产速率，确保整体任务的稳定性以及多租户之间的公平性。当输出缓冲区容量持续偏高时，Presto 会减少可消费的 split 数量。输入缓冲区这端会有一个类似 TCP 滑动窗口的策略动态控制上游的生产速率。</p>

<p>回顾开篇总结的 Presto 特点，其中很重要的一个是<strong>自适应的多租户场景</strong>，上一段落介绍 shuffle 缓冲区的时候其实已经涉及到部分针对性的优化。本质上资源管理需要考虑的就是 CPU 和内存这两种资源，Presto 分别都有不同的解决方案。</p>

<p>CPU 调度场景每个 split 都会有一个允许在一个线程上一秒内执行的最大 quanta，当 quanta 超出时这个 split 将会被放回队列释放线程给其它 split。当输出缓冲区满（下游消费慢）、输入缓冲区空（上游生产慢）或者集群内存紧张时，即使 quanta 没使用完调度器也会强行切换任务。这个基于 quanta 的调度策略使得 Presto 能够最大化 CPU 资源的利用率。当线程被释放应该如何挑选下一个运行的任务呢？Presto 建立了一个 5 级的反馈队列（feedback queue），每个等级都分配了一个可配置的 CPU 时间比例。随着一个任务使用的 CPU 时间不断累积，这个任务会移动到更高等级的队列。也就是说 Presto 倾向于优先执行那些「快」的任务，因为用户期望轻的查询尽快完成，而对于那些重的查询所需的时间不太敏感。</p>

<p>内存管理是一个比 CPU 更复杂的场景。Presto 将内存分为两种类别：用户（user）和系统（system），并分别维护不同的内存池。引擎对用户内存和总内存（用户 + 系统）都有不同的限制，超过全局（所有 worker 聚合以后）或者单节点内存限制的任务将会被强行杀掉。虽然有全局的内存限制，但是为了满足并行执行多个任务的需求通常还是会超卖（overcommit）内存，即使真的出现部分节点内存耗尽的情况，Presto 也提供了两种机制去确保整体集群的稳定性。这两种机制分别是：spilling 和预留内存池（reserved pool）。Spilling 其实就是在节点内存耗尽时按照任务的执行时间升序排列，依次把内存中的状态写到本地磁盘。不过 Facebook 内部并没有开启这个特性，因为集群资源（TB 级的内存）足够支撑用户的使用场景，全内存计算也更加能保证查询的执行时间。如果没有开启 spilling 特性，那 Presto 将会采用预留内存池的策略。这个策略的大意是把内存池分为通用（general）和预留（reserved）两种，当一个 worker 节点的通用内存池耗尽时将会把这个节点上占用最多内存的查询「晋升」到预留内存池，整个集群同一时间只允许一个查询晋升。后续的内存申请会优先满足这个晋升的查询，直到它执行完毕。这个策略当然会影响整体集群的效率，因此用户也可以选择直接杀掉查询。</p>

<p>最后是容错（fault tolerance）。作为一个多租户的分布式系统，优良的容错性是一个必不可少的需求。但是遗憾的是在这一点上 Presto 做得并不好，coordinator 依然是单点（一个题外话，Starburst Data 这家提供商业 Presto 版本的公司<a href="https://docs.starburstdata.com/latest/aws/high-availability.html">支持 coordinator HA</a>），worker 宕机将会导致所有运行在这个节点上的查询失败（社区有<a href="https://github.com/prestodb/presto/issues/9855">一</a><a href="https://github.com/prestosql/presto/issues/455">些</a> issue 但是目前没有进展），Presto 非常依赖客户端自己去重试。Facebook 内部是通过外部的编排系统来确保集群的可用性，对于交互式分析和 ETL 场景有 standby 的 coordinator，A/B 测试和开发者／广告主分析场景部署了多活（multiple active）集群。监控系统将会识别不可用的节点自动从集群中移除，并在之后再重新加入集群。</p>

<p>在开发 Presto 的过程中，作者也总结了一些工程上的经验：</p>

<ul>
<li><strong>自适应胜过手动调优（Adaptiveness over configurability）</strong>：前面已经介绍了很多 Presto 自适应的特性，作者认为当面对一个多租户场景，且查询的特征千变万化的时候，自适应显得尤为重要。否则就需要人工去针对性地手动调优，这种方式在面对大规模的查询场景时是没法扩展的。</li>
<li><strong>非常轻松地监控（Effortless instrumentation）</strong>：Presto 作者相信可观察（observable）的系统设计是非常重要的，要允许工程师去了解和优化自己代码的性能。Presto 每个 worker 平均导出了约 10000 个监控指标，粒度细到了 operator 级别，并会聚合到 task 和 stage 级别。</li>
<li><strong>静态配置（Static configuration）</strong>：错误的配置可能会对系统性能造成非常大的影响，为了保证时刻对系统整体状态有一个清晰的了解，Presto 作者选择使用静态配置的方案而不是动态配置。</li>
<li><strong>垂直集成（Vertical integration）</strong>：这其实是一个要不要重复造轮子的问题，对于一个大型项目来说肯定会依赖很多基础库，那什么时候选择用开源实现，什么时候选择自研是一个需要认真思考的问题（当然类似 Google 这种只考虑自研的公司就没有这个困扰了）。Presto 作者倾向于在那些对性能和效率要求比较高的场景选择自研。</li>
</ul>


<p>最后是一个八卦。Presto 最早是由一批 Facebook 的员工开发，2018 年这批员工中的部分核心离职，全职建设 Presto 开源社区。2019 年 1 月 31 日<a href="http://www.prweb.com/releases/presto_software_foundation_launches_to_advance_presto_open_source_community/prweb16070792.htm">成立</a>「Presto Software Foundation」，并在 GitHub 上创建了新的组织 <a href="https://github.com/prestosql">PrestoSQL</a>。有趣的是在 2019 年 9 月 23 日 Facebook 联合多家公司<a href="https://www.linuxfoundation.org/press-release/2019/09/facebook-uber-twitter-and-alibaba-form-presto-foundation-to-tackle-distributed-data-processing-at-scale">成立</a>了一个新的基金会叫「Presto Foundation」，在 GitHub 上的组织叫 <a href="https://github.com/prestodb">PrestoDB</a>。按照 Presto 作者的<a href="https://github.com/prestosql/presto/issues/380">说法</a>，他们在成立 Presto Software Foundation 之后其实是有邀请过 Facebook 加入的，但是显然对方拒绝了这个邀请。于是你会发现目前在开源社区有两个版本的 Presto，并且项目名是一样的，不过为了便于区分一般还是分别叫做 PrestoSQL 和 PrestoDB。前者背后的商业公司主要是 Starburst Data，这家公司的 3 个 CTO 同时也是 Presto 的原始作者（是的，这家公司有 3 个 CTO）；后者背后的商业公司有 Facebook、Uber、Twitter、阿里巴巴、Alluxio 和 Ahana。为了不至于让用户混淆，Starburst Data 还在官网<a href="https://www.starburstdata.com/prestosql-and-prestodb">比较</a>了这两个版本的 Presto。目前公有云厂商提供的产品中，<a href="https://docs.aws.amazon.com/athena/latest/ug/presto-functions.html">AWS Athena</a>、<a href="https://help.aliyun.com/document_detail/169871.html">阿里云 DLA</a> 都是基于 PrestoDB 开发的 serverless 产品，<a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html">AWS EMR</a> 两种 Presto 都支持，<a href="https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5">Google Dataproc 1.5</a>、<a href="https://help.aliyun.com/document_detail/132036.html?#title-fm0-jq8-sog">阿里云 EMR 3.25.0</a> 以后默认集成的是 PrestoSQL，<a href="https://cloud.tencent.com/document/product/589/20279">腾讯云 EMR</a> 默认集成的是 PrestoDB。</p>

<h2>Spark Architecture: Shuffle</h2>

<p><a href="https://0x0fff.com/spark-architecture-shuffle">[链接]</a></p>

<p>要理解什么是 shuffle 就得先了解什么是 MapReduce，自从 2004 年 Google 那篇惊世骇俗的介绍 MapReduce 的<a href="https://research.google/pubs/pub62">论文</a>发表以来，大数据的生态就被彻底改变了（并沿用至今）。基于这样一个简单的编程模型实现了各种复杂的计算逻辑，但也存在一些「问题」，shuffle 就是其中一个。当 map 任务完成以后，数据需要根据 partition 策略重新分配到不同的 reduce 任务中，这个过程即称为 shuffle。这篇文章详细介绍了 Spark 历史上各种 shuffle 方案是怎么实现的。</p>

<h2>Cosco: An Efficient Facebook-Scale Shuffle Service</h2>

<p><a href="https://databricks.com/session/cosco-an-efficient-facebook-scale-shuffle-service">[链接]</a></p>

<p>接上一篇文章，这是 Facebook 在 2018 年的 Spark+AI Summit 上的一个分享，介绍了他们实现的一个外部 shuffle 服务 Cosco，可以同时用于 Hive 和 Spark 任务。当时已经在 90%+ 的 Hive 任务上使用，并在生产环境运行 1 年以上，Spark 任务也在逐渐推广中。为什么要开发一个外部 shuffle 服务呢？Facebook 列举了一些他们当时面临的问题，比如单次 shuffle 需要交换的数据量级是 PiB 级，总共有 10 万个 mapper、1 万个 reducer，3 倍的写放大（shuffle 1 PiB 的数据实际要写 3 PiB 到磁盘），平均 IO 大小只有 200 KiB。这些都是促使他们开发 Cosco 的原因（当然不是所有公司都会遇到），另一个好处是 executor 变成了无状态，对于动态伸缩更加友好。如果对 Cosco 有兴趣还可以继续看一看他们在 2019 年的 Spark+AI Summit 上做的<a href="https://databricks.com/session_na20/flash-for-apache-spark-shuffle-with-cosco">后续分享</a>。</p>

<h2>Federated Learning: Collaborative Machine Learning without Centralized Training Data</h2>

<p><a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html">[链接]</a></p>

<p>传统机器学习中的优化算法（例如 <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>）是将大规模数据集分布式运行在多个节点上，这需要低延时、高吞吐地读取训练数据，因此数据一般都是提前收集到一个中心化存储里。但是在某些场景并不适合这样做，不管是因为数据量太大不易收集，还是出于数据隐私的考虑。因此 Google 提出了联邦学习（Federated Learning）的概念，这个词源于发表在 2017 年 AISTATS 会议上的一篇论文 <a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>。联邦学习的大体思想就是在数据的生产端（例如你的手机）直接进行模型训练，经过汇总以后把对模型的更新数据发送到服务端，服务端再把其它客户端上传的更新数据一起汇总生成一个新的模型，最后下发这个新模型到所有客户端。可以看到整个过程中训练数据依然保留在客户端，并不需要上传。如果你在 Android 系统中使用 Gboard 这个 app，那其实你已经参与到联邦学习的过程中了，当然只会在当你的手机空闲并且连接电源和 Wi-Fi 的时候才会进行。</p>

<h2>分布式文件系统架构对比</h2>

<p><a href="https://juicefs.com/blog/cn/posts/distributed-filesystem-comparison">[链接]</a></p>

<p>2003 年 Google 发表了 <a href="https://research.google/pubs/pub51">The Google File System</a> 论文，就像前面提到的 MapReduce 一样，从此对业界产生了非常深远的影响。这篇博客梳理了 GlusterFS、CephFS、GFS、HDFS、MooseFS 和 <a href="https://juicefs.com">JuiceFS</a> 这几个分布式文件系统的架构设计。随着网络带宽的发展，在云计算和云原生的大趋势下，总的来说正逐步朝着存储计算分离的方向演进，这对于基础设施的架构也有着一定的要求。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #5]]></title>
    <link href="https://blog.xiaogaozi.org/2020/07/21/maybe-news-issue-5/"/>
    <updated>2020-07-21T14:08:28+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/07/21/maybe-news-issue-5</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>DynamicEmbedding: Extending TensorFlow for Colossal-Scale Applications</h2>

<p><a href="https://arxiv.org/abs/2004.08366">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1/">第一期</a> Maybe News 中介绍了腾讯提出的解决 TensorFlow 中大规模稀疏特征模型训练的方案，本期的这篇论文来自 Google（准确说是 Google Smart Campaigns 团队）。作为发明 TensorFlow 的公司，Google 内部团队的设计思想值得借鉴。</p>

<p>这个系统被称之为 DynamicEmbedding（DE），名字简单直观，要解决的场景也是很多公司都遇到的如何动态维护 embedding。系统内部分为两个组件：DynamicEmbedding Master（DEM）和 DynamicEmbedding Worker（DEW），合起来叫做 DynamicEmbedding Service（DES）。DEM 负责处理所有客户端请求，包括 embedding 查找（lookup）、更新（update）等。DEW 负责 embedding 存储、梯度更新等，所有请求都来自 DEM。同时新增了几个 TensorFlow API，如 <code>dynamic_embedding_lookup()</code>、<code>compute_sampled_logits()</code>，这些 API 是整个系统的关键入口，任何模型在接入 DES 的时候都需要在特定的地方调用这些 API。以上设计看起来跟大部分公司的方案没有太大差别。</p>

<p>通过实现一个叫做 EmbeddingStore 的通用接口，DEW 后端支持对接多种类型的存储，例如 Protocol Buffers、GFS、Bigtable，比较巧妙地将大规模 embedding 存储时面临的扩展性和稳定性问题转移到了外部存储系统。当然因为多了一次网络请求是否会影响整体的训练效率这点有待商榷，论文中介绍 BigtableEmbedding 时提到会将数据同时存储到本地缓存和远端，猜测这里本地缓存的目的便是为了加速存储操作。</p>

<p>Embedding 更新这一步涉及到一些常用的梯度下降（gradient descent）算法，为了保持一致，DEW 内部实现了跟 TensorFlow 原生提供的优化器（optimizer）同样的逻辑，并且大部分代码是可以复用的。当训练数据时间跨度很大时（如数月或者数年），可能存在很多无效的特征或者一些需要特殊处理的特征。因此 DEW 在每次 embedding 更新的时候会同时统计这个 embedding 的更新频率，通过设定一个恰当的阈值来保证只有部分 embedding 会持久化到存储系统里，那些低频的数据便不会继续保存。除了统计频率这种方法，通过 bloom filter 也可以实现类似的效果。</p>

<p>Serving 的时候因为 embedding 都已经存储到了外部系统，所以 DEW 就没有必要存在了，只需要在本地部署 DEM 负责处理读请求。为了提升推理的性能，本地缓存肯定是少不了的，同时批量处理查询请求也是非常重要的。</p>

<p>实验评估阶段首先比较了和原生 TensorFlow 训练同样的模型、同样的超参是否会有指标上的差异，模型选择的是 Word2Vec，梯度下降算法选择的是 SGD、Adagrad 和 Momentum。从最终训练的 loss 上看几乎没有差别，说明 DE 系统不会对模型质量有影响。</p>

<p>接着测试了字典（dictionary）大小对模型精度（accuracy）的影响，理论上 DE 系统其实是不限定字典大小的，从实验的两个模型 Word2Vec 和 Sparse2Seq 上来看也的确是字典大小越大模型精度越高。</p>

<p>然后是评测模型训练时两个重要的系统指标：集群总的内存占用和每秒训练的 global steps（GSS）。分别测试了三个模型：Word2Vec，Image2Lable 和 Seq2Seq。在使用原生的 TensorFlow 时集群内存占用会随着 worker 数量的增大而显著增长（在 Word2Vec 模型中尤为明显），相比之下 DE 系统的内存占用只跟 embedding 的总大小有关，与 DEW 的数量无关。之所以有这样的差异也是因为原生的 TensorFlow 会在不同 worker 间重复存储 embedding 数据。GSS 的对比上两者的加速比都差不多，但是总体上 DE 还是会更优。</p>

<p>最后论文中详细介绍了 DE 在 Google Smart Campaigns 产品中的一个重要应用：给广告主自动推荐投放的关键词。这是一个叫做 Sparse2Label 的模型，输出即是推荐的关键词（label）。这个模型带来的最大变化是以前需要针对每一种语言训练一个单独的模型，而现在只需要一个模型即可。通过对比一些核心指标（如 CTR），DE 推荐的关键词都明显更好。整个模型也是随着时间不断增长的，截止 2020 年 2 月这个模型的参数量已经达到了 1249 亿个，如果每个参数按 4 字节算的话模型大小差不多为 465 GiB（其实比想象中小）。</p>

<p>另一个更难评估的指标是用户搜索的关键词（query）与广告投放的关键词之间的关联度，很多时候两者之间并不是完全匹配的。作者是通过人工评估 38 万个样本的方式来解决的，每个样本都会有 5 个人类进行打分，分数区间从 -100 到 100，越高越匹配，然后计算这 5 个分数的平均值作为这个样本的最终分数。大于等于 50 分的样本认为是好（good）的样本，小于等于 0 分的则认为是不好（bad）的样本，前者除以后者被称作 GB ratio，这个比率越大越好。每个推荐的关键词都会同时有一个置信值（也就是网页和关键词 embedding 之间的 cosine 距离），从评测结果上来看当这个置信值大于 0.7 时，不好的样本量将会显著减少。实际生产环境收集的数据也印证了 DE 系统推荐的关键词是 GB ratio 最高的。</p>

<p>总结一下 DE 系统解决了原生 TensorFlow 在大规模 embedding 模型训练时效率低下（甚至不可用）的问题，短期内这个系统估计也不会开源或者合并到上游。目前可以期待的还是腾讯的方案，他们已经提交了<a href="https://github.com/tensorflow/tensorflow/pull/41371">代码</a>到 TensorFlow 社区。</p>

<h2>The Next Step for Generics</h2>

<p><a href="https://blog.golang.org/generics-next-step">[链接]</a></p>

<p>在<a href="https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2/">第二期</a> Maybe News 中曾经介绍过 Go 语言开发者关于泛型设计的一些思考，近期 Ian Lance Taylor 又和社区同步了一下最新进展。最大的变化就是去掉了 contract 这个新增的概念，改为复用 interface。同时创建了一个新的 <a href="https://go2goplay.golang.org">playground</a>，可以方便大家试验泛型代码。如果最新的这一版设计社区没有太大异议的话，乐观估计将会在 Go 1.17 加入泛型特性，也就是在 2021 年 8 月左右。当然最终实现这个目标还是有很多的不确定性，特别是当前疫情对于全球影响的情况下。</p>

<h2>Fiber: Distributed Computing for AI Made Simple</h2>

<p><a href="https://eng.uber.com/fiberdistributed">[链接]</a></p>

<p>分布式计算在 AI 领域的需求一直都很强烈，但分布式计算不仅仅是将单机迁移到多机这样就足够了，还需要考虑如：易用性（降低用户从单机迁移的成本）、稳定性（自动容错）、弹性伸缩（和底层资源调度层配合）、线性加速（横向扩展多少机器就能带来多少性能提升）。Uber 和 OpenAI 共同开发的 Fiber 框架便是尝试解决以上问题的一个例子，从 Fiber 的<a href="https://arxiv.org/abs/2003.11164">论文</a>能看到这个框架最初设计面向的是强化学习（Reinforcement Learning）场景，在这个领域有很多类似的框架，比如 Google 的 <a href="https://github.com/google/dopamine">Dopamine</a>、Facebook 的 <a href="https://github.com/facebookresearch/ReAgent">ReAgent</a>、UC Berkeley 的 <a href="https://github.com/ray-project/ray">Ray</a>。自动容错和弹性伸缩这两个特性又让我联想到蚂蚁金服的 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a> 和才云的 <a href="https://github.com/caicloud/ftlib">FTLib</a>。</p>

<h2>The impact of slow NFS on data systems</h2>

<p><a href="https://engineering.linkedin.com/blog/2020/the-impact-of-slow-nfs-on-data-systems">[链接]</a></p>

<p>LinkedIn 分享了他们使用 NFS 进行数据库备份时遇到的性能问题，因为备份进程和数据库进程是一起部署的，因此这个问题还间接影响到了在线业务的稳定性。整个问题分析过程清晰易懂，还能顺便复习一下大学里学习的计算机网络和操作系统的一些知识。但问题的根源 NFS 服务的性能为什么这么差还是没有特别好的解决方案，可能在这个场景里 NFS 就不是特别好的选择吧。</p>

<h2>Kubeflow &amp; Kale simplify building better ML Pipelines with automatic hyperparameter tuning</h2>

<p><a href="https://medium.com/kubeflow/kubeflow-kale-simplify-building-better-ml-pipelines-with-automatic-hyperparameter-tuning-5821747f4fcb">[链接]</a></p>

<p>Jupyter Notebooks 是当下数据科学家或者算法工程师日常工作非常重要的一个组件，交互式的界面加上即时的代码运行反馈极大地提升了开发效率。但是如果要将机器学习任务提交到集群中运行往往还得依靠类似 Kubeflow Pipelines 这种 DAG 管理及调度组件，Kubeflow Pipelines 有一套基于 Python API 的语法，因此用户需要再重新定义一个独立的 pipeline。有没有办法直接将 notebook 中已经验证过的代码自动转换成 pipeline 并提交到集群呢？<a href="https://kubeflow-kale.github.io">Kale</a>（<strong>K</strong>ubeflow <strong>A</strong>utomated Pipe<strong>L</strong>ines <strong>E</strong>ngine）即是为了解决这个问题而诞生，它是一个能够将 Jupyter Notebooks 自动转换为 Kubeflow Pipelines 的工具。在最新的 0.5 版本中 Kale 新增了对 <a href="https://github.com/kubeflow/katib">Katib</a> 的集成，后者是进行自动超参调优（Hyperparameter Tuning）和神经网络架构搜索（Neural Architecture Search）的组件。</p>

<h2>GoogleCloudPlatform/spark-on-k8s-operator #976: Add support for dynamic allocation via shuffle tracking</h2>

<p><a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/pull/976">[链接]</a></p>

<p>Spark 3.0 为<a href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">动态资源分配</a>（dynamic resource allocation）新增了 shuffle tracking 特性（默认关闭），具体实现可以查看 <a href="https://issues.apache.org/jira/browse/SPARK-27963">SPARK-27963</a>。当使用动态资源分配时用户需要预先设定诸如初始、最小和最大 executor 数量这样的参数，之后 Spark 运行时会根据当前任务排队时间和 executor 空闲时间这些指标去创建或者销毁 executor。对于有状态的 executor（如 shuffle 时存储到磁盘的数据、cache 到内存和磁盘的数据）会有一些特殊的策略防止错误回收资源，过去的做法是使用外部 shuffle 服务。开启 shuffle tracking 以后就不再依赖外部 shuffle 服务，而是设置一个 executor 持有 shuffle 数据的超时时间。过去 Spark 的 K8s 模式不支持外部 shuffle 服务，有了这个新的特性以后使得动态资源分配在 K8s 模式上成为可能。spark-on-k8s-operator 项目近期也支持了这个特性，可以直接通过 <a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#dynamic-allocation">YAML 配置</a>来开启。</p>

<h2>Boiled Hippo</h2>

<p><a href="https://spacefruityrecords.bandcamp.com/album/boiled-hippo-2">[Bandcamp]</a> <a href="https://music.163.com/#/album?id=91278378">[网易云音乐]</a> <a href="https://www.xiami.com/album/1ttwrEdcce1">[虾米]</a></p>

<p>本期最后推荐一张来自我的一个好朋友的唱片，Boiled Hippo 是一支北京的迷幻摇滚乐队，经过多年的演出积累终于在今年发行了乐队的第一张同名专辑。虽说是迷幻摇滚，但如果从旋律上讲绝对是非常「好听」的。如果你有兴趣购买实体唱片（黑胶、磁带、CD 都有），目前可以在北京的 fRUITYSPACE、fRUITYSHOP、独音唱片，上海的 Daily Vinyl，金华的 Wave 这几个地方购买。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #4]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4/"/>
    <updated>2020-06-17T14:07:52+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>AliGraph: A Comprehensive Graph Neural Network Platform</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3292500.3340404">[链接]</a></p>

<p>AliGraph 是阿里巴巴团队研发的 GNN（Graph Neural Network）分布式训练框架（虽然标题里是「平台」但感觉还算不上），论文发表在 KDD 2019 和 PVLDB 2019。</p>

<p>论文开篇便提出了当下 GNN 模型训练的 4 个挑战：</p>

<ol>
<li>如何提高大规模图模型的训练效率及优化空间占用？</li>
<li>怎样优雅地将异构（heterogeneous）信息组合到一个统一的 embedding 结果中？</li>
<li>如何将结构化的拓扑（topological）信息与非结构化的属性（attribute）信息统一来共同定义那些需要保留的信息？</li>
<li>如何设计一个高效的增量更新动态图的 GNN 方法？</li>
</ol>


<p>后面的篇章便是详细介绍 AliGraph 如何解决以上这 4 个问题。框架从上至下整体分为 3 层：算子（operator）、采样（sampling）、存储（storage）。算子层包含常见的 GNN 运算操作，采样层包含几种预设的采样算法，存储层主要关注如何高效对大规模图进行分布式存储。在这 3 层基础之上可以实现任意的 GNN 算法以及应用。</p>

<p>存储层因为是要解决一个图的分布式存储问题，因此首先要将图进行分割（partition）。AliGraph 内置了 4 种图分割算法：<a href="https://dm.kaist.ac.kr/kse625/resources/metis.pdf">METIS</a>、<a href="https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez">顶点切割和边切割</a>、<a href="https://dl.acm.org/doi/10.1145/2503210.2503293">2D 分割</a>、<a href="https://dl.acm.org/doi/10.1145/2339530.2339722">流式分割</a>。这 4 种算法分别适用于不同的场景，METIS 适合处理稀疏（sparse）的图，顶点切割和边切割适合密集（dense）的图，2D 分割适合 worker 数量固定的场景，流式分割通常应用在边（edge）频繁更新的图。用户需要根据自己的需求选择恰当的分割算法，当然也可以通过插件的形式自己实现。</p>

<p>另一个存储层关心的问题是如何将图结构和属性（attribute）共同存储。这里讲的图结构即顶点和边的信息，这是最主要的图数据。同时每个顶点也会附加一些独特的属性，例如某个顶点表示一个用户，那附加在这个用户上面的属性就是类似性别、年龄、地理位置这样的信息。如果直接将属性信息和图结构一起存储会造成非常大的空间浪费，因为从全局角度看同一种类型的顶点的属性是高度重合的。并且属性与图结构的大小差异也非常明显，一个顶点 ID 通常占用 8 字节，但是属性信息的大小从 0.1KB 到 1KB 都有可能 。因此 AliGraph 选择将属性信息单独存储，通过两个单独的索引分别存储顶点和边的属性，而图结构中只存储属性索引的 ID。这样设计的好处自然是显著降低了存储所需的空间，但代价就是降低了查询性能，因为需要频繁访问索引来获取属性信息。AliGraph 选择增加一层 LRU 缓存的方式对查询性能进行优化。</p>

<p>存储层关心的最后一个问题也是跟查询性能有关。在图算法中一个顶点的邻居（neighbor）是非常重要的信息，邻居可以是直接（1 跳）的也可以是间接（多跳）的，由于图被分割以后本地只会存储直接的邻居，当需要访问间接邻居的时候就必须通过网络通信与其它存储节点进行交互，这里的网络通信代价在大规模图计算中是不容忽视的。解决思路也很直接，即在每个节点本地缓存顶点的间接邻居，但要缓存哪些顶点的邻居，要缓存几个邻居是需要仔细考量的问题。AliGraph 没有使用目前常见的一些缓存算法（如 LRU），而是提出了一种新的基于顶点重要性（importance）的算法来对间接邻居进行缓存。在有向图中计算一个顶点重要性的公式是 <code>入邻居的个数 / 出邻居的个数</code>，注意这里的邻居个数同样可以是直接的或者间接的。当这个公式的计算结果大于某个用户自定义的阈值时即认为这是一个「重要」的顶点。从实际测试中得出的经验值是通常只需要计算两跳（hop）的邻居个数就够了，而阈值本身不是一个特别敏感的数值，设置在 0.2 左右是对于缓存成本和效果一个比较好的平衡。选出所有重要的顶点以后，最终会在所有包含这些顶点的节点上缓存 <em>k</em> 跳的出邻居（out-neighbor）。</p>

<p>GNN 算法通常可以总结为 3 个步骤：采样（sample）某个顶点的邻居，聚合（aggregate）这些采样后的顶点的 embedding，将聚合后的 embedding 与顶点自己的进行合并（combine）得到新的 embedding。这里可以看到采样是整个流程中的第一步，采样的效果也会直接影响后续计算的 embedding 结果。AliGraph 抽象了 3 类采样方法：遍历采样（traverse）、近邻采样（neighborhood）和负采样（negative）。遍历采样是从本地子图中获取数据；近邻采样对于 1 跳的邻居可以从本地存储中获取，多跳的邻居如果在缓存中就从缓存中获取否则就请求其它节点；负采样通常也是从本地挑选顶点，在某些特殊情况下有可能需要从其它节点挑选。</p>

<p>在采样完邻居顶点以后就是聚合这些顶点的 embedding，常用的聚合方法有：element-wise mean、max-pooling 和 LSTM。最后是将聚合后的 embedding 与顶点自己的进行合并，通常就是将这两个 embedding 进行求和。为了加速聚合和合并这两个算子的计算，AliGraph 应用了一个物化（materialization）中间向量的策略，即每个 mini-batch 中的所有顶点共享采样的顶点，同样的聚合和合并操作的中间结果也共享，这个策略会大幅降低计算成本。</p>

<p>在最后的评估环节用了两个来自淘宝的数据集，两个数据集之间只有大小的区别，大数据集是小数据集的 6 倍左右。大数据集的基础数据是：4.8 亿个用户顶点，968 万个商品顶点，65.8 亿条用户到商品的边，2.3 亿条商品到商品的边，用户平均有 27 个属性，商品平均有 32 个属性。当使用 200 个 worker（节点配置论文中没有说明）时大数据集只需要 5 分钟即可将整个图构建完毕，相比之下以往的一些方案可能需要耗费数小时。基于顶点重要性的缓存算法相比 LRU 这些传统算法也是明显更优。3 类采样方法的性能评估结果从几毫秒到几十毫秒不等，但最长也不超过 60 毫秒，并且采样性能与数据集大小不太相关。聚合和合并算子相比传统的实现也有一个数量级的性能提升，这主要得益于前面提到的物化策略。</p>

<p>AliGraph 目前已经开源（一部分？）但是换了一个名字叫做 <a href="https://github.com/alibaba/graph-learn">graph-learn</a>，跟大多数深度学习框架一样，底层使用 C++ 语言实现并提供 Python 语言的 API，目前支持 TensorFlow，未来会支持 PyTorch。有意思的是刚刚开源不久就有人提了一个 <a href="https://github.com/alibaba/graph-learn/issues/16">issue</a> 希望能够跟另外几个流行的 GNN 框架进行比较，但是项目成员的回答比较含糊。</p>

<h2>Building Uber’s Go Monorepo with Bazel</h2>

<p><a href="https://eng.uber.com/go-monorepo-bazel">[链接]</a></p>

<p>Uber 应该是除了 Google 以外很早选择在后端服务中大规模使用 Go 语言的公司之一，并贡献了很多著名的 Go 语言项目（如 <a href="https://github.com/uber-go/zap">zap</a>、<a href="https://github.com/jaegertracing/jaeger">Jaeger</a>）。早在 2017 年，Uber 的 Android 和 iOS 团队就已经只使用一个代码仓库进行开发，俗称 monorepo。实践 monorepo 最著名的公司应该还是 Google，有兴趣可以看看 <a href="https://research.google/pubs/pub45424">Why Google Stores Billions of Lines of Code in a Single Repository</a> 这篇文章。现在后端团队也开始采用 monorepo 来管理 Go 语言项目，但是和客户端团队的不同之处在于没有用 <a href="https://buck.build">Buck</a> 而是用 <a href="https://bazel.build">Bazel</a>（前者是 Facebook 开源，后者是 Google 开源）。这篇文章介绍了在 monorepo 中将 Go 语言和 Bazel 结合遇到的一些问题。</p>

<h2>Optimising Docker Layers for Better Caching with Nix</h2>

<p><a href="https://grahamc.com/blog/nix-and-layered-docker-images">[链接]</a></p>

<p>恐怕大多数时候接触容器是从构建一个 Docker 镜像开始的，这一步往往也是最容易被忽视的。为什么我的镜像这么大？为什么每次拉取镜像都要从头开始？这些问题可能会随着使用时间越来越长逐渐浮现出来，要回答它们需要了解 Docker 镜像的一个核心概念「layer」，本质上你在 <code>Dockerfile</code> 里写的每一行命令都会生成一个 layer，一个镜像便是由很多 layer 构成。Layer 之间是有层级关系的，当拉取镜像时如果本地已经存在某个 layer 就不会重复拉取。在传统的 Linux 发行版中安装依赖时 Docker 是不知道具体有哪些文件被修改的，而 <a href="https://github.com/NixOS/nix">Nix</a> 这个特殊的包管理器采用了不一样的设计思路使得安装依赖这件事情对于 Docker layer 缓存非常友好。衍生阅读推荐 Jérôme Petazzoni 写的关于如何减少镜像大小的<a href="https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html">系列文章</a>。</p>

<h2>Proposal: Permit embedding of interfaces with overlapping method sets</h2>

<p><a href="https://github.com/golang/proposal/blob/master/design/6977-overlapping-interfaces.md">[链接]</a></p>

<p>Interface 是 Go 语言一个重要的特性，类似很多其它语言中的概念，接口定义好以后是需要通过 struct 来实现的。但不同之处又在于 struct 不需要显式声明实现了什么 interface，只要满足 interface 中定义的接口就行，这个关键设计使得 Go 语言的 interface 使用场景可以非常灵活。跟 struct 一样 interface 也允许嵌套，也就是可以在一个 interface 定义中嵌套另一个 interface。如果同时嵌套了多个 interface，并且这些 interface 之间有重复的接口在编译时是会报错的。实际开发过程中为了规避这个限制可能需要修改 interface 的定义，这对于开发者来说不太友好。上面这个提案允许开发者在不修改代码的情况下避开这个限制，目前这个功能已经在 <a href="https://golang.org/doc/go1.14#language">Go 1.14</a> 中发布。</p>

<h2>VexTab</h2>

<p><a href="https://github.com/0xfe/vextab">[链接]</a></p>

<p>不管是音乐创作还是音乐演奏，乐谱都是一个必不可少的东西。还记得刚学吉他那会儿非常热衷的一件事情就是去网上搜集各种歌曲的六线谱，这些乐谱的格式从最朴素的纯文本到高级的 <a href="https://www.guitar-pro.com">Guitar Pro</a> 格式都有。再后来开始学习扒歌，也面临把扒下来的谱子纪录下来的需求。虽然 Guitar Pro 很好但毕竟是一个收费软件，文件格式也是私有的。就像我更喜欢 Markdown 而不是直接用 Word 一样，一直希望能有一个类似的标记语言用于编写乐谱。VexTab 即是这样一个专门用于编写五线谱和六线谱的语言，也提供一个 JavaScript 库方便嵌入到网页中。有意思的是 VexTab 的作者同时也是 Google 的一名员工。</p>
]]></content>
  </entry>
  
</feed>
