<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Maybe News | Freedom]]></title>
  <link href="https://blog.xiaogaozi.org/categories/maybe-news/atom.xml" rel="self"/>
  <link href="https://blog.xiaogaozi.org/"/>
  <updated>2020-06-17T14:24:05+08:00</updated>
  <id>https://blog.xiaogaozi.org/</id>
  <author>
    <name><![CDATA[xiaogaozi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #4]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4/"/>
    <updated>2020-06-17T14:07:52+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/17/maybe-news-issue-4</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>AliGraph: A Comprehensive Graph Neural Network Platform</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3292500.3340404">[链接]</a></p>

<p>AliGraph 是阿里巴巴团队研发的 GNN（Graph Neural Network）分布式训练框架（虽然标题里是「平台」但感觉还算不上），论文发表在 KDD 2019 和 PVLDB 2019。</p>

<p>论文开篇便提出了当下 GNN 模型训练的 4 个挑战：</p>

<ol>
<li>如何提高大规模图模型的训练效率及优化空间占用？</li>
<li>怎样优雅地将异构（heterogeneous）信息组合到一个统一的 embedding 结果中？</li>
<li>如何将结构化的拓扑（topological）信息与非结构化的属性（attribute）信息统一来共同定义那些需要保留的信息？</li>
<li>如何设计一个高效的增量更新动态图的 GNN 方法？</li>
</ol>


<p>后面的篇章便是详细介绍 AliGraph 如何解决以上这 4 个问题。框架从上至下整体分为 3 层：算子（operator）、采样（sampling）、存储（storage）。算子层包含常见的 GNN 运算操作，采样层包含几种预设的采样算法，存储层主要关注如何高效对大规模图进行分布式存储。在这 3 层基础之上可以实现任意的 GNN 算法以及应用。</p>

<p>存储层因为是要解决一个图的分布式存储问题，因此首先要将图进行分割（partition）。AliGraph 内置了 4 种图分割算法：<a href="https://dm.kaist.ac.kr/kse625/resources/metis.pdf">METIS</a>、<a href="https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez">顶点切割和边切割</a>、<a href="https://dl.acm.org/doi/10.1145/2503210.2503293">2D 分割</a>、<a href="https://dl.acm.org/doi/10.1145/2339530.2339722">流式分割</a>。这 4 种算法分别适用于不同的场景，METIS 适合处理稀疏（sparse）的图，顶点切割和边切割适合密集（dense）的图，2D 分割适合 worker 数量固定的场景，流式分割通常应用在边（edge）频繁更新的图。用户需要根据自己的需求选择恰当的分割算法，当然也可以通过插件的形式自己实现。</p>

<p>另一个存储层关心的问题是如何将图结构和属性（attribute）共同存储。这里讲的图结构即顶点和边的信息，这是最主要的图数据。同时每个顶点也会附加一些独特的属性，例如某个顶点表示一个用户，那附加在这个用户上面的属性就是类似性别、年龄、地理位置这样的信息。如果直接将属性信息和图结构一起存储会造成非常大的空间浪费，因为从全局角度看同一种类型的顶点的属性是高度重合的。并且属性与图结构的大小差异也非常明显，一个顶点 ID 通常占用 8 字节，但是属性信息的大小从 0.1KB 到 1KB 都有可能 。因此 AliGraph 选择将属性信息单独存储，通过两个单独的索引分别存储顶点和边的属性，而图结构中只存储属性索引的 ID。这样设计的好处自然是显著降低了存储所需的空间，但代价就是降低了查询性能，因为需要频繁访问索引来获取属性信息。AliGraph 选择增加一层 LRU 缓存的方式对查询性能进行优化。</p>

<p>存储层关心的最后一个问题也是跟查询性能有关。在图算法中一个顶点的邻居（neighbor）是非常重要的信息，邻居可以是直接（1 跳）的也可以是间接（多跳）的，由于图被分割以后本地只会存储直接的邻居，当需要访问间接邻居的时候就必须通过网络通信与其它存储节点进行交互，这里的网络通信代价在大规模图计算中是不容忽视的。解决思路也很直接，即在每个节点本地缓存顶点的间接邻居，但要缓存哪些顶点的邻居，要缓存几个邻居是需要仔细考量的问题。AliGraph 没有使用目前常见的一些缓存算法（如 LRU），而是提出了一种新的基于顶点重要性（importance）的算法来对间接邻居进行缓存。在有向图中计算一个顶点重要性的公式是 <code>入邻居的个数 / 出邻居的个数</code>，注意这里的邻居个数同样可以是直接的或者间接的。当这个公式的计算结果大于某个用户自定义的阈值时即认为这是一个「重要」的顶点。从实际测试中得出的经验值是通常只需要计算两跳（hop）的邻居个数就够了，而阈值本身不是一个特别敏感的数值，设置在 0.2 左右是对于缓存成本和效果一个比较好的平衡。选出所有重要的顶点以后，最终会在所有包含这些顶点的节点上缓存 <em>k</em> 跳的出邻居（out-neighbor）。</p>

<p>GNN 算法通常可以总结为 3 个步骤：采样（sample）某个顶点的邻居，聚合（aggregate）这些采样后的顶点的 embedding，将聚合后的 embedding 与顶点自己的进行合并（combine）得到新的 embedding。这里可以看到采样是整个流程中的第一步，采样的效果也会直接影响后续计算的 embedding 结果。AliGraph 抽象了 3 类采样方法：遍历采样（traverse）、近邻采样（neighborhood）和负采样（negative）。遍历采样是从本地子图中获取数据；近邻采样对于 1 跳的邻居可以从本地存储中获取，多跳的邻居如果在缓存中就从缓存中获取否则就请求其它节点；负采样通常也是从本地挑选顶点，在某些特殊情况下有可能需要从其它节点挑选。</p>

<p>在采样完邻居顶点以后就是聚合这些顶点的 embedding，常用的聚合方法有：element-wise mean、max-pooling 和 LSTM。最后是将聚合后的 embedding 与顶点自己的进行合并，通常就是将这两个 embedding 进行求和。为了加速聚合和合并这两个算子的计算，AliGraph 应用了一个物化（materialization）中间向量的策略，即每个 mini-batch 中的所有顶点共享采样的顶点，同样的聚合和合并操作的中间结果也共享，这个策略会大幅降低计算成本。</p>

<p>在最后的评估环节用了两个来自淘宝的数据集，两个数据集之间只有大小的区别，大数据集是小数据集的 6 倍左右。大数据集的基础数据是：4.8 亿个用户顶点，968 万个商品顶点，65.8 亿条用户到商品的边，2.3 亿条商品到商品的边，用户平均有 27 个属性，商品平均有 32 个属性。当使用 200 个 worker（节点配置论文中没有说明）时大数据集只需要 5 分钟即可将整个图构建完毕，相比之下以往的一些方案可能需要耗费数小时。基于顶点重要性的缓存算法相比 LRU 这些传统算法也是明显更优。3 类采样方法的性能评估结果从几毫秒到几十毫秒不等，但最长也不超过 60 毫秒，并且采样性能与数据集大小不太相关。聚合和合并算子相比传统的实现也有一个数量级的性能提升，这主要得益于前面提到的物化策略。</p>

<p>AliGraph 目前已经开源（一部分？）但是换了一个名字叫做 <a href="https://github.com/alibaba/graph-learn">graph-learn</a>，跟大多数深度学习框架一样，底层使用 C++ 语言实现并提供 Python 语言的 API，目前支持 TensorFlow，未来会支持 PyTorch。有意思的是刚刚开源不久就有人提了一个 <a href="https://github.com/alibaba/graph-learn/issues/16">issue</a> 希望能够跟另外几个流行的 GNN 框架进行比较，但是项目成员的回答比较含糊。</p>

<h2>Building Uber’s Go Monorepo with Bazel</h2>

<p><a href="https://eng.uber.com/go-monorepo-bazel">[链接]</a></p>

<p>Uber 应该是除了 Google 以外很早选择在后端服务中大规模使用 Go 语言的公司之一，并贡献了很多著名的 Go 语言项目（如 <a href="https://github.com/uber-go/zap">zap</a>、<a href="https://github.com/jaegertracing/jaeger">Jaeger</a>）。早在 2017 年，Uber 的 Android 和 iOS 团队就已经只使用一个代码仓库进行开发，俗称 monorepo。实践 monorepo 最著名的公司应该还是 Google，有兴趣可以看看 <a href="https://research.google/pubs/pub45424">Why Google Stores Billions of Lines of Code in a Single Repository</a> 这篇文章。现在后端团队也开始采用 monorepo 来管理 Go 语言项目，但是和客户端团队的不同之处在于没有用 <a href="https://buck.build">Buck</a> 而是用 <a href="https://bazel.build">Bazel</a>（前者是 Facebook 开源，后者是 Google 开源）。这篇文章介绍了在 monorepo 中将 Go 语言和 Bazel 结合遇到的一些问题。</p>

<h2>Optimising Docker Layers for Better Caching with Nix</h2>

<p><a href="https://grahamc.com/blog/nix-and-layered-docker-images">[链接]</a></p>

<p>恐怕大多数时候接触容器是从构建一个 Docker 镜像开始的，这一步往往也是最容易被忽视的。为什么我的镜像这么大？为什么每次拉取镜像都要从头开始？这些问题可能会随着使用时间越来越长逐渐浮现出来，要回答它们需要了解 Docker 镜像的一个核心概念「layer」，本质上你在 <code>Dockerfile</code> 里写的每一行命令都会生成一个 layer，一个镜像便是由很多 layer 构成。Layer 之间是有层级关系的，当拉取镜像时如果本地已经存在某个 layer 就不会重复拉取。在传统的 Linux 发行版中安装依赖时 Docker 是不知道具体有哪些文件被修改的，而 <a href="https://github.com/NixOS/nix">Nix</a> 这个特殊的包管理器采用了不一样的设计思路使得安装依赖这件事情对于 Docker layer 缓存非常友好。衍生阅读推荐 Jérôme Petazzoni 写的关于如何减少镜像大小的<a href="https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html">系列文章</a>。</p>

<h2>Proposal: Permit embedding of interfaces with overlapping method sets</h2>

<p><a href="https://github.com/golang/proposal/blob/master/design/6977-overlapping-interfaces.md">[链接]</a></p>

<p>Interface 是 Go 语言一个重要的特性，类似很多其它语言中的概念，接口定义好以后是需要通过 struct 来实现的。但不同之处又在于 struct 不需要显式声明实现了什么 interface，只要满足 interface 中定义的接口就行，这个关键设计使得 Go 语言的 interface 使用场景可以非常灵活。跟 struct 一样 interface 也允许嵌套，也就是可以在一个 interface 定义中嵌套另一个 interface。如果同时嵌套了多个 interface，并且这些 interface 之间有重复的接口在编译时是会报错的。实际开发过程中为了规避这个限制可能需要修改 interface 的定义，这对于开发者来说不太友好。上面这个提案允许开发者在不修改代码的情况下避开这个限制，目前这个功能已经在 <a href="https://golang.org/doc/go1.14#language">Go 1.14</a> 中发布。</p>

<h2>VexTab</h2>

<p><a href="https://github.com/0xfe/vextab">[链接]</a></p>

<p>不管是音乐创作还是音乐演奏，乐谱都是一个必不可少的东西。还记得刚学吉他那会儿非常热衷的一件事情就是去网上搜集各种歌曲的六线谱，这些乐谱的格式从最朴素的纯文本到高级的 <a href="https://www.guitar-pro.com">Guitar Pro</a> 格式都有。再后来开始学习扒歌，也面临把扒下来的谱子纪录下来的需求。虽然 Guitar Pro 很好但毕竟是一个收费软件，文件格式也是私有的。就像我更喜欢 Markdown 而不是直接用 Word 一样，一直希望能有一个类似的标记语言用于编写乐谱。VexTab 即是这样一个专门用于编写五线谱和六线谱的语言，也提供一个 JavaScript 库方便嵌入到网页中。有意思的是 VexTab 的作者同时也是 Google 的一名员工。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #3]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3/"/>
    <updated>2020-06-10T17:37:27+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/10/maybe-news-issue-3</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>Kudu: Storage for Fast Analytics on Fast Data</h2>

<p><a href="https://kudu.apache.org/kudu.pdf">[链接]</a></p>

<p><a href="https://en.wikipedia.org/wiki/Online_analytical_processing">OLAP</a>（Online Analytical Processing）一直是大数据领域非常重要的应用场景，光有数据也不行，你得「分析」啊。自从有了 Hadoop，OLAP 的工具就一直在演变，从最早的裸写 MapReduce 任务，到 <a href="https://pig.apache.org">Pig</a>、<a href="https://hive.apache.org">Hive</a>、<a href="https://prestosql.io">Presto</a>、<a href="https://impala.apache.org">Impala</a>、<a href="https://druid.apache.org">Druid</a>、<a href="https://clickhouse.tech">ClickHouse</a>，以及今天要介绍的 <a href="https://kudu.apache.org">Kudu</a>。一个明显的趋势是 OLAP 引擎在逐步朝着「去 Hadoop 化」和「实时化」发展，当然这些项目里最新的也已经是 2016 年发布的了，接下来会怎么变化还是个未知数。</p>

<p>先讲讲为什么会有类似 Kudu 这样的项目诞生。传统的 OLAP 引擎因为是构建在 HDFS 上的，要想分析数据首先得将数据存储到 HDFS 上，而这个过程（通常叫做 ETL）往往是比较耗时以及复杂的。同时由于 HDFS 天生不支持随机读写，为了弥补这个「缺陷」，有了 HBase 这样的项目。但 HBase 对于 OLAP 场景是不够友好的，因此往往需要把数据从 HBase 再导入到 HDFS 中，这个过程也可能比较耗时，维护成本也比较高。因此 Kudu 的目标是实现一个即支持随机读写（主要是写），又针对大批量查询进行优化的存储引擎。这种时候 HDFS 就显得很累赘了，这也是为什么越来越多引擎选择不依赖 HDFS 的缘故（Kudu 官网也在 FAQ 中专门<a href="https://kudu.apache.org/faq.html#why-doesnt-kudu-store-its-data-in-hdfs">解释</a>了为什么不用 HDFS）。当然并不是说 HDFS 就没用了，有很多数据还是非常静态的，对于实时性要求也不高，此时用 HDFS 是一种简单经济的选择。</p>

<p>这篇论文虽然介绍的是 Kudu 早期的一些设计思想，但基本上属于最核心的功能。跟很多分布式数据库一样，Kudu 也是受 <a href="https://research.google/pubs/pub39966">Spanner</a> 启发。系统架构上分为一个 Master 服务和若干 Tablet 服务。Master 负责维护元信息，包括 Tablet 节点和数据的。Tablet 服务则负责数据存储，每台节点上会有几十至数百个 tablet，每个 tablet 中包含了若干数据，最大可以达到几十 GB 的规模（这里你可以把 tablet 类比为很多别的系统中的 region 概念）。</p>

<p>跟很多关系数据库一样，Kudu 是有 table 的概念的。但跟很多 NoSQL 数据库不一样的地方是，强制用户必须显式定义 schema。Kudu 一个有意思的设计在于同时支持了 hash 和 range 这两种数据 partition 方法，而不像别的系统只支持其中一种（有关这两种 partition 的介绍可以看我之前的<a href="https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5/">一篇文章</a>）。这样设计的好处是即保留了 hash 的数据均匀分配特点，可以在一定程度上防止读写热点，又保留了 range 对于范围扫描的友好性。</p>

<p>Tablet 服务之间是通过 Raft 来进行数据复制，因此可以认为 Kudu 是一个保证强一致性的存储系统。值得注意的是 Kudu 的默认设置是 500 毫秒的心跳间隔以及 1.5 秒的选举超时，这个跟 Raft 论文推荐的时间相比长了不少（推荐的选举超时是 150~300 毫秒）。当集群扩容时，新节点将会首先进入 <code>PRE_VOTER</code> 状态，等到 log 追上以后再变成 <code>VOTER</code> 状态，这个设计也是 Raft 论文中建议的，不过论文中叫做 learner 或者 non-voting member。Master 服务虽然是单点设计（即状态不是分布式存储），但为了保障高可用也可以通过 Raft 实现多节点状态复制，只不过任意时间只能有一个节点工作。</p>

<p>Kudu 的数据存储引擎是完全自己设计的，没有直接用任何现有的引擎，虽然也能多少看出一些别的引擎的影子。关于这一点可以理解，OLAP 系统区别于 <a href="https://en.wikipedia.org/wiki/Online_transaction_processing">OLTP</a> （Online Transactional Processing）系统的最大不同即在于数据存储的形式，简单理解后者是行式（row-oriented）存储，而前者是列式（column-oriented）存储。著名的 <a href="https://parquet.apache.org">Parquet</a> 就是广泛被用于 OLAP 场景的列式存储格式，Kudu 在实现上也复用了很多 Parquet 的代码。</p>

<p>每个 table 在存储级别会被分割为多个 RowSets，顾名思义每个 RowSets 是由很多行（row）组成，RowSets 之间不会有重复的数据，但主键的范围可能会交叉。</p>

<p>当有新的数据时会首先存储到内存中的 MemRowSets，底层实现是一个使用乐观锁（optimistic locking）的并发（concurrent）B 树。比较特别的一点是数据并不是一开始就按照列式进行存储，MemRowSets 中还是用的行式存储。当数据累积到一定程度 MemRowSets 就会持久化到磁盘上，称之为 DiskRowSets，每个 DiskRowSet 大小上限是 32MB。DiskRowSet 由两部分组成：基础数据（base data）和增量数据（delta stores）。</p>

<p>基础数据是列式格式，即每一列都单独连续存储，每一列内部又划分成了多个小的页（page），有一个 B 树根据行号索引了这些页。每一列可以由用户指定不同的编码（encoding）方法（如 dictionary encoding、bitshuffle、front coding），同时也可以使用通用的压缩算法对数据进行压缩（如 LZ4、gzip、bzip2），基于列的编码及数据压缩是列式存储非常大的一个特点。</p>

<p>增量数据也分为内存和磁盘两种形式。内存中的叫做 DeltaMemStores，这个跟 MemRowSets 的实现一样。磁盘中的叫做 DeltaFiles，是一个二进制类型的列块（column block）。不管是内存还是磁盘上的数据都会有一个额外的从 <code>(row_offset, timestamp)</code> 到 RowChangeList 的映射，<code>row_offset</code> 是某一行在一个 RowSet 中的偏移，RowChangList 是二进制编码以后的增量操作（如更新某一列、删除某一行）列表。同样的，DeltaMemStores 也会持久化到磁盘上变成 DeltaFiles。</p>

<p>这些增量数据会定期跟基础数据进行合并（compation），以防止过多的增量文件。同时 DiskRowSets 之间也会进行合并，目的是清理已经被删除的行以及减少 DiskRowSets 之间的主键交叉范围。</p>

<p>前面提到的将内存中的数据持久化到磁盘及对数据进行合并操作，都是由一组单独的后台任务来完成，但是什么时候执行什么操作是由一个调度器来控制的。有趣的是 Kudu 将调度器的逻辑抽象成了一个<a href="https://en.wikipedia.org/wiki/Knapsack_problem">背包问题</a>，只不过需要权衡的不是背包容量，而是 I/O 带宽。</p>

<p>Kudu 本身只提供编程语言级别的 API（如 Java、C++），而 OLAP 系统中常用的 SQL 需要配合其它项目来实现。Kudu 原生已经跟 Spark 和 Impala 集成，也就是说你可以在这两个系统中通过 SQL 来查询。</p>

<p>最后是性能评测。在 <a href="http://www.tpc.org/tpch">TPC-H</a> 数据集上与 Parquet 进行对比测试，Kudu 平均有 31% 的性能提升，尽管如此 Kudu 团队认为随着 Parquet 的迭代这个差距可能会逐渐缩小。在与 <a href="http://phoenix.apache.org">Phoenix</a> 的对比测试中也有 16~187 倍的性能提升。最后与 HBase 进行的 <a href="https://github.com/brianfrankcooper/YCSB">YCSB</a> 测试是为了看看 Kudu 在 OLTP 场景的性能，虽然它本身并不是为 OLTP 场景而设计，结果上的确也是 HBase 表现更好，但 Kudu P99 6 毫秒的响应时间在某些时候也许可以代替 OLTP 系统。</p>

<p>Kudu 由 Cloudera 公司开发并于 2016 年正式发布，现已捐献给 Apache 基金会，整个系统使用 C++ 语言编写。</p>

<p>顺带说个题外话这两年炒得比较火的 <a href="https://en.wikipedia.org/wiki/Hybrid_transactional/analytical_processing">HTAP</a>（Hybrid Transactional/Analytical Processing），本质上是希望在一个引擎中同时适配 OLTP 和 OLAP 这两个场景。但在我看来就目前的技术现状这个愿景实现起来还是比较困难，软件工程界的名言<a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">「没有银弹」</a>告诉我们不存在一个可以通吃的、完美的方案，因此 HTAP 目前更多还只是一个营销概念吧。</p>

<h2>字节跳动自研强一致在线 KV &amp; 表格存储实践</h2>

<p><a href="https://mp.weixin.qq.com/s/jdPE9WClBuimIHVxJnwwUw">[上篇]</a> <a href="https://mp.weixin.qq.com/s/DvUBnWBqb0XGnicKUb-iqg">[下篇]</a></p>

<p><a href="https://github.com/cockroachdb/cockroach">又</a><a href="https://github.com/pingcap/tidb">又</a><a href="https://github.com/dgraph-io/dgraph">又</a><a href="https://kudu.apache.org">又</a><a href="https://github.com/vesoft-inc/nebula">又</a>一个受 Spanner 启发的分布式存储（Google 功德无量！Jeff Dean 万寿无疆！），这次的项目来自字节跳动。关键词：range 分割、Raft、RocksDB、MVCC、分布式事务、SQL 层，看看这些也基本能对整体设计猜个八九不离十了，比较有价值的信息是学习学习字节跳动在他们的实践中的一些经验。项目使用 C++ 语言编写，目前没有开源。</p>

<h2>Challenges Supporting MIG in Kubernetes</h2>

<p><a href="https://docs.google.com/document/d/1Dxx5MwG_GiBeKOuMNwv4QbO8OqA7XFdzn7fzzI7AQDg">[链接]</a></p>

<p>随着深度学习的蓬勃发展，GPU 共享逐渐成为了 K8s 社区的一个<a href="https://github.com/kubernetes/kubernetes/issues/52757">热门话题</a>。目前 NVIDIA 官方提供的<a href="https://github.com/NVIDIA/k8s-device-plugin">设备插件</a>可以申请的最小资源粒度还是 1 个 GPU，但很多时候资源是浪费的。为了提升 GPU 的资源利用率，社区已经出现了多种解决方案，例如分别来自<a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender">阿里云</a>、<a href="https://github.com/tkestack/gpu-manager">腾讯云</a>以及 <a href="https://github.com/awslabs/aws-virtual-gpu-device-plugin">AWS</a> 的实现。现在 NVIDIA 官方终于在新一代的 Ampere 架构硬件上原生支持了共享，也就是标题中的 MIG（Multi-Instance GPUs）。这篇文档来自 NVIDIA 团队，首先介绍了当前是如何在 K8s 中管理 GPU 资源的，然后介绍了 MIG 的一些概念，最后提议了 4 个支持 MIG 的可能的解决方案。整体感觉 GPU 共享还是没有 CPU 灵活，不少地方设置了限制，但毕竟这是 NVIDIA 官方迈出的第一步。</p>

<h2>How to read deep learning papers?</h2>

<p><a href="https://www.reddit.com/r/MachineLearning/comments/gi3ihe/d_how_to_read_deep_learning_papers">[链接]</a></p>

<p>Reddit 上一个有趣的讨论：如何阅读深度学习的论文？我们常常调侃机器学习就是在「炼丹」，没有人能解释为什么结果就是有效的，反正<a href="https://www.youtube.com/watch?v=YPN0qhSyWy8">「it just works」</a>。最高票的评论说你不需要接受论文中的每一个观点，只要把注意力集中在作者提供的证据并有选择性地调整你的想法就好了。正好前段时间前微软执行副总裁沈向洋博士做了一个主题名为<a href="https://www.bilibili.com/video/BV1df4y1m74k">「You are how you read」</a>的演讲，主要内容就是一些阅读论文的经验（有趣的是沈博士在几年前还写过一篇叫做<a href="https://www.linkedin.com/pulse/you-what-write-harry-shum">「You are what you write」</a>的博客）。</p>

<h2>Farewell, TensorFlow</h2>

<p><a href="https://mrry.github.io/2020/05/10/farewell-tensorflow.html">[链接]</a></p>

<p>TensorFlow 核心开发者、Google Brain 团队的 Derek Murray 宣布离开，这位大哥在 GitHub 和 Stack Overflow 上都很活跃，如果你经常浏览社区应该对他的头像不陌生。在这篇告别文中 Murray 提到了很多 Google 内部帮助工程师解决问题的工具，并详细介绍了近期对 TensorFlow 底层运行时进行的一项性能优化的过程，在部分评测中可以提升 10% 的推理性能。这个优化目前已经合入 master，并将在 TensorFlow 2.3 发布。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #2]]></title>
    <link href="https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2/"/>
    <updated>2020-06-02T09:25:45+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/06/02/maybe-news-issue-2</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>In Search of an Understandable Consensus Algorithm (Extended Version)</h2>

<p><a href="https://raft.github.io/raft.pdf">[链接]</a></p>

<p>终于有机会仔细阅读一遍 Raft 的论文，如果你还不了解 Raft 是什么可以看看我过去的一篇介绍分布式系统基础概念的<a href="https://blog.xiaogaozi.org/2020/05/25/how-to-design-a-distributed-index-framework-part-5/">文章</a>。</p>

<p>Raft 为节点定义了三种状态：leader、follower 和 candidate（以及一个非正式状态 learner 或者叫做 non-voting member）。一个集群只会有 1 个 leader，其余节点都是 follower。Leader 负责处理所有的读写请求，如果请求 follower 会失败并告知客户端 leader 的地址。</p>

<p>每个节点都有一个自己的 log，log 中每个条目都有一个下标（index）。这个 log 基本算是 append-only 的，通常也需要持久化到可靠的存储上（例如磁盘）。当处理写请求时 leader 会首先更新自己的 log，然后通过 RPC 复制到其它节点，只要大多数（majority）节点更新成功 leader 就会认为这个请求已经 committed，此时会更新自己的状态机（state machine）并返回给客户端。如果 RPC 请求失败 leader 会不断重试直到成功。</p>

<p>如果出现异常，如 leader 宕机、网络故障等，就可能触发 leader 重新选举。选举过程是所有 follower 为 candidate 投票，只要获得多数票 candidate 就会升级为 leader。如果投票失败会继续新一轮选举，选举过程通常是毫秒级的。每一轮新的选举都会产生一个对应的 term（任期），Raft 在协议上保证了重新选举后的新 leader 一定是包含之前所有 term 已经 committed 的 log，这样就避免了新 leader 选举成功以后需要首先补上缺失的数据。</p>

<p>当集群需要伸缩时，leader 会首先将旧集群配置（configuration）和新集群配置合并到一起并通过 log 的形式复制到 follower。成功收到这个合并后配置的节点会用这个配置替代老的配置。一旦这个合并后的配置 committed，leader 就会创建一个只包含新配置的 log 继续复制到 follower。等到新的配置 committed，旧配置将不再生效，需要下线的节点也可以被安全关闭。</p>

<p>随着时间增长，log 的容量会越来越大，Raft 引入了快照（snapshot）机制，定期将 log 压缩到快照文件。这个快照文件同时也可以帮助新加入的节点快速补上缺失的数据。</p>

<p>总结一下 Raft 算法保证了以下几个属性始终成立：</p>

<ul>
<li><strong>Election Safety</strong>：在一个特定的任期最多只能有一个 leader 被选举出来</li>
<li><strong>Leader Append-Only</strong>：leader 永远不会覆盖或者删除 log 中的条目，只会追加新的条目。</li>
<li><strong>Log Matching</strong>：如果两份 log 同时包含一个具有相同任期数和下标的条目，那么这两份 log 中这个下标之前的所有条目都应该是一致的。</li>
<li><strong>Leader Completeness</strong>：如果某个任期中的一个 log 条目已经 committed，那么在之后任期中选举出的新 leader 一定包含这个条目。</li>
<li><strong>State Machine Safety</strong>：如果一个节点已经将一个给定下标的 log 条目更新到自己的状态机，那么其它节点上同样的下标一定不会是不同的条目，也就是说不会更新一个不同的条目到自己的状态机。</li>
</ul>


<p>更多有关 Raft 的信息可以查看它的<a href="https://raft.github.io">官网</a>，强烈建议初次接触一致性协议的朋友看看网站上的动画演示，非常有助于建立一个形象直观的认知。</p>

<h2>Scaling Raft</h2>

<p><a href="https://www.cockroachlabs.com/blog/scaling-raft">[链接]</a></p>

<p>作为前面介绍 Raft 的一篇衍生阅读，原始的 Raft 实现是将所有节点看作一个 group，这种设计在某些场景（例如集群规模很小）是可行的。但是当集群规模大到一定程度，或者类似 <a href="https://github.com/cockroachdb/cockroach">CockroachDB</a> 和 <a href="https://github.com/tikv/tikv">TiKV</a> 这种将数据划分为非常多的 range，多个 range 组成一个 Raft group 的场景（通常叫做 Multi-Raft），就会发现 Raft 的基础网络通信已经足以影响单节点的性能（比如过多的心跳请求）。因此社区已经针对这样的问题有了一些优化方案，比如 <a href="https://github.com/cockroachdb/cockroach/issues/357">CockroachDB 的方案</a>和 <a href="https://github.com/tikv/tikv/pull/4591">TiKV 的方案</a>。这两个方案都很类似，基本思想是暂停那些不活跃的 Raft group 的网络通信，等到需要的时候再唤醒。</p>

<h2>Why Generics?</h2>

<p><a href="https://blog.golang.org/why-generics">[链接]</a></p>

<p>这篇文章是 Ian Lance Taylor 在 GopherCon 2019 演讲的文字版（文章中也附带了视频），主要介绍了目前 Go 的核心开发者关于泛型（generics）的一些思考。总的来说 Go 核心团队的设计思想还是保持 Go 语言一贯的简洁，不希望引入过多的概念和复杂性。大部分新增的语法特性都由提供泛型接口的开发者来学习，对于使用者来说和调用普通接口几乎没有区别。早在 2016 年社区就已经有了 <a href="https://github.com/golang/go/issues/15292">#15292</a> 这个关于泛型的讨论，并且还在持续更新中，目前已经有了 710 条评论，Ian Lance Taylor 也在其中积极回复。虽然这个 issue 打上了 Go2 的标签，但泛型特性是否能在 Go 语言的 2.0 版本中出现现在还是个未知数。</p>

<h2>The Open Application Model from Alibaba’s Perspective</h2>

<p><a href="https://www.infoq.com/articles/oam-alibaba">[链接]</a></p>

<p>阿里云和微软在去年<a href="https://cloudblogs.microsoft.com/opensource/2019/10/16/announcing-open-application-model">共同宣布</a>了 Open Application Model（OAM），OAM 组织的<a href="https://github.com/orgs/oam-dev/people">核心成员</a>同时也是前 CoreOS 团队成员以及 etcd、K8s Operator 的创造者。简单理解 OAM 就是希望将传统的 K8s YAML 配置抽象成两部分：开发者和运维，开发者的配置中只包含与业务最相关的内容，而运维的配置中则包含与运行环境相关的内容。本质上是希望将开发者和运维的界线分得更清楚，让不同的角色更专注于自己的领域。在我看来 OAM 的好处当然是降低了普通开发者接入 K8s 的门槛，所谓大道至简，但这种表面上的「简」背后隐藏的复杂性也是不能忽略的。理想情况是某个云服务商能够完全包办所有跟运维有关的事情，用户只需要负责业务开发就好了。但现状还是不管多小的公司都肯定会有专人在负责运维工作。很多年前 Google App Engine 刚诞生时让所有人都眼前一亮，都认为这才是软件开发的未来啊，但即使是 Google 也没能让这个趋势持续下去。最近几年这个趋势又开始回潮，只不过换了一个名字叫做「Serverless」，希望这一次能够持续下去，虽然还有很长的路要走。</p>

<h2>Lightweight coscheduling based on back-to-back queue sorting</h2>

<p><a href="https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/kep/20200116-lightweight-coscheduling-based-on-back-to-back-queue-sorting.md">[链接]</a></p>

<p>自从 K8s 1.15 新增了 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework">Scheduling Framework</a> 以后，原生调度器的扩展性有了很大程度的增强。这个 KEP 来自阿里云团队，提出了基于 Scheduling Framework 来实现 coscheduling（或者叫做 gang scheduling）。Coscheduling 这个特性对于机器学习任务来说是非常重要的，一个任务通常包含多个 pod，只有当多个 pod 能够同时运行时这个任务才算是正常运行，如果只有部分 pod 可以运行其实是一种资源的浪费。因此 coscheduling 保证的就是一个任务必须满足一定数量的 pod 都能够被调度时才会实际分配资源。这个特性在 K8s 社区早有讨论，也诞生了一些相关联的项目，如 <a href="https://volcano.sh">Volcano</a>（前身是 <a href="https://github.com/kubernetes-sigs/kube-batch">kube-batch</a>）。5 月初这个插件的第一版已经被 <a href="https://github.com/kubernetes-sigs/scheduler-plugins/pull/4">merge</a> 到 scheduler-plugins 项目。</p>

<h2>Scheduler Support for Elastic Quota Management</h2>

<p><a href="https://docs.google.com/document/d/1ViujTXLP1XX3WKYUTk6u5LTdJ1sX-tVIw9_t9_mLpIc/edit?usp=sharing">[链接]</a></p>

<p>同样是与 K8s 相关的一个讨论，也同样来自阿里云团队。<code>ResourceQuota</code> 是 K8s 目前提供的一种限制某个 namespace 最大资源使用量的方式，但是在实际的多租户场景中，<code>ResourceQuota</code> 往往显得不够灵活。很多时候我们是希望给每个租户一个可以保证（guarantee）的最小资源量，以及一个超卖的最大资源量。当某个租户的资源比较空闲时，就允许其它租户临时租用。但是调度器也要保障这个租户有能力在必要的时候可以拿回这些被租用的资源，这通常是通过抢占（preemption）的方式来实现。这个提案就提出了扩展 <code>ResourceQuota</code> 来实现类似功能的想法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #1]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1/"/>
    <updated>2020-05-21T17:34:22+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/21/maybe-news-issue-1</id>
    <content type="html"><![CDATA[<blockquote><p>「Maybe News」是一个定期（或许不定期）分享一些可能是新闻的知识的<a href="/categories/maybe-news/">系列文章</a>，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>它。</p></blockquote>

<!-- more -->


<h2>CFS: A Distributed File System for Large Scale Container Platforms</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3299869.3314046">[链接]</a></p>

<p>跟<a href="https://blog.xiaogaozi.org/2020/04/26/weekly-reading-list-issue-1/">上次介绍</a>的 FoundationDB Record Layer 一样，这篇来自京东团队的论文也是发表在 SIGMOD 2019，介绍了一个为大规模容器平台设计的分布式文件系统。</p>

<p>系统整体由 3 部分组成：元数据子系统（metadata subsystem）、数据子系统（data subsystem）、资源管理器（resource manager）。元数据子系统负责维护 inode 和 dentry（directory entry），数据子系统负责存储数据块，资源管理器负责处理客户端的各种文件操作请求以及维护前面两个子系统的状态。元数据子系统和数据子系统都是多 partition 的分布式系统，多个元数据和数据的 partition 逻辑上共同组成一个卷（volume），这个卷即是对客户端（容器）可见的存储单元并且可以被挂载，通过传统的 POSIX 接口访问。</p>

<p>因为上述 3 部分组件内部其实都是一个分布式系统，因此都用到了 Raft 作为一致性协议，资源管理器还用到了 RocksDB 作为本地持久化存储。稍微特殊的是数据子系统根据不同类型的写操作选择了不同的复制方案，论文里把这个叫做 Scenario-Aware Replication，具体讲就是顺序写操作（比如 append）用的是 primary-backup，而覆盖（overwrite）操作用的是 Raft。</p>

<p>系统的另一个亮点是基于资源利用率的 partition 分配策略，论文中叫做 Utilization-Based Placement。传统的 partition 分配策略通常是哈希，这种策略的优点是简单但是当扩缩容时必须进行 rebalance。CFS 的做法是元数据和数据子系统定期上报内存、磁盘使用率到资源管理器，当需要创建新的 partition 时根据资源利用率选择最低的那个节点，这样设计的好处是不再需要 rebalance。但是对于这种设计方案是否会造成数据不均衡表示存疑，论文中也没有做过多论述。</p>

<p>为了尽量减少客户端的网络交互，不让某个系统组件成为瓶颈，客户端会缓存元数据子系统、数据子系统和资源管理器的信息到本地，当执行文件操作时会优先读取本地缓存。当然某些组件（比如资源管理器）还是有可能在某一天成为瓶颈，但是基于京东的经验这件事情基本上不会发生。</p>

<p>在与 Ceph 的评测中，CFS 平均有 3 倍的 IOPS 提升，特别是多客户端和随机读写的场景。这很大程度上得益于元数据和数据节点分离的设计，且 CFS 的元数据是全内存存储，而 Ceph 并不是。</p>

<p>分布式文件系统一直都是比较重要的基础组件，在分布式数据库、大数据、机器学习领域有广泛应用。常见的分布式文件系统如 HDFS、Ceph，在如今这个全面推行容器化的时代越来越显得捉襟见肘。容器化一个很大的特点是快速扩缩容，传统的存储系统在这一点上是非常不友好的，因此才会有越来越多针对容器化场景的基础组件诞生（具体可以访问 <a href="https://www.cncf.io">CNCF</a> 查看），这里介绍的 CFS 是一个例子，另一个类似的是 <a href="https://juicefs.com">JuiceFS</a>。</p>

<p>CFS 目前属于 CNCF 下的 <a href="https://www.cncf.io/sandbox-projects">sandbox 项目</a>，且已经<a href="https://github.com/chubaofs/chubaofs">开源</a>，使用 Go 语言编写。</p>

<h2>tensorflow/community #237: RFC: Sparse Domain Isolation for Supporting large-scale Sparse Weights Training</h2>

<p><a href="https://github.com/tensorflow/community/pull/237">[链接]</a></p>

<p>推荐系统大规模稀疏特征分布式训练一直是工业界一件有挑战的事情，大公司内部自研的训练框架大多已经解决了这个问题，但是在开源社区问题仍然存在。TensorFlow 作为也许目前最流行的深度学习训练框架，社区里也早有相关的讨论（比如 <a href="https://github.com/tensorflow/tensorflow/issues/19324">#19324</a>、<a href="https://github.com/tensorflow/tensorflow/issues/24539">#24539</a>、<a href="https://github.com/tensorflow/tensorflow/pull/24915">#24915</a>），但基本都以烂尾告终。最新的 RFC #237 来自腾讯，区别于现有的一些开源实现（比如阿里巴巴的 <a href="https://github.com/alibaba/x-deeplearning">XDL</a>、字节跳动的 <a href="https://github.com/bytedance/byteps">BytePS</a>、蚂蚁金服的 <a href="https://github.com/sql-machine-learning/elasticdl">ElasticDL</a>）完全自己重新造了一个 parameter server，腾讯的方案最大限度复用了 TensorFlow 现有的组件，对用户的代码侵入也最小。目前这个 RFC 还在讨论中，有兴趣可以订阅 PR。</p>

<h2>深入云原生 AI：基于 Alluxio 数据缓存的大规模深度学习训练性能优化</h2>

<p><a href="https://mp.weixin.qq.com/s/2Pj8erPbYuMo7mBJvweJgQ">[链接]</a></p>

<p>机器学习模型训练由于依赖大量的数据作为输入，因此数据 I/O 的性能会直接影响模型训练的效率。有时间会发现计算设备的算力升级了，但是数据 I/O 跟不上了，反而拖慢了整个训练流程。阿里云团队分享的这篇文章便是他们在使用 Alluxio（试图）加速数据 I/O 的过程中的经验，虽然最后的优化结果性能指标其实也只是基本跟本地读取持平。</p>

<h2>Rob Pike interview: “Go has indeed become the language of cloud infrastructure”</h2>

<p><a href="https://evrone.com/rob-pike-interview">[链接]</a></p>

<p>没啥好介绍的了，值得一读的一篇采访。文中有两个有趣的问题：</p>

<ul>
<li><strong>对于 Rust 宣称的「没有 GC」的设计有什么看法</strong>：Rob Pike 只是表示了他对 Rust 很感兴趣，其它意见不便发表。</li>
<li><strong>如果可以时间旅行到最初设计 Go 的时候想给自己一个什么忠告</strong>：无视那些仇恨者（haters），只需要聆听那些理解以及和你有共同目标的人的声音。不可能每一个人都认同你正在做的事情，但是那些鼓励你前进的人会是提供给你非常棒（fantastic）的想法、能量和灵感的源泉。</li>
</ul>


<h2>孤芳「自赏」：盯鞋音乐的前世与今生</h2>

<p><a href="https://www.gcores.com/articles/121368">[上]</a> <a href="https://www.gcores.com/articles/123770">[下]</a></p>

<p>这两篇文章来自「竟然还能聊游戏」的机核，相对系统地介绍了「盯鞋（shoegaze）」这种音乐风格，作为目前可能是除了后朋克以外我最喜欢的音乐风格非常高兴能够有人科普，稍微欠缺的是文中没有提到任何中国的乐队。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maybe News Issue #0]]></title>
    <link href="https://blog.xiaogaozi.org/2020/05/11/maybe-news-issue-0/"/>
    <updated>2020-05-11T14:44:52+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/05/11/maybe-news-issue-0</id>
    <content type="html"><![CDATA[<blockquote><p>前言：从这一期开始这个系列将会有一个正式的名字「Maybe News」，名字来源于我非常喜欢的一个国内的音乐厂牌<a href="https://en.wikipedia.org/wiki/Maybe_Mars">「兵马司」</a>（Maybe Mars）。本身我分享的内容也很有可能是一些旧闻，只不过对于我来说是还未了解的知识罢了。<a href="/2020/04/26/weekly-reading-list-issue-1/">上一期</a>的名字还是维持原样就不做修改。你也可以通过<a href="https://digest.xiaogaozi.org/maybe-news">邮件订阅</a>这个系列的文章。</p></blockquote>

<!-- more -->


<h2>LightRec: a Memory and Search-Efficient Recommender System</h2>

<p><a href="http://staff.ustc.edu.cn/~liandefu/paper/lightrec.pdf">[链接]</a></p>

<p>这篇论文由微软亚洲研究院与中科大共同发表在 <a href="https://www2020.thewebconf.org">WWW 2020</a> 会议上，提出了一种新的表示物品向量的方法，大幅降低存储向量所需空间的同时还显著提升了召回效果。一个直观的数据：LightRec 将 1 千亿 256 维双精度向量的内存占用从 9.5 GB 降到了 337 MB，这是非常惊人的！现在工业界常用的 <a href="https://github.com/nmslib/nmslib">nmslib</a> 和 <a href="https://github.com/facebookresearch/faiss">Faiss</a> 都无法实现如此高的压缩比，因此很多时候都需要借助分布式存储来满足业务场景，如果真的如论文中所描述的一样那单机存储在未来很长一段时间来说都是完全足够的。</p>

<p>这里简单解释一下为什么向量召回对于当下的推荐系统如此重要，传统的召回是基于倒排索引的方式，正如我在<a href="/2020/04/21/how-to-design-a-distributed-index-framework-part-1/">之前的一篇文章</a>中介绍的那样，召回与模型优化目标之间的差异较大导致召回效果始终较差。自从 <a href="https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data">Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</a> 这篇论文（同样也是由微软研究院发表）提出 DSSM（Deep Structured Semantic Models）以后，将召回与 DNN 进行结合，显著提升了召回的效果，在很多公司的实践中也的确论证了 DSSM 是一个非常有效的召回方式。DSSM 的核心是分别为物品和用户生成向量，再通过 ANN（Approximate Nearest Neighbors）查询相似向量从而实现召回。因此向量的存储和查询效率决定了在线请求的效果和性能，如何平衡向量索引的空间占用和召回效果是非常重要的。</p>

<p>微软研究院的微信公众号有一篇简短的针对这篇论文的<a href="https://mp.weixin.qq.com/s/E43gc16A3OVWgxyfdUxr7g">中文版介绍</a>，有兴趣也可以先看这篇文章。</p>

<h2>TFRT: A new TensorFlow runtime</h2>

<p><a href="https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html">[链接]</a></p>

<p>Google 近期开源了新的 TensorFlow 运行时 TFRT（TensorFlow Runtime），这是一个介于上层用户代码和底层设备之间的执行环境。项目的愿景是实现一个统一的、可扩展的、性能首屈一指（best-in-class）的，同时可跨越多种领域硬件（domain specific hardware）的运行时。未来 TFRT 会成为 TensorFlow 默认的运行时，目前还在集成中。从 ResNet-50 的 inference 测试结果上看平均提升了 28% 的性能。</p>

<h2>Why We Need DevOps for ML Data</h2>

<p><a href="https://tecton.ai/blog/devops-ml-data">[链接]</a></p>

<p>虽然这是一篇产品推广软文（在文章最后一节），但是文章中普及的关于 DevOps 与机器学习之间的关系还是非常有价值的。很多人可能以为机器学习就只是模型算法而已，诚然这是学术研究的基石，但是要真正把机器学习应用到工业界光有算法是远远不够的。Google 著名的 <a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf.">Hidden Technical Debt in Machine Learning Systems</a> 论文已经论述了那些隐藏在模型背后的往往被人忽略的技术，模型规模越大需要付出的工程努力也是越大的（所以很多时候大公司才需要自己造轮子）。作为衍生阅读也可以同时看看 <a href="https://towardsdatascience.com/how-linkedin-uber-lyft-airbnb-and-netflix-are-solving-data-management-and-discovery-for-machine-9b79ee9184bb">How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions</a> 这篇文章。</p>

<h2>Mid-stack inlining in Go</h2>

<p><a href="https://dave.cheney.net/2020/05/02/mid-stack-inlining-in-go">[链接]</a></p>

<p>Dave Cheney 继续科普 Go 的一些实现细节，这次的主题是编译器如何实现 mid-stack inlining。所谓 mid-stack inlining 就是将那些调用了其它函数的函数变成 inline，相对的还有 leaf inlining，即不调用任何其它函数。有兴趣了解 leaf inlining 的可以看 Dave Cheney 的<a href="https://dave.cheney.net/2020/04/25/inlining-optimisations-in-go">上一篇文章</a>。</p>

<h2>Why We Leverage Multi-tenancy in Uber’s Microservice Architecture</h2>

<p><a href="https://eng.uber.com/multitenancy-microservice-architecture">[链接]</a></p>

<p>Uber 介绍了他们在微服务领域实践的一个经验「多租户」，简单讲就是让请求链路上的所有组件和系统都能够感知「租户」这个概念，比如租户可以分为生产环境和测试环境。Uber 列举了两个应用场景：集成测试和 Canary 部署，这两个场景都依赖生产环境的请求，有了租户的概念就可以自动进行请求路由和数据隔离。愿景其实挺美好，但「代价」也是不容忽视，前面讲了要让所有组件和系统都感知就非常依赖基础组件的统一，要解决这个问题很多时候并不单纯是一个技术问题。如何做好不同环境的数据隔离也是一个难题，关于这一点文章并没有做特别详细的介绍。</p>
]]></content>
  </entry>
  
</feed>
