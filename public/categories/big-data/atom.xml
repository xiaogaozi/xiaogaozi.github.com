<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | Freedom]]></title>
  <link href="https://blog.xiaogaozi.org/categories/big-data/atom.xml" rel="self"/>
  <link href="https://blog.xiaogaozi.org/"/>
  <updated>2021-03-23T12:30:39+08:00</updated>
  <id>https://blog.xiaogaozi.org/</id>
  <author>
    <name><![CDATA[xiaogaozi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[直播回顾：如何使用 JuiceFS 优化 Kylin 4.0 的存储性能]]></title>
    <link href="https://blog.xiaogaozi.org/2021/02/01/kylin-meetup-write-up/"/>
    <updated>2021-02-01T17:32:20+08:00</updated>
    <id>https://blog.xiaogaozi.org/2021/02/01/kylin-meetup-write-up</id>
    <content type="html"><![CDATA[<blockquote><p>本篇是 2021 年 1 月 30 日 <a href="https://mp.weixin.qq.com/s/9oFP-RIhc6HS3C9YRiPWYA">Kylin Meetup</a> 的直播回顾，主要介绍 JuiceFS 如何优化 Kylin 4.0 的存储性能。完整 slide 请点击<a href="https://www.slidestalk.com/juicedata/kylinmeetupwriteupoptimizekylinonjuicefs">这里</a>查看。</p></blockquote>

<!-- more -->


<p>大家好，我是来自 Juicedata 的架构师高昌健，今天给大家分享的主题是「如何使用 JuiceFS 优化 Kylin 4.0 的存储性能」。</p>

<p><img src="/images/posts/kylin-meetup-write-up-2.png" alt="Agenda" /></p>

<p>首先看一下今天分享的提纲，主要分为几个部分。首先对 Kylin 4.0 以及它在云上面临的挑战进行简单介绍，然后详细介绍一下 JuiceFS 是什么以及为什么我们要在 Kylin 中使用 JuiceFS，最后是一些 benchmark。</p>

<h2>Kylin 4.0 架构简介</h2>

<p><img src="/images/posts/kylin-meetup-write-up-4.png" alt="Kylin 4.0 Architecture" /></p>

<p>我们先来了解一下 Kylin 4.0 的架构。这是 4.0 的架构图，Kylin 4.0 摒弃了之前基于 HBase 的架构，改为使用 Spark 作为构建以及查询引擎，并且使用 Parquet 作为 Cube 的存储格式直接写到 HDFS 或者对象存储上。Kylin 4.0 的架构不仅性能上有大幅提升，也更加贴合现在云原生的部署方式。</p>

<p><img src="/images/posts/kylin-meetup-write-up-5.png" alt="Kylin 4.0 Architecture" /></p>

<p>这里想重点讲一下有关存储这块儿的改动，新的存储方式帮助 Kylin 真正实现了存储与计算分离的架构。传统大数据的架构都是存储与计算耦合的，也就是说没有办法单独对存储或者计算资源进行扩缩容。但是在云上使用对象存储替代 HDFS 作为大数据平台的底层存储已经越来越成为主流，也就需要上层计算组件的架构更加灵活。</p>

<h2>Kylin on Parquet 在云上的挑战</h2>

<p>但是我们也看到云上的对象存储或多或少都存在一些问题，也就为 Kylin 4.0 在云上部署和使用带来了一些挑战。这里我们来看一下对象存储的具体问题。首先很多时候大家会误以为对象存储是完全等价于 HDFS 的，但其实不是，在很多维度上对象存储和 HDFS 都是不一样的。</p>

<p><img src="/images/posts/kylin-meetup-write-up-7.png" alt="Object Storage Drawback" /></p>

<p>比如一致性模型，HDFS 是保证强一致性的文件系统，但是对象存储往往是最终一致性，也就是说当你往对象存储中写入新的数据以后并不能立即看到，有可能需要等待一段时间，并且这个等待时间对于用户来说是不可控的，最终一致性会给大数据平台的计算任务带来很多的不确定性，影响任务的稳定性。</p>

<p>然后是元数据操作性能。在大数据场景常见的一些元数据操作，比如 list 是列举某个目录有什么文件、rename 是重命名、delete 是删除，这些元数据操作在对象存储上性能都是不太好的，本质上是因为对象存储是一个 K/V 存储，没法高效实现刚才提到的这些元数据操作。这里比较明显的操作是 rename，因为很多对象存储都不支持重命名，因此真正底层实现的时候都是通过先拷贝原始数据到新的路径，再删除原始数据的方式来实现。这种实现在大数据这种大批量处理数据的场景对性能的影响会比较明显。</p>

<p>然后是数据本地性。Hadoop 因为是存储计算耦合的架构，因此当计算任务被调度时会尽量把计算任务分配到数据节点上，这样就可以提升数据读取的性能。但是对象存储并不提供这样的能力，所有数据都必须通过网络获取，这对于对象存储的带宽有很高的要求，也会对计算的性能造成影响。</p>

<p>对象存储还有一些隐性的点可能不一定被大家关注。对于一个 bucket 中的每个路径其实都有最大的 API 请求频率限制，如果计算任务发送的请求超过了这个限制，就会报错，当然我们可以通过重试或者降低请求并发的方式来一定程度上缓解这个问题，但都是治标不治本，并且也会给业务带来很多不便。对象存储的 API 请求也不是免费的，大数据场景很容易产生大量的请求，这些都会带来一些成本。</p>

<p>Hadoop 兼容性也会成为一个问题，Hadoop 生态具有繁多的组件，不同云厂商的对象存储也可能都会有一些差异，导致上层组件在接入时不一定能完全兼容，甚至出现组件无法正常使用的情况。</p>

<h2>JuiceFS 简介</h2>

<p><img src="/images/posts/kylin-meetup-write-up-9.png" alt="Intro to JuiceFS" /></p>

<p>针对刚才提到的这些问题，因此我们开发了 JuiceFS 这样一个项目。这里简单介绍一下 JuiceFS。JuiceFS 是一个开源的云原生分布式文件系统，从今年 1 月初开源以来已经在 GitHub 上积累了超过 2700 个 star。JuiceFS 创新性地基于 Redis 和对象存储构建，同时提供传统文件系统的诸多特性。</p>

<p>例如强一致性，利用独立的元数据管理以及 Redis 的事务特性，JuiceFS 能够确保数据的强一致性。目前 JuiceFS 已经支持市面上几乎所有的对象存储，不管是公有云上的，还是开源对象存储系统，因此对于机房用户来说也是非常友好的。</p>

<p>除了兼容 HDFS 接口以外，JuiceFS 还提供了标准的 POSIX 接口，这是目前 HDFS 以及对象存储都不支持或者支持得不好的一块儿。借助 FUSE 你可以像使用本地文件系统一样把 JuiceFS 挂载到机器上，再通过标准的命令行工具读写数据。此外 JuiceFS 还支持 S3、NFS、Samba 等协议，多种协议的支持使得只用 JuiceFS 一个文件系统就可以同时满足多种业务场景，而不用在不同存储之间重复拷贝数据。</p>

<p>用户访问 JuiceFS 是通过特定的客户端，这个客户端本身也是支持多系统的，包括 Linux、macOS、Windows。在 CPU 架构支持上，除了 x86 以外也支持现在应用广泛的 ARM 平台。</p>

<p>JuiceFS 也提供 K8s CSI 驱动，也就是说在 K8s 平台上你可以很方便地把 JuiceFS 挂载到容器里，这个挂载的 volume 是支持多个容器同时读写的（ReadWriteMany）。同时你也不用担心挂载的 volume 它的一个生命周期，都是由 JuiceFS 的 CSI 驱动来管理，自动地创建、销毁。JuiceFS 的 CSI 驱动很好地满足了容器平台共享存储的需求。</p>

<p>JuiceFS 也具备数据缓存的能力，可以把远端的数据，也就是对象存储的数据缓存到本地，这个后面在介绍 benchmark 的时候会再详细说明数据缓存的能力。最下面是 JuiceFS 的 <a href="https://github.com/juicedata/juicefs">GitHub 链接</a>。</p>

<p><img src="/images/posts/kylin-meetup-write-up-10.png" alt="JuiceFS Architecture" /></p>

<p>这是 JuiceFS 的架构图。结合刚刚讲到的，JuiceFS 在最底层依赖的是对象存储来作为最基本的数据块存储。JuiceFS 不是原封不动地把数据存储到对象存储上，所有通过 JuiceFS 写入的数据默认会按照 4MiB 一个块来分块，例如写入一个 100MiB 或者 1GiB 的文件会按照 4MiB 这个粒度来切分，切分以后再存储到对象存储上。之所以这样设计很大程度上是因为希望对大文件进行小的分块可以提升读写的吞吐和性能，相比直接一个大文件读取或者写入的话，拆分成小的块之后读写的性能提升还是蛮明显的。</p>

<p>然后在图的左边可以看到是一个 Redis，这里是把 Redis 作为元数据的存储，相当于所有文件系统的元数据都会存到 Redis 中。JuiceFS 依赖 Redis 的事务特性保证所有元数据的操作的原子性，也就是保证元数据的强一致性。</p>

<p>然后右边是 JuiceFS 的客户端。JuiceFS 已经提供了多种客户端，比如 FUSE 是通过挂载来提供 POSIX 接口；Java SDK 是在 Hadoop 环境中使用，我们近期也已经将 Hadoop SDK 开源出来；最右边的是 S3 Gateway，通过这个 gateway 可以对上层提供 S3 兼容的接口。可以看到 JuiceFS 提供了多协议的客户端，这些客户端底层都是共享了同一个实现，也就是图上的 Client Core。除了 Java SDK 以外，客户端整体上都是用 Go 语言实现，Java SDK 底层也是通过 JNI 去调用 Go 语言的接口。</p>

<p>前面提到了数据缓存，意思就是说当从对象存储读取数据时，client 端会自动地把数据缓存到本地配置的某个缓存路径，JuiceFS 也会自动地管理缓存空间，比如说当缓存盘满了之后应该怎么去失效、怎么去维护缓存数据的生命周期，以及保证缓存数据与对象存储之间的一致性。这些都是由 JuiceFS client 端提供的特性。</p>

<h2>为什么 Kylin 和 JuiceFS 要一起使用？</h2>

<p>下面一个问题就是为什么 Kylin 要和 JuiceFS 一起使用呢？经过前面的介绍，大家可能也或多或少能够想到一些，本质上希望通过 JuiceFS 来解决刚才提到的对象存储的各种问题。所以具体来说 JuiceFS 能给 Kylin 带来的收益有这么几个。</p>

<p><img src="/images/posts/kylin-meetup-write-up-12.png" alt="JuiceFS Advantages" /></p>

<p>首先是强一致性，也就是相比对象存储的最终一致性来说 JuiceFS 是一个强一致性的文件系统。然后是高性能，JuiceFS 不管是在元数据还是数据的读写上都有很好的性能表现。元数据是基于 Redis 的，Redis 因为是全内存的 K/V 存储，所以元数据操作的性能是非常高的，同时 JuiceFS 在 Redis 之上也做了一些优化。数据的读写 JuiceFS 默认是按照 4MiB 的块来分块存储，在读写上也有蛮多的性能提升。</p>

<p>然后数据本地性也是 JuiceFS 能够带来的一个好处，通过缓存可以把数据缓存到计算节点上，虽然计算节点的缓存空间不一定特别大，但一定程度上也提供了数据本地性。这块儿的具体实现是通过 Java SDK 提供了缓存数据的 location 给上层的调度器（比如 YARN），使得调度器能够知道缓存数据当前是在哪一个计算节点上，可以在调度时把相应的计算任务调度到具有缓存数据的节点上，也就一定程度上实现了数据本地性。即使调度失败了，也有一些进一步的优化，例如在不同计算节点之间组成一个分布式缓存系统，也就是节点互相之间是能够读取对方的缓存数据，对数据读取进行优化，而不是完全依赖底层的对象存储的吞吐和性能。</p>

<p>JuiceFS 同时也是完整兼容 Hadoop 生态的，不管是对于 Hadoop 2.x 和 Hadoop 3.x 这种大版本的变化，还是对于某个 Hadoop 发行版的所有组件都是完全兼容的。因为 JuiceFS 本身提供的就是一个标准的 HDFS 接口，对于上层的组件来说基本上是透明的，也没有任何侵入，就可以当作是 HDFS 来使用。</p>

<p>TCO 低也是 JuiceFS 的一个好处。前面提到对象存储的 API 请求是收费的，这些 API 请求中涵盖了很多元数据的请求， 当使用 JuiceFS 之后这些请求会直接发给元数据服务（也就是 Redis），也就不存在元数据请求的费用。数据的请求依靠比如缓存这样的特性很大程度上减少对对象存储的依赖，整体上来说成本都能有一定优化。</p>

<p>JuiceFS 还支持一些 HDFS 或者对象存储可能不提供的功能。比如快照，快照的意思是可以针对某一个目录创建一个 snapshot，看起来好像是对数据进行了一次拷贝，但底层的实现其实是不存在拷贝的。本质上是一个 copy-on-write 的原理，在创建快照时只是对元数据进行了拷贝，底层是共享的同一份数据，并没有拷贝对象存储上数据。对你对快照进行修改时，就会将原始数据进行拷贝，然后存储修改后的数据。这个特性对于很多场景是非常有帮助的，比如测试、多版本管理。</p>

<p>符号链接在操作系统中是一个常见的功能，通过符号链接可以将一个文件指向另一个文件的路径。在 JuiceFS 上也实现了这个功能，不仅可以将 JuiceFS 的路径映射到另一个路径，也可以链接到任何 JuiceFS 支持的存储上（比如 HDFS、对象存储）。通过 JuiceFS 实现一个统一的文件系统命名空间管理，可以在这一个命名空间中看到多种存储的数据。这其实是一个蛮有用的特性，比如我们对数据进行迁移时是非常有帮助的（可以参考之前的<a href="https://juicefs.com/blog/cn/posts/migrate-data-from-hdfs-to-juicefs">一篇文章</a>）。</p>

<p>本身 JuiceFS 是一个开源的项目，但同时我们也提供一个云上全托管的版本，会将元数据服务进行统一管理，也有一个 web 的控制台可以让大家很方便地使用。这是 JuiceFS 商业版提供的一个功能。</p>

<h2>性能比较</h2>

<p><img src="/images/posts/kylin-meetup-write-up-14.png" alt="Benchmark Config" /></p>

<p>然后分享一下之前做的一个 benchmark 结果。先看一下测试环境，这里用了标准的 10GB 大小的 TPC-H 作为数据集，用了 1 台 master，3 台 worker，大概是这样的配置。对比的是 Kylin on OSS 和 Kylin on JuiceFS 的性能。</p>

<p><img src="/images/posts/kylin-meetup-write-up-15.png" alt="Benchmark Result" /></p>

<p>这是最终 benchmark 的对比图，黄色的是 OSS，绿色的是 JuiceFS，图上展示的是 TPC-H 每一个查询的执行时间，这个时间越低越好。</p>

<p><img src="/images/posts/kylin-meetup-write-up-16.png" alt="Benchmark Summary" /></p>

<p>总结一下刚刚提到的测试。因为 Kylin 需要提前构建 cube，我们在测试时遇到 Kylin on OSS 这个方案构建 cube 直接失败了，导致没有办法在 OSS 上构建 cube。最终是通过把 Kylin on JuiceFS 构建完的 cube 拷贝到 OSS 上完成的测试。总的查询时间上 JuiceFS 快 38% 左右，然后看单个查询的执行时间 JuiceFS 最多能够快 85%，平均下来也能快 46% 左右。</p>

<h2>未来展望</h2>

<p><img src="/images/posts/kylin-meetup-write-up-18.png" alt="Future Plan" /></p>

<p>最后想展望一下未来 JuiceFS 的一些新特性。JuiceFS 已经在性能上进行了很多优化，前面的 benchmark 也能看到，但其实也有很多细粒度的点我们可以优化。比如说查询预读，可以从 JucieFS 的角度预先知道某个查询接下来会读取的文件，也就可以自动地在后台进行一些预读的处理，进一步加速查询的效率。刚刚提到的 P2P 分布式缓存特性也会是接下来在开源版本的 Hadoop SDK 去优化的一个点。最后 JuiceFS 会提供一个 profiling 的工具，这个工具是为了帮助用户更快更简便地调优，发现性能热点，从而针对性地对读写进行优化。</p>

<p>今天的分享就到这里，感谢大家。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[环球易购数据平台如何做到既提速又省钱？]]></title>
    <link href="https://blog.xiaogaozi.org/2020/11/03/globalegrow-big-data-platform-user-case/"/>
    <updated>2020-11-03T11:58:51+08:00</updated>
    <id>https://blog.xiaogaozi.org/2020/11/03/globalegrow-big-data-platform-user-case</id>
    <content type="html"><![CDATA[<blockquote><p>这篇文章最初发表在 JuiceFS 官方博客，点击<a href="https://juicefs.com/blog/cn/posts/globalegrow-big-data-platform-user-case">这里</a>查看原文。</p></blockquote>

<!-- more -->


<h2>客户简介</h2>

<p><a href="https://www.globalegrow.com">环球易购</a>创建于 2007 年，致力于打造惠通全球的 B2C 跨境电商新零售生态，2014 年通过与百圆裤业并购完成上市，上市公司「跨境通（SZ002640）」是 A 股上市跨境电商第一股。经过多年的努力，在海外市场建立了广阔的销售网络，得到了美国、欧洲等多国客户的广泛认可，公司业务多年来一直保持着 100% 的增长速度。</p>

<h2>数据平台现状及需求</h2>

<p>环球易购提供面向全球的跨境电商服务，选择 AWS 作为云服务商。基于 EC2 和 EBS 自建 CDH 集群，计算引擎使用了 Hive 和 Spark。当时的环球易购大数据平台面临这么几个问题：</p>

<ul>
<li>基于 EBS 搭建的 HDFS 集群成本很高</li>
<li>Hadoop 集群缺乏弹性伸缩能力</li>
</ul>


<p>因此希望能够在降低 HDFS 存储成本的同时，不会在性能上造成太大损失。说到降低成本那么很自然地会联想到 S3，S3 在提供高达 11 个 9 的数据持久性的同时也能够做到足够低廉的存储成本。但是大数据集群存储由 HDFS 迁移到 S3 是唯一选择么？迁移和使用中会遇到哪些问题呢？这些我们在后面都会详细介绍，不过首先来看看为什么 EBS 自建的 HDFS 集群成本很高。</p>

<h2>云上自建 HDFS 的痛点</h2>

<p>EBS 是一种易于使用的高性能数据块存储服务，通过挂载到 EC2 上来提供近乎无限容量的存储空间。为了保证 EBS 上数据的可用性，所有数据都会自动在同一可用区内进行复制，防止数据丢失。</p>

<p>HDFS 是目前大数据领域最常使用的分布式文件系统，每个文件由一系列的数据块组成。同样的，为了保证数据的可用性，HDFS 默认会将这些数据块自动复制到集群中的多个节点上，例如当设置副本数为 3 时同一数据块在集群中将会有 3 份拷贝。</p>

<p>通过以上介绍可以看到 EBS 和 HDFS 都会通过复制数据来保证可用性，区别在于 EBS 是只针对每块存储卷（即磁盘）的数据进行复制，而 HDFS 是针对整个集群的数据。这种双重冗余的机制其实有些多余，也变相增加了存储成本。同时 HDFS 的多副本特性使得集群的实际可用容量会小很多，例如当副本数为 3 时实际可用容量其实只有总磁盘空间大小的 1/3，再加上通常会在集群空间到达一定水位时就进行扩容，这会进一步压缩可用容量。<strong>基于以上原因，在云上通过 EBS 自建 HDFS 集群的存储成本通常会高达￥1000/TB/月。</strong></p>

<h2>从 HDFS 迁移到 S3 我们需要考虑什么？</h2>

<p>Hadoop 社区版默认已经支持从 S3 读写数据，即通常所说的「S3A」。但是如果你去看 S3A 的<a href="http://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#Warnings">官方文档</a>，会在最开始看到几个大大的警告，里面列举了一些类 S3 的对象存储都会存在的问题。</p>

<h3>一致性模型（Consistency Model）</h3>

<p>S3 的一致性模型是<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel">最终一致性</a>，也就是说当创建了一个新文件以后，并不一定能立即看到它；当对一个文件执行删除或者更新操作后，有可能还是会读到旧的数据。这些一致性问题会导致程序崩溃，比如常见的 <code>java.io.FileNotFoundException</code>，也可能导致错误的计算结果，更麻烦的是这种错误很难发现。我们在测试过程中就因为 S3 的一致性问题使得执行 DistCp 任务频繁报错，导致数据迁移受到严重影响。</p>

<h3>没有真实的目录</h3>

<p>S3 中的「目录」其实是通过对象名称的前缀模拟出来的，因此它并不等价于通常我们在 HDFS 中见到的目录。例如当遍历一个目录时，S3 的实现是搜索具有相同前缀的对象。这会导致几个比较严重的问题：</p>

<ul>
<li><strong>遍历目录可能会很慢。</strong>遍历的时间复杂度取决于目录中的总文件数。</li>
<li><strong>重命名目录也可能会很慢。</strong>跟遍历目录一样，总文件数是影响性能的重要因素。同时 S3 重命名一个文件其实是先拷贝到新路径，再删除原始文件，这个过程也是比较耗时的。</li>
<li><strong>重命名或者删除目录不是原子操作。</strong>HDFS 上只需要 O(1) 的操作，在 S3 上变成了 O(n)。如果操作过程中任务失败，将会导致数据变成一个不可知的中间状态。</li>
</ul>


<h3>认证模型（Authorization Model）</h3>

<p>S3 的认证模型是在 S3 服务内部基于 IAM 实现的，这区别于传统的文件系统。因此当通过 Hadoop 访问 S3 时会看到文件的 owner 和 group 会随着当前用户的身份而动态变化，文件的权限都是 666，而目录的权限都是 777。这种与 HDFS 大相径庭的认证模型会使得权限管理复杂化，并且也显得不够通用，只能限定在 AWS 内使用。</p>

<h2>JuiceFS 带来了什么？</h2>

<p>JuiceFS 基于对象存储实现了一个<strong>强一致性的分布式文件系统</strong>，一方面保持了 S3 弹性伸缩无限容量，99.999999999% 的数据持久性安全特性，另一方面前面提到的 S3 的种种「问题」都能完美解决。同时 JuiceFS 完整兼容 Hadoop 生态的各种组件，对于用户来说可以做到无缝接入。认证模型上 JuiceFS 遵循与 HDFS 类似的 user/group 权限控制方式，保证数据的安全性，也能对接 Hadoop 生态中常用的如 Kerberos、Ranger、Sentry 这些组件。更加重要的是，相比环球易购现有的基于 EBS 的存储方案，使用 JuiceFS 以后<strong>每 TB 每月的存储成本将会至少节省 70%</strong>。</p>

<p>存储成本大幅下降的同时，性能表现又如何呢？下面分享一下相关的测试结果。</p>

<h2>测试结果</h2>

<p>测试环境是 AWS 上自建的 CDH 集群，CDH 版本为 5.8.5。测试的计算引擎包括 Hive 和 Spark，数据格式包括纯文本和 ORC，使用 TPC-DS 20G 和 100G 这两个规模的数据集。对比的存储系统有 S3A、HDFS 及 JuiceFS。</p>

<h3>创建表</h3>

<p><img src="/images/posts/globalegrow-create-table.png" alt="" />
这里以创建 <code>store_sales</code> 这个分区表为例</p>

<h3>修复表分区</h3>

<p><img src="/images/posts/globalegrow-repair-table.png" alt="" />
这里以修复 <code>store_sales</code> 这个表的分区为例</p>

<h3>写入数据</h3>

<p><img src="/images/posts/globalegrow-insert-table.png" alt="" />
这里以读取 <code>store_sales</code> 这个分区表并插入临时表为例</p>

<h3>读取纯文本格式数据</h3>

<p><img src="/images/posts/globalegrow-text-20g.png" alt="" />
<img src="/images/posts/globalegrow-text-100g.png" alt="" />
分别使用 Spark 测试了 20G 和 100G 这两个数据集，取 TPC-DS 前 10 个查询，数据格式为纯文本。</p>

<h3>读取 ORC 格式数据</h3>

<p><img src="/images/posts/globalegrow-orc-20g.png" alt="" />
<img src="/images/posts/globalegrow-orc-100g.png" alt="" />
分别使用 Spark 测试了 20G 和 100G 这两个数据集，取 TPC-DS 前 10 个查询，数据格式为 ORC。</p>

<h3>测试结果总结</h3>

<p>对于建表和修复表分区这样的操作，因为依赖对底层元数据的频繁访问（例如遍历目录），JuiceFS 的性能大幅领先于 S3A，<strong>最多有 60 倍的性能提升</strong>。</p>

<p>在写入数据的场景，JuiceFS 的性能相对于 S3A 有 5 倍的提升。这对于 ETL 类型的任务来说非常重要，通常 ETL 任务都会涉及多个临时表的生成和销毁，这个过程会产生大量的元数据操作（例如重命名、删除）。</p>

<p>当读取类似 ORC 这种列式存储格式的数据时，区别于纯文本文件的顺序读取模式，列式存储格式会产生很多随机访问，JuiceFS 的性能再次大幅领先 S3A，<strong>最高可达 63 倍</strong>。同时相比于 HDFS，JuiceFS 也能有最多 2 倍的性能提升。</p>

<h2>数据迁移</h2>

<p>环球易购的大数据平台经过长期的发展已经积攒大量的数据和业务，怎么从现有方案迁移到新的方案也是评估新方案是否合适的重要因素。在这方面，JuiceFS 提供了多种数据迁移方式：</p>

<ul>
<li><strong>将数据拷贝到 JuiceFS。</strong>这种方式的读取性能最好，可以高效地利用本地磁盘缓存和分布式缓存，也能保证数据的强一致性。但是涉及数据拷贝，因此迁移成本比较高。</li>
<li><strong>通过 import 命令将 S3 的数据导入。</strong>这种方式只涉及元数据的导入，将 S3 上面的对象导入到 JuiceFS 的目录树。这种方式无需拷贝数据，迁移速度快。但是没有办法保证强一致性，并且不能利用缓存加速功能。</li>
<li><strong>通过符号链接将已有数据和新数据融合到一起。</strong>JuiceFS 不仅可以在文件系统内部建立符号链接，也可以跨文件系统建立符号链接。例如通过 <code>ln -s hdfs://dir /jfs/hdfs_dir</code> 这行命令可以创建一个指向 HDFS 的符号链接。基于这种方式，可以将历史数据直接链接到 JuiceFS 中，然后通过统一的 JuiceFS 命名空间访问其它所有 Hadoop 文件系统。</li>
</ul>


<h2>选择</h2>

<p>结合测试结果以及综合成本分析，全面对比了 HDFS、S3 和 JuiceFS 的方案，环球易购认为 JuiceFS 相比另外两个方案有显著的性能和成本优势，决定用 JuiceFS 替换自建的 HDFS。这些优势具体体现为以下 3 个方面：</p>

<p>首先，JuiceFS 可以实现从 HDFS 的平滑迁移，对上游的计算引擎可以做到全面兼容，对现有的权限管理体系可以保持一致，同时性能上没有任何下降。这几点对数据平台的迁移可以说是至关重要的，没有这样的基础，数据平台的迁移将是一场耗时耗力的战役。<strong>而有了这样的基础，客户只用不到一个月的时间就完成了业务和数据的迁移。</strong></p>

<p>第二，在成本方面，「云上自建 HDFS 的痛点」一节中已经有过说明，基于 EBS 自建 HDFS 单独计算磁盘成本就大约有￥1000/TB/月，而 JuiceFS 仅为 27%。这还不是 TCO 成本，TCO 还应该包括 HDFS 所消耗的 CPU、内存、运维管理投入的人力成本，按经验值来说至少翻倍。而 JuiceFS 客户使用全托管服务，没有任何运维管理的投入。<strong>这样从 TCO 角度看，可以节省近 90% 的成本。</strong></p>

<p>最后，也是最重要的一点。大数据平台的存储引擎从 HDFS 换成 JuiceFS 后，整个平台就实现了存储计算分离，在 <a href="https://juicefs.com/blog/cn/posts/why-disaggregated-compute-and-storage-is-future">「为什么说存储和计算分离的架构才是未来？」</a> 一文中详细分析了存储计算耦合的痛点，以及业界的一些实践。现在 JuiceFS 作为完全兼容 HDFS 的云原生文件系统，已经是 基于 Hadoop 生态构建的大数据平台的完美存储方案。存储计算分离是大数据平台弹性伸缩的基础，这一步的改造对环球易购数据平台的架构设计来说也有着重要的意义，接下来环球易购的数据团队将深入到集群弹性伸缩、工作负载混合部署等研究和实践中。</p>
]]></content>
  </entry>
  
</feed>
